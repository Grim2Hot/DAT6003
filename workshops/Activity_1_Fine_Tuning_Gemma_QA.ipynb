{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8b1689",
   "metadata": {},
   "source": [
    "# DAT6004 Week 6\n",
    "# Activity-1: Fine-tuning an LLM using LoRA on a CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe733e3",
   "metadata": {},
   "source": [
    "## The objective of this worksheet is to demonstrate how large pre-trained language models (LLMs) can be fine-tuned using LoRA to perform a specific downstream task, such as Question Answering (Q-A), using limited computational resources (CPU only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ebdf3",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e999c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0486049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert data into a Hugging Face Dataset format\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab91325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to import tools from Hugging Face to load models, tokenizers, and training utilities\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895a5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to import LoRA-related tools\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ecb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for tensor manipulation and model training\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fb00e",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0f20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a CSV file containing question-answer pairs - each row has two columns: \"question\" and \"answer\"\n",
    "df = pd.read_csv('QA_Dataset.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9332905",
   "metadata": {},
   "source": [
    "## 3. Create prompt text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15827ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines question and answer into a single prompt format - mimicing how generative models learn from prompts\n",
    "df[\"text\"] = \"### Question:\\n\" + df[\"question\"] + \"\\n\\n### Answer:\\n\" + df[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a804c",
   "metadata": {},
   "source": [
    "## 4. Convert to Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "685cb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the pandas DataFrame into a Hugging Face-compatible Dataset object - keeps only the \"text\" column\n",
    "dataset = Dataset.from_pandas(df[[\"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb9f28",
   "metadata": {},
   "source": [
    "## 5. Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd3c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a30242",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5496807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the padding token to be the same as the End-of-Sequence (EOS) token for compatibility\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12c33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to the end of the input, not the beginning\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b032c90",
   "metadata": {},
   "source": [
    "## 6. Tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3291a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizes the text into input IDs and attention masks - ensures all inputs are the same length (max_length=512)\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512, add_special_tokens=True)\n",
    "    # Initialize an empty list to store the labels for each tokenized input\n",
    "    labels = []           \n",
    "    # Iterate through each tokenized sequence of input IDs\n",
    "    for input_ids in tokenized[\"input_ids\"]:\n",
    "        # Create a label sequence where each token ID remains the same, except for padding tokens which are replaced with -100\n",
    "        label = [(token if token != tokenizer.pad_token_id else -100) for token in input_ids]\n",
    "        labels.append(label)\n",
    "        tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdeefe",
   "metadata": {},
   "source": [
    "## 7. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a07492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d23f6425ed7495e8ac90d57c0270358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applies the tokenizer to each batch of examples - batched=True enables faster vectorized processing\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e434133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d28f10",
   "metadata": {},
   "source": [
    "## 8. Clean & format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9ce50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes original text column to save memory\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea3249bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the dataset format to PyTorch tensors, ready for training\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2392c",
   "metadata": {},
   "source": [
    "## 9. Load full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0e0583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68471d90bc004faab2828d024f9850e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads the full causal language model for text generation - this is the model we will fine-tune using LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da82177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.norm\n",
      "model.rotary_emb\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577effe",
   "metadata": {},
   "source": [
    "## 10. Prepare for LoRA (CPU version, no 4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae8c1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                                  # Rank of the low-rank adapters\n",
    "    lora_alpha=16,                        # Scaling factor for LoRA updates\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Specific layers to apply LoRA to - module-names are model specific\n",
    "    lora_dropout=0.1,                     # Applies dropout to only the LoRA weights to control regularization\n",
    "    bias=\"none\",                          # Controls whether the bias terms in the model are also adapted\n",
    "    task_type=TaskType.CAUSAL_LM          # Tells PEFT the task type - determines which parts of the model to freeze/unfreeze\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "827f2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds LoRA adapters to the model without changing the original weights\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a870062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 921,600 || all params: 2,507,094,016 || trainable%: 0.0368\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f7ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cede1f4",
   "metadata": {},
   "source": [
    "## 11. Data collator and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b399e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles and formats batches during training - pads sequences, aligns labels, formats inputs etc.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c981279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter sapce\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-qa\",   # Where to save model checkpoints\n",
    "    per_device_train_batch_size=1,  # Controls how many examples are included in a single training step\n",
    "    gradient_accumulation_steps=2,  # Controls how many batches the model processes before updating the weights\n",
    "    learning_rate=0.0001,           # Step size for weight updates during training\n",
    "    num_train_epochs=1,             # Controls how many times to go over the full training dataset\n",
    "    logging_steps=1,                # Controls logging training loss and other metrics every N steps\n",
    "    save_steps=10,                  # Controls saving a model checkpoint every N steps\n",
    "    save_total_limit=1,             # Keeps only the most recent N checkpoints to save disk space\n",
    "    remove_unused_columns=False,    # Prevents Hugging Face from automatically dropping any unused columns in the dataset\n",
    "    save_safetensors=False,         # Whether to save model weights in .safetensors format - set True for safe serialization\n",
    "    report_to=\"none\"                # Whether or not log to experiment tracking systems\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5810727",
   "metadata": {},
   "source": [
    "## 12. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5213f409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sm24310\\AppData\\Local\\Temp\\ipykernel_22008\\507548412.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downgrade numpy if you get an error in Colab\n",
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8d1cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sm24310\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 05:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.475700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.453838038444519, metrics={'train_runtime': 351.6965, 'train_samples_per_second': 0.057, 'train_steps_per_second': 0.028, 'total_flos': 121823601623040.0, 'train_loss': 2.453838038444519, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ada52",
   "metadata": {},
   "source": [
    "## 13. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d315c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./gemma-lora-qa/lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a7035a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gemma-lora-qa/tokenizer\\\\tokenizer_config.json',\n",
       " './gemma-lora-qa/tokenizer\\\\special_tokens_map.json',\n",
       " './gemma-lora-qa/tokenizer\\\\tokenizer.model',\n",
       " './gemma-lora-qa/tokenizer\\\\added_tokens.json',\n",
       " './gemma-lora-qa/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./gemma-lora-qa/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31afb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
