{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cc88ad",
   "metadata": {},
   "source": [
    "# DAT6004 Week 6\n",
    "# Activity-2: Using the Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001cbd2",
   "metadata": {},
   "source": [
    "## The objective of this worksheet is to demonstrate how to use a fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c182630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade evaluate datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307692b",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c7f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af4b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to import tools from Hugging Face to load models and tokenizers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d399a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class to load PEFT (e.g. LoRA) adapter on a base model\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9a3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for tensor manipulation and model training\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011a83c",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b3ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a CSV file containing question-answer pairs - each row has two columns: \"question\" and \"answer\"\n",
    "df = pd.read_csv('QA_Dataset_Test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e45b2",
   "metadata": {},
   "source": [
    "## 3. Load tokenizer, base model, and LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a27d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"google/gemma-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9146a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_adapter_path = \"./gemma-lora-qa/lora_adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22eef9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"./gemma-lora-qa/tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb3713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name) # also try with tokenizer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5922a08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daa02dcfd334f6fbfe76de980b6c050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d438c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    lora_adapter_path,\n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a8d50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode (disables dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Move the model to CPU for inference or resource management\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151da65d",
   "metadata": {},
   "source": [
    "## 4. Setup the Generator Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5781dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    device= -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfaec3d",
   "metadata": {},
   "source": [
    "## 5. Define QA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a39eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    prompt = \"You are a helpful assistant. Please respond with the answer to the given question only and avoid explanation etc.\\n\"\n",
    "    prompt += f\"### Question:\\n{question}\\n\\n### Answer:\"\n",
    "    answer = generator(prompt, return_full_text=False)[0]['generated_text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2cfac",
   "metadata": {},
   "source": [
    "## 6. Test the model with a single question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f17c330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = answer_question(\"What is the capital of Pakistan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e81278c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Islamabad\n",
      "\n",
      "<strong>Note:</strong> <em>The question was taken from one of my old assignments that I have uploaded on this website in past</em>.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a51e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the original tokenizer from gemma-2b model and compare the reults with the fine-tuned tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782ddce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "445f5dcc",
   "metadata": {},
   "source": [
    "## 7. Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a7aa42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61d3cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in df['question']:\n",
    "    pred = answer_question(q)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f92ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_answer'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ceaa3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>clean_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>\\nParis</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Walse?</td>\n",
       "      <td>Cardiff</td>\n",
       "      <td>\\nWalse's capital is &lt;strong&gt;Palghar&lt;/strong&gt;....</td>\n",
       "      <td>Walse's capital is &lt;strong&gt;Palghar&lt;/strong&gt;.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is Imran Khan?</td>\n",
       "      <td>Former Pakistani Cricket Captan</td>\n",
       "      <td>\\nImran Khan, born in Lahore, Pakistan, is one...</td>\n",
       "      <td>Imran Khan, born in Lahore, Pakistan, is one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which country London is in?</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>\\na) Belgium\\nb) France\\nc) Switzerland\\nd) Un...</td>\n",
       "      <td>a) Belgium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question                           answer  \\\n",
       "0  What is the capital of France?                            Paris   \n",
       "1   What is the capital of Walse?                          Cardiff   \n",
       "2              Who is Imran Khan?  Former Pakistani Cricket Captan   \n",
       "3     Which country London is in?                   United Kingdom   \n",
       "\n",
       "                                    predicted_answer  \\\n",
       "0                                            \\nParis   \n",
       "1  \\nWalse's capital is <strong>Palghar</strong>....   \n",
       "2  \\nImran Khan, born in Lahore, Pakistan, is one...   \n",
       "3  \\na) Belgium\\nb) France\\nc) Switzerland\\nd) Un...   \n",
       "\n",
       "                                          clean_pred  \n",
       "0                                              Paris  \n",
       "1       Walse's capital is <strong>Palghar</strong>.  \n",
       "2  Imran Khan, born in Lahore, Pakistan, is one o...  \n",
       "3                                         a) Belgium  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9998e7",
   "metadata": {},
   "source": [
    "## 8. Evaluate performance using BLEU, ROUGE and BERT-SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f4d4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7e82ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01b1c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f58eaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f8f98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c5b7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e13163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean model prediction by stripping whitespace and returning only the first line\n",
    "\n",
    "def clean_pred(text):\n",
    "    # Remove leading/trailing whitespace and newlines\n",
    "    text = text.strip()\n",
    "    # Take only the first line (before first newline)\n",
    "    first_line = text.split('\\n')[0]\n",
    "    return first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffb04cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to each predicted answer and store in new column\n",
    "\n",
    "df['clean_pred'] = df['predicted_answer'].apply(clean_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7278690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of predicted answer strings\n",
    "predictions = df[\"clean_pred\"].tolist()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d2e88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists of reference strings\n",
    "references = [[ref] for ref in df[\"answer\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23d08707",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result = bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00acfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"BLEU score: {bleu_result['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a57235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_result = rouge.compute(predictions=df['clean_pred'], references=df['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2caac56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1: 0.2635\n",
      "ROUGE-2 F1: 0.0000\n",
      "ROUGE-L F1: 0.2635\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROUGE-1 F1: {rouge_result['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {rouge_result['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L F1: {rouge_result['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe7d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "921c1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTScore metric\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48c271da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Compute BERTScore (default model is 'bert-base-uncased', you can change it)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "808217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertscore_result contains precision, recall, and F1 lists\n",
    "# We usually look at the average F1 score\n",
    "average_f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47dc9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.8572\n"
     ]
    }
   ],
   "source": [
    "print(f\"BERTScore F1: {average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfa665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
