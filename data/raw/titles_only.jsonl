"run_squad questions"
"MRPC hyperparameters question"
"Failure during pytest (and solution for python3)"
"Crash at the end of training"
"Is there a plan to have a FP16 for GPU so to have larger batch size or longer text documents support ?"
"Swapped to_seq_len/from_seq_len in comment"
"py2 code"
"Bug in run_classifier.py"
"activation function in BERTIntermediate"
"will you push the pytorch code for the pre-training process?"
"model loading the checkpoint error"
"ValueError while using --optimize_on_cpu"
"[Feature request] Port SQuAD 2.0 support"
"can you push the run-pretraining and create_pretraining_data codes?"
"Checkpoints not saved"
"how to load checkpoint?"
"speed is very slow"
"[Feature request] Add example of finetuning the pretrained models on custom corpus"
"BERT model for Machine Translation"
"[Bug report] Ineffective no_decay when using BERTAdam"
"Can not find vocabulary file for Chinese model"
"issues with accents on convert_ids_to_tokens()"
"How to detokenize a BertTokenizer output?"
"using BERT as a language Model"
"truncated normal initializer"
"Command-line interface Document Bug"
"Typo in README"
"grad is None in squad example"
"Race condition when prepare pretrained model in distributed training "
"Issue of `bert_model` arg in `run_classify.py`"
"Assertion `srcIndex < srcSelectDimSize` failed."
"Fine-Tuned BERT-base on Squad v1. "
"example for is next sentence"
"Multilingual Issue"
"pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py  error"
"Missing options/arguments in run_squad.py for BERT Large"
"UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 3920: character maps to <undefined>"
"Multi-GPU training vs Distributed training"
"example in BertForSequenceClassification() conflicts with the api "
"Loss calculation error"
"[Feature request ] Add support for the new cased version of the multilingual model"
"Missing function convert_to_unicode in tokenization.py"
"not good when I use BERT for seq2seq model in keyphrase generation"
"BERTConfigs in example usages in `modeling.py` are not OK (?)"
"Specify a model from a specific directory for extract_features.py"
"Unseen Vocab"
"Feature extraction for sequential labelling"
"3 sentences as input for BertForSequenceClassification?"
"`TypeError: object of type 'NoneType' has no len()` when tuning on squad  "
"Accuracy on classification task is lower than the official tensorflow version"
"cannot access to pretrained vocab file on S3"
"run_squad script gets stuck"
"Wrong signature in model call in run_classifier.py example (?)"
"TypeError: object of type 'WindowsPath' has no len()"
"numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1"
"How can I apply BERT to a cloze task?"
"There is some problem in supporting continuously training"
"AttributeError: 'tuple' object has no attribute 'backward'"
"Error while runing example"
"How to use pre-trained SQUAD model? "
"code in run_squad.py line 263"
"Error when calculating loss and running backward"
"bert-base-multilingual-cased - Text bigger than 512"
"Fine tuned to Multi-choice dataset?"
"Bert uncased and Bert large giving much lower results than Bert cased base"
"Not updating the BERT embeddings during the fine tuning process"
"RuntimeError: cuda runtime error (59) : device-side assert triggered"
"Problem about convert TF model and pretraining"
"run_squad.py stuck on batch size greater than 1"
"Squad dataset has multiple answers to a question."
"How to modify the model config?"
"Words after tokenization replaced with #"
"BERT for classification example training files"
"weights initialized two times"
"Picking max_sequence_length in run_classifier.py CoLA task"
"Does max_seq_length specify the maxium number of words"
"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
"Pretrained Tokenizer Loading Fails: 'PosixPath' object has no attribute 'rfind'"
"What is the best dataset structure for BERT?"
"How to run a saved model?"
"logging.basicConfig overrides user logging"
"Segmentation fault (core dumped)"
"RuntimeError: Expected object of type torch.LongTensor but found type torch.cuda.LongTensor for argument #3 'index'"
"High accuracy for CoLA task"
"_load_from_state_dict() takes 7 positional arguments but 8 were given"
"big memory occupied"
"Warning/Assert when embedding sequences longer than positional embedding size"
"Benchmarking Prediction Speed"
"BERT + CNN classifier doesn't work after migrating from 0.1.2 to 0.4.0"
"bert-base-multilingual-cased, do lower case problem"
"NONE"
"lower accuracy on OMD(Obama-McCain Debate twitter sentiment dataset)"
"Problem loading a finetuned model."
"It's possible to avoid download the pretrained model?"
"run_squad.py without GPU.. Without CUPY"
"Problem loading finetuned model for squad"
"Not able to use FP16 in pytorch-pretrained-BERT"
"Not able to use FP16 in pytorch-pretrained-BERT. Getting error **Runtime error: Expected scalar type object Half but got scalar type Float for argument #2 target**"
"bug in init_bert_weights"
"Some questions in Loss Function for MaskedLM"
"BertForQuestionAnswering: Predicting span on the question?"
"Does the final hidden state contains the <CLS> for Squad2.0"
"Embeddings from BERT for original tokens"
"Speedup using NVIDIA Apex "
"BertLayerNorm not loaded in CPU mode"
"Using large model with fp16 enable causes the server down"
"Did you suport squad2.0"
"the run_squad report \"for training,each question should exactly have 1 answer\" when I tried to fintune bert on squad2.0"
"Why not the mlm use the information of adjacent sentences?"
"Is it feasible to set num_workers>=1 in DataLoader to quickly load data?"
"AttributeError: 'BertForPreTraining' object has no attribute 'global_step'"
"Weights of BertForQuestionAnswering not initialized from pretrained model"
"Predict Mode: Weights of BertForQuestionAnswering not initialized from pretrained model"
"TypeError: Class advice impossible in Python3"
"pretrained model "
"Question about hidden layers from pretained model"
"Cannot reproduce the result of run_squad 1.1"
"How to pretrain my own data with this pytorch code?"
"What 's the mlm accuracy of pretrained model? "
"RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
"Add [CLS] and [SEP] tokens in Usage"
"run_lm_finetuning.py does not define a do_lower_case argument"
"Can we use BERT for Punctuation Prediction?"
"Weights not initialized from pretrained model"
"All about the training speed in classification job "
"Python 3.5 + Torch 1.0 does not work"
"got an unexpected keyword argument 'cache_dir'"
"BertOnlyMLMHead is a duplicate of BertLMPredictionHead"
"issue is, that ##string will repeats at intermediate, it collapses all index for mask words"
"Weight Decay Fix Original Paper"
"run_classifier.py doesn't save any configurations and I can't load the trained model. "
"Potentially redundant learning rate scheduling"
"TODO statement on Question/Answering Model"
"seems meet the GPU memory leak problem"
"HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-uncased.tar.gz (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000002456AF21710>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))"
"training new BERT seems not working"
"Add some new layers from BertModel and then 'grad' error occurs"
"Two to Three mask word prediction at the same sentence is very complex"
"What is the meaning of Attention Mask"
"Classifier example not training on CoLa data"
"AttributeError: 'NoneType' object has no attribute 'start_logit'"
"Missing softmax in BertForQuestionAnswering after linear layer?"
"error: the following arguments are required: --bert_model, --output_dir"
"How convert pytorch to tf checkpoint?"
"Pytorch-Bert: Why this command: pip install pytorch-pretrained-bert doesn't work for me"
"will examples update the parameters of bert model?"
"SQuAD output layer and the computation loss"
"Loading fine tuned BertForMaskedLM"
"Training classifier does not work for more than two classes"
"Loading fine_tuned BertModel fails due to prefix error"
"How can I get the confidence score for the classification task"
"Questions Answering Example"
"Using BERT with custom QA dataset"
"ConnectionError returned if Internet network is not stable"
"how to add new vocabulary?"
"max sentence length"
"Logical error in the run_lm_finetuning?"
"RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
"Freezing base transformer weights"
"Is BERT suitable for seq2seq tasks, such as machine translation? "
"Cleaning `~/.pytorch_pretrained_bert`"
"Why is the output bias computed separately?"
"What is get_lr() meaning in the optimizer.py"
"Fine tuning for evaluation"
"Training BERT behind a proxy server"
"Preprocessing necessary for lengthier text"
"How can I change vocab size for pretrained model?"
"padded positions are ignored when embedding position ids"
"cannot load BERTAdam when restoring from BioBert"
"Tokenization doesn't seem to match BERT paper"
"seems there is a bug in fine tuning language model"
"can you do a new release + pypi"
"Multilabel classification and diverging loss"
"Fine tuning Bert for Question answering "
"BERT tuning all parameters? "
"Error while using Apex"
"does run_lm_finetuning.py actually use --eval_batch_size?"
"Minor redundancy in model defintion?"
"please add option to load fine-tuned file to CPU if trained on GPU"
"pretrained model(s) in onnx format"
"speed becomes slow"
"potential bug in extract_features.py"
"RuntimeError: cuda runtime error while running run_classifier.py with 'bert-large-uncased' bert model"
"Variance Sources"
"Tokenization Incorrect"
"Missing files for Transformer-XL examples"
"Get hidden states from all layers of Transformer-XL?"
"Transformer-XL: hidden states are nan"
"Transformer-XL: wrong encoding in the vocab"
"Facing issue in Run Fine tune LM"
"Help: how to get index/symbol from last_hidden, on text8?"
"Argument do_lower_case is repeated in run_lm_finetuning.py"
"80min training time to fine-tune BERT-base on the SQuAD dataset instead of 24min?"
"PAD symbols change the output"
"DataParallel imbalanced memory usage"
"Have you eval the inference speed of transformer-xl?"
"Conversion of gpt-2 small model"
"unicode"
"Error in Apex's FusedLayerNorm"
"Anyone tried this model to write a next sentence?"
"HugginFace or HuggingFace?"
"Too much info @ stdout"
"Extract Features for GPT2 and Transformer-XL"
"How to change config parameters when loading the model with `from_pretrained`"
"Sudden catastrophic classification output during NER training"
"Transformer-XL: Convert lm1b model to PyTorch"
"Tests failure"
"bert.pooler.dense initialization "
"`train_dataset` and `eval_dataset` in run_openai_gpt.py"
"Example Code in README fails."
"Can I do a code reference in implementing my code?"
"Issue happens while using convert_tf_checkpoint_to_pytorch  "
"It seems the eval speed of transformer-xl is not faster than bert-base-uncased."
"Tests error: Issue with python3 compatibility, on zope interface implementation"
"Shouldn't GPT2 use Linear instead of Conv1D?"
"Problems converting TF BioBERT model to PyTorch"
"run_lm_finetuning"
"Issue with apex import on MAC "
"run_classifier.py :  TypeError: join() argument must be str or bytes, not 'PosixPath'"
"anyone notice large difference of using fp16 ?"
"TransfoXLLMHeadModel output interpretation"
"run_classifier.py: TypeError: __init__() got an unexpected keyword argument 'cache_dir'"
"what is the batch size we can use for SQUAD task?"
"how to load classification model and predict?"
"Single sentence corpus in run_lm_finetuning?"
"What should be the label of sub-word units in Token Classification with Bert"
"warmup_linear for BertAdam and OpenAIAdam"
"run_classifier with evaluation job only"
"PyTorch Huggingface BERT-NLP for Named Entity Recognition"
"run_lm_finetuning - ZeroDivisionError"
"Can we fine tune our model on Chinese corpus"
"Can BERT do the next-word-predict task? As it is bidirectional."
"Train with custom data on bert question answering"
"Add lm and next sentence accuracy for run_lm_finetuning example"
"pip install [--editable] . ---> Error"
"Feature Request: GPT2 fine tuning"
"F1 and EM scores output for run_squad.py"
"Why the weights are not intialized ?"
"optimizer.zero_grad() in run_openai_gpt.py?"
"Usage example needs [CLS] and [SEP] added post-tokenization"
"Tokenizer defaults lowercase even when bert_model is cased"
"BertEmbedding not initialized with `padding_idx=0`"
"Not able to import RandomSampler, Getting error \"ImportError: cannot import name 'RandomSampler'\"?"
"MRPC Score Lower than Expected"
"Unable to train (fine-tuning) BERT with small training set"
"Bert Uncased Large giving very low results with SQUAD v1.1 dataset"
"Little training has no impact"
"How to incrementally do fine tune train "
"Dropout Layer in OpenAIGPTMultipleChoiceHead not used"
"[Question] Best choice for Sentence Compression model? "
"How to add input mask to GPT?"
"Ranking predictions with BertForQuestionAnswering"
"Separator token for custom QA input (multi paragraph, longer than 512)"
"Potential redundancy in run_classifier.py example script"
"BERT accuracy reduced after providing custom training..The answer is also not correct"
"Vocabularly file not available for Squad predictions"
"how does the run_squad.py deal with non-answerable questions "
"When i fine tune the BERT on my serve, it always says Segmentation fault? "
"BertForQuestionAnswering: How to split  output between query hidden state and context hidden state"
"What is Synthetic Self-Training?"
"a single sentence classification task, should the max length of sentence limited to half of 512, that is to say 256"
"performance degraded when using paddings between queries and contexts."
"How to input the fine-tuned model?"
"run_lm_finetuning generates short training cases"
"Empty nbest_predictions.json for run_squad.py"
"fp16 overflow in GPT-2"
"Incrementally Train BERT with minimum QnA records - to get improved results"
"pre-training a BERT from scratch"
"run_squad.py cannot predict only"
"'NoneType' object with constructor"
"Reproduce the results on CoLA "
"AttributeError: 'BertForPreTraining' object has no attribute 'shape'"
"AttributeError: 'BertOnlyMLMHead' object has no attribute 'seq_relationship'"
"Allow do_lower_case regardless of do_basic_tokenize"
"Is the GPT-2 pretrained model language agnostic?"
"how to freeze bert model and just train a classifier?"
"How can I generate new text after having fine-tuned BERT on a custom dataset ? "
"gpt2 tokenizer issue with ValueError: chr() arg not in range(256) in Python 2.X"
"[Question]Embedding Generate Problem"
"embeddings after fine tuning"
"error when trying to get embeddings after fine tuning"
"AllenNLP TransformerXL"
"slow training speed even 20 steps"
"something wrong in example"
"Why average the loss when training on multi-GPUs"
"Possible error in \"pytorch-pretrained-BERT/examples/run_gpt2.py\" unconditional"
"Bert Pretrained model has no modules nor parameters"
"Help with implementing strides into features for multi-label classifier"
"For sequence classification, is this model using the wrong token?"
"Distributed Training Gets Stuck"
"Is there any pre-training example code?"
"can I fine-tuning pretrained gpt2 model on my corpus?"
"bug in examples/run_squad.py line 88 & 90"
"Advantage of BertAdam over Adam?"
"pytorch model to tensorflow checkpoint"
"BertForTokenClassification for NER, mask labels "
"Difference between base and large tokenizer?"
"Cannot find Synthetic self-training in this repository."
"GPT2Tokenizer <|endoftext|>"
"How to fine tune Transformer-XL on own dataset?"
"Predictions from BertForSequenceClassification model keep changing across runs"
"how to do the pre training the model form scratch?"
"Model not training at all in Google Colab"
"BertTokenizer.from_pretrained('bert-base-multilingual-cased') does not recognize Korean"
"convert_tf_checkpoint_to_pytorch 'BertPreTrainingHeads' object has no attribute 'squad'"
"DistributedDataParallel Not Working"
"How can i use bert for finding word embeddings"
"Unable to incrementally train BERT with custom training"
"How do you train custom corpus with bert?"
"if crf needed when do ner?  "
"How to select a certain layer as token's representation?"
"Dynamic max_seq_length implementation?"
"pretrain for chinese dataset"
"Convert_tf_checkpoint_to_pytorch for bert-joint-baseline"
"Understanding pre-training and fine-tuning"
"Help: cannot load pretrain models from .pytorch_pretrained_bert folder"
"Pregenerating data requires multiple documents"
"What\u2018s op-for-op meaning?"
"getting sequence embeddings for pair of sentences"
"LM fine tuning on top of a custom model"
"max_seq_length for squad"
"Load Biobert pre-trained weights into Bert model with Pytorch bert hugging face run_classifier.py code"
"Suggestion: add warning when using BertForSequenceClassification without special [CLS] token"
"Question about BertForQuestionAnswering model"
"run_classifier on CoLA fails with illegal memory access"
"Pooler weights not being updated for Multiple Choice models? "
"Vocab changes in lm_finetuning in BERT"
"How to get vocab.txt and bert_config.json as output of fine tuning?"
"Errors when using Apex"
"Mismatch in pre-processed wikitext-103 corpus and using pre-trained tokenizer for TransfoXLLMHeadModel"
"GPT-2 fine tunning"
"Why can`t we just use cached pytorch-model without internet"
"how to correctly do classifying?"
"modeling_openai.py bug report"
"Compilation terminated"
"GPT as a Language Model"
"Non-Determinism Behavior that cannot reproduce result when evaluate on each epoch"
"cannot run squad script "
"Getting Sentence level log probabilities using this model"
"Using GPT2 to implement GLTR"
"BERT does mask-answering or sequence prediction or both???"
"Suggestion: exception handling for out-of-vocab in pretrained model"
"Perplexity number of wikitext-103 on gpt-2 don't match the paper"
"KeyError: in convert_tokens_to_ids()"
"UnboundLocalError: local variable 'i' referenced before assignment when using fine_tuning code"
"Difference between this repo and bert-as-service"
"BERT multilingual for zero-shot classification"
"pretrained GPT-2 checkpoint gets only 31% accuracy on Lambada"
"no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']"
"how to use extracted features in extract_features.py?"
"UnboundLocalError: local variable 'special_tokens_file' referenced before assignment"
"error when do python3 run_squad.py"
"Test a fine-tuned BERT-QA model"
"How to obtain attention values for each layer "
"Init BertForTokenClassification from from_pretrained"
"Generating text with Transformer XL"
"GPT-2 FineTuning on Cloze/ ROC"
"How to read a checkpoint and continue training?"
"Adam optimiser not following Pytorch conventions"
"error when trying to use multilingual model for fine tuning"
"How many epochs are necessary for finetuning BERT?"
"ADD ERNIE"
"Same loss values but different eval result"
"More SEPs"
"No GPT2 model"
"unable to load finetuned LM \"No file bert_config.json\""
"extending of Transformer-XL for new tasks "
"ImportError: cannot import name 'WEIGHTS_NAME' from 'pytorch_pretrained_bert.file_utils'"
"Mixed up isNextSentence label in simple_lm_finetuning.py script? "
"Should I use weight_decay or weight_decay_rate?"
"Will BERT weights for SQuAD be released?"
"__init__() got an unexpected keyword argument 'do_basic_tokenize'"
"Why classifier fine-tuning don't save best model based on the evaluation on dev dataset"
"GPT2 training and generating on text longer than 1024"
"[Feature request] Support configurable BertLayerNorm epsilon"
"How many datasets does Bert use in pretraining process?"
"gpt2 fine tuning sources"
"New GPT2 tokenizer no longer encodes Unicode characters properly in Python 3"
"key error in BertQuestionAsnwering predict?"
"Can we use 'bert-base-uncased' to question_answer just for start, rather rather than run_squad pretraining?"
"no to_json_file(file) in BERT"
"Any way to reduce the model size to <250mb?"
"Clarifying attention mask"
"How to train our own domain-specific data instead of using pre-training models?"
"TypeError: '<' not supported between instances of 'NoneType' and 'int'"
"Import Error"
"How to get masked word prediction probabilities"
"how to ensemble different checkpoints?"
"CUDA out of memory issue when training "
"Pad inputs to multiple of 8"
"should loss_scale be multiplied to the loss explicitly?"
"How to get back input and predictions as string"
"ValueError: For training, each question should have exactly 1 answer."
"Transformer XL from Pytorch model"
"Training beyond specified 't_total' steps with schedule 'warmup_linear'. Learning rate set to 0.0. Please set 't_total' of BertAdam correctly."
"Expanding vocab size for GTP2 pre-trained model."
"can one run squad using gpt2?"
"the size of words and the size of lables do not match"
"Training Transformer XL from scratch"
"performance does not change but loss decrease"
"Results of Fine-tuned model changes in every run"
"Bug in run_classifier.py fp16 learning rate"
"about pytorch 1.1.0 rerlease"
"Fine-tuning Bert"
"License of the pretrained models"
"BERT pre-training using only domain specific text"
"GPT2 doesn't accept inputs of varying tokens length (despite the padding at the end)"
"understanding of the output from TransfoXLModel "
"Different BERT representations when text is with and without single quotes"
"key error when using run_classifier.py in predict mode, expecting label?"
"GPT2 lm_labels masking using (-1) throws an index out of range"
"\"Easy\" path for classifier training / pre-training"
"Resetting current_random_doc and current_doc"
"Bert for passage reranking"
"BertAdam gradient clipping is not global"
"Add GPT-2 Bigger Model"
"BERT + PyTorch + XLA"
"The number of train examples in STS-B is only 5749"
"Padding Token in Transformer XL"
"From which layer is fine tuning starting in BERT?"
"installation error "
"Can't save converted checkpoint"
"What is the use of [SEP]?"
"Can the use of [SEP] reduce the information extraction between the sentences?"
"Embedding' object has no attribute 'shape' "
"size mismatch for lm_head.decoder.weight"
"Unclear error message when unable to cache the model"
"[Question] Cross-lingual sentence representations"
"BERT tokenizer - set special tokens "
"Fine tuning time did not change much after freezing layers "
"How to reduce embedding size from 768?"
"Different GPT-2 outputs with mixed precision vs single precision"
"Using BERT as feature extractor"
"why use  self.apply(self.init_bert_weights) in inhiritance class\uff1f"
"How can we import cased bert model?"
"How to check the vocab size of bert large and bert small?"
"when using multiple GPUs, `loss.mean()` may have subtle bias "
"t_total"
"t_total"
"extract_features"
"How to use the fine tuned model for classification (CoLa) task?"
"Learning from scratch not working"
"Couldn't import '''BertPreTrainedModel'''"
"TransfoXLModel and TransforXLLMModel have the same example"
"How to get the softmax probabilities from the TransfoXLLMModel"
"Loss function of run_classifier.py takes in 2 inputs of different dimensions. "
"Custom data, gradient explosion, accuracy is 0"
"Question on duplicated sentence"
"In run_classifier.py, is \"warmup_proportion\" a fraction or percentage?"
"Integration with a retriever Model"
"tokenization_gpt2.py - on python 2 you can use backports.functools_lru_cache package from pypi"
"Tried to visualize the CLS Token embeddings after fine-tuning on SST-2 using t-SNE, but no clear clustered visualizations of positive and negative sentences !"
"How to use run_squad.py to produce multiple answers for a question?"
"BERT QnA is not matching correct answer when document is in QnA format"
"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
"Is loss.mean() needed?"
"from_pretrained"
"run_classifier.py:TypeError: forward() missing 1 required positional argument: 'input_ids'"
"bert->onnx ->caffe2 weird error"
"convert_tf_checkpoint_to_pytorch get different result?"
"GPT2 - support data type torch.DoubleTensor for Position embedding"
"Training Dataset of GPT-2"
"run_squad.py F1 and EM score are differ from tensorflow version "
"GPT-2 Tokenizer error!"
"Isn't it too few activations?"
"The prediction accuracy for the masked token is ZERO when using the pretrained model. Does it make sense?"
"Performing optimization on CPU"
"FileNotFoundError: [Errno 2] No such file or directory: 'uncased_L-12_H-768_A-12\\\\pytorch_model.bin'"
"RuntimeError: cublas runtime error : an internal operation failed at /pytorch/aten/src/THC/THCBlas.cu:258"
"BertAdam's get_lr() not return correct learning rate"
"No softmax activation in BertForTokenClassification"
"[Dropout] why  there is no dropout for the dev and eval?"
"fine-tuning BERT, next sentence prediction loss is not decreasing"
"RuntimeError: CUDA error: device-side assert triggered"
"Different Results from version 0.4.0 to version 0.5.0"
"use of special tokens in gpt2?"
"Use of GPT for multilingual LM"
"How to use different learning rates in the classifier example."
"SQuAD 1.1 very low evaluation score when using `--fp16`"
"Whole Word Masking Models update"
"Recommended batch size and epochs for finetuning on large data"
"How to load a existing model"
"MRPC / SQuAD stuck in \"Running training\""
"Accumulation"
"Padding in GPT-2  "
"GPT-2 medium and large release?"
"GPT2 generating repetitive text"
"when I use bert-large-uncased to load bert,runtimeError occured,but base-uncase is ok"
"`get_final_text` bug when dealing with chinese sentence"
"warmup for BertAdam"
"BERT  what's different with step and t_total"
"LM fine-tuning without NSP"
"Gradual unfreezing and discriminative fine-tuning for BERT"
"Importing TF checkpoint as BertForTokenClassificiation"
"Download the model without executing a Python script"
"Transformer XL ProjectedAdaptiveLogSoftmax bug (maybe?)"
"Why the output of models are random."
"Limit on the input text length?"
"Can BertForMaskedLM be used to predict out-of-vocabulary words?"
"Can't find gpt2 vocab file. "
"Implementation of 15% words masking in pretraining"
"How to use GPT2 to predict and fit a word into an existing sentence?"
"Failing to run pregenerate_training_data.py & finetune_on_pregenerated.py"
"Include a reference on in-domain LM pre-training for BERT"
"Have no GPU to train language modelling"
"BERT output not deterministic"
"convert_gpt2_checkpoint_to_pytorch dimensions assertion error"
"Fine tuning GPT-2 for LM objective function"
"Low SQuADv2 F1 & EM Score"
"\"Received 'killed' signal\" during the circleci python3 build after submitting PR"
"Implementing XLNet in pytorch"
"Future attention masking in GPT/GPT-2?"
"layer_norm_eps"
"A way to increase input length limitation?"
"BERT Tokenizer not working! Failed to load the bert-base-uncased model."
"TypeError: expand_as() takes 1 positional argument but 5 were given"
"BPE vocab"
"Embedding and predictions in one forward pass"
"Import Error: cannot import name 'warmup_linear'"
"Usual loss when pretraining?"
"low accuracy when fine tuning for the MRPC task with large model"
"BERT Input size reduced to half in forward function "
"Examples does not work with apex optimizers"
"Poor Training and evaluation accuracy even with low loss"
"UnicodeDecodeError:"
"Grover generator support"
"bertForNextSentencePrediction"
"GPT & GPT2: binary classification fails"
"Erroneous Code"
"BERT encoding layer produces same output for all inputs during evaluation "
"Question regarding crossentropy loss function for BERTMaskedLM "
"gpt-2 model doesn't output hidden states of all layers."
"BertTokenizer never_split issue"
"where is \"pytorch_model.bin\"?"
"How to get perplexity score of a sentence using anyone of the given Language Models?"
"Using BertForNextSentencePrediction and GPT2LMHeadModel in a GAN setup."
"Cannot reproduce results from version 0.4.0"
"Recommended multilingual bert cased model returns similar embeddings "
"GPT2Tokenizer for Hindi Data"
"BERT pretraining routine"
"Attribute Error : 'BertModel' object has no attribute 'bert'"
"Incorrect training loss scaling factor in examples/run_classifier.py?"
"Slower and more memory hungry than the TensorFlow BERT?"
"how to set the init learning rate when use bertAdam?"
"`bert-base-uncased` works for CoLA, `bert-large-uncased` always predicts one class"
"Get Attention Values for Pretrained Model"
"Invalid Syntax Error trying to run pregenerate_training_data.py"
"Simple LM finetuning falls with RunTime Error: CUDA out of memory"
"Help loading BioBERT weights"
"randrange() error when running pregenerate_training_data.py code in lm_finetuning"
"''bert-large-uncased-whole-word-masking-finetuned-squad' CAN'T be reached."
"Adding extra inputs when fine-tuning BERT"
"Is is possible to fine-tune GPT2 on downstream tasks currently?"
"Fine tune Xlnet"
"GPT-2 language model decoding method"
"XLNet tensor at wrong device issuse"
"How can I load a fine-tuned model? "
"Performance dramatically drops down without training. "
"Cannot load 'bert-base-german-cased'"
"XLNet text generation ability"
"Order of tokens in vocabulary of German model"
"Should close the SummaryWriter after using it"
"Fail to run finetune_on_pregenerated.py"
"Why the activation function is tanh in BertPooler"
"how to get the word vector from bert pretrain model ?"
"[bug] from_pretrained  error with from_tf"
"Implementation of 15% words masking would cause the drop of performance in short text "
"How to use Bert QA model for predictions?"
"bert-large config file"
"XLNet text generation ability : inference is slow"
"XLNet Embeddings"
"Issue running run_transfo_xl.py"
"BertModel docstring missing pooled_output"
"XLNet-large-cased: hyper-parameters for fine-tuning on SST-2"
"[bug]BertAdam change to AdamW in example"
"Error while adding new tokens to GPT2 tokenizer"
"attention_mask at run_squad.py"
"fp16+xlnet did not gain any speed increase "
"AssertionError in BERT-Quickstart example"
"Answers to Bullet/List Items by bert"
"Where is \"run_bert_classifier.py\"?"
"AttributeError: 'tuple' object has no attribute 'softmax'"
"GPT2 model does not have attention mask"
"Problem loading finetuned XLNet model"
"SEG_ID constants for XLNet misleading/off"
"do I need to add sep and cls token in each sequence ?"
"How to use BertModel ?"
"Is there any plan of developing softmax-weight function for using 12 hidden BERT layer? "
"from pytorch-pretrained-bert to pytorch-transformers\uff0csome problem"
"GPT sentence log loss: average or summed loss?"
"Output of BertModel does not match the last hidden layer from fixed feature vectors"
"RuntimeError: Creating MTGP constants failed"
"Couldn't reach server "
"XLNet-large-cased on Squad 2.0: can't replicate results"
"Updating simple_lm_finetuning.py for FP16 training"
"Bertology example is probably broken"
"Chinese BERT broken probably after `pytorch-transformer` release"
"Providing older documentation"
"xlnet input_mask and attention_mask type error"
"CUDA error: invalid configuration argument when not using DataParallel"
"RoBERTa support"
"AdamW does not have args warmup and t_total"
"finetune_on_pregenerate Loss.backwards() throw an error "
"Training with wrong GPU count"
"missing 1 required positional argument: 'num_classes' in 'from_pretrained'"
"git pull pytorch-transformers??"
"How to use the pretrain script with only token classification task ?"
"BertForNextSentencePrediction labels"
"run_openai_gpt.py issues with Adamw"
"Standardized head for Question Answering"
"How to restore a training?"
"AttributeError: 'BertModel' object has no attribute '_load_from_state_dict'"
"Detaching Variables"
"16 GB dataset for finetuning fail on reduce_memory "
"Issue "
"XLNET completely wrong and random output"
"adaptive softmax in transformer-xl"
"can't find utils_glue"
"Confused about the prune heads operation."
"problem when calling resize_token_embeddings"
"UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector."
"Error loading converted pytorch checkpoint"
"Get the different result at BertModel"
"modeling_xlnet.py lines 798  torch.eisum('i,d->id', pos_seq, inv_freq)"
"manually download models"
"XLMForMaskedLM"
"CLS segment_id for BERT"
"Bug of BertTokenizer"
"Deleting models"
"Bert encodings"
"PreTrainedModel.from_pretrained(...) doesn't pass cache_dir to PretrainedConfig.from_pretrained(...)"
"Using Fp16 half precision makes Bert prediction slower."
"XLnet sentence vector"
"fp16 is broken"
" module 'torch.nn' has no attribute 'Identity'"
"How to load a fine-tuned model pytorch_model.bin produced by run_bert_swag.py"
"fp16 is not work"
"Fine-tuning model and Generation"
"XLNet bidirectional input pipeline requires batch size at least 2"
"How to use BERT for finding similar sentences or similar news? "
"error when tried to migrate from pretrained-bert to transformers."
"Fail to load pre-trained tokens."
"Printing Iteration every example problem"
"can not convert_tf_checkpoint_to_pytorch"
"Upgrade to new FP16"
"Customized BertForTokenClassification Model"
"Can lm_finetuning be used with non-english data ? "
"BERT uncased model outputs a tuple instead of a normal pytorch tensor "
"No gradient clipping in AdamW"
"Increased number of hidden states returned from transformers in latest release"
"PreTrainedTokenizer.from_pretrained should be more general"
"BERT: run_squad.py falling over after eval"
"How to add new special token"
"Sequence length more than 512"
"fp16 is still has the problem"
"SpanBERT support"
"bug: it is broken to use tokenizer path"
"Torchscript Trace slower with C++ runtime environment."
"why the acc of chinese model(bert) is just 0.438"
"AssertionError while using DataParallelModel"
"cuda out of memory"
"Cannot inherit from BertPretrainedModel  anymore after migrating to pytorch-transformers"
"adding vocabulary in OpenAI GPT2 tokenizer issue"
"Best practices for combining large pretrained models with smaller models?"
"Using new pretrained model with it's own vocab.txt file. "
"Wrong layer names for selecting parameters groups (run_openai_gpt.py)"
"Avoid i/o in class __init__ methods"
"XLNet: Sentence probability/perplexity"
"Export to Tensorflow not properly implemented"
"Code snippet on docs page using old import"
"Unigram frequencies in GPT-2 or XLnet?"
"Issues in visualizing a fine tuned model"
"TypeError: 'NoneType' object is not callable"
"[RuntimeError: sizes must be non-negative] in run_squad.py using xlnet large model"
"Torchscipt mode for BertForPreTraining"
"Feature request: roBERTa"
"`do_wordpiece_only` argument"
"ERNIE 2.0 ?"
"AttributeError: 'NoneType' object has no attribute 'split'"
"Updating run_swag script for new pytorch_transformers setup"
"pip install error: \"regex_3/_regex.c:48:10: fatal error: Python.h: No such file or directory\""
"Feature Request : run_swag with XLNet and XLM "
"run_glue : Evaluating in every grad_accumulation_step if flag eval during training is true"
"XLNet large low accuracy"
"Wrong refactoring of mandatory parameters for run_squad.py"
"Performance dramatically drops down after replacing pytorch-pretrained-bert with pytorch-transformers"
"Chinese BERT broken"
"Unexpectedly preprocess when multi-gpu using"
"Using BERT for predicting masked token"
"Is pytorch-transformers useful for training from scratch on a custom dataset?"
"Missing lines in Readme examples?"
"_convert_id_to_tokens for XLNet not working"
"Using memory states with XLNet / TransfoXL"
"[XLNet] Parameters to reproduce SQuAD scores"
"How to train BertModel"
"<model>ForQuestionAnswering loading non-deterministic weights"
"CONFIG_NAME and WEIGHTS_NAME are missing in modeling_transfo_xl.py"
"How to add some parameters in gpt-2 (in attention layer) and initialize the original gpt-2 parameters with pre-trained model and the new introduced parameters randomly?"
"Bert model instantiated from BertForMaskedLM.from_pretrained('bert-base-uncased') and BertForMaskedLM(BertConfig.from_pretrained('bert-base-uncased')) give different results "
"Tokenizer added special token attributes missing"
"total training steps and tokenization in run_glue"
"Use the fine-tuned model for another task"
"Deep learning NLP models for children's story understanding?"
"How to output a vector"
"AttributeError: module 'tensorflow.python.training.training' has no attribute 'list_variables'"
" Unable to load weights properly from tf checkpoint"
"Error when running run_squad.py in colab"
"Finetune GPT2 "
"How to use   GPT2LMHeadModel  for conditional generation"
"Brackets are not aligned in the DocString of Bert."
"XLNetForQuestionAnswering - weight pruning"
"Support longer sequences with BertForSequenceClassification"
"Inconsistant output between pytorch-transformers and pytorch-pretrained-bert"
"Issue: Possibly wrong documentation about labels in BERT classifier"
"RuntimeError: bool value of Tensor with more than one value is ambiguous"
"The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False."
"How to predict masked whole word which was tokenized as sub-words for bert-base-multilingual-cased"
"Worse performance of gpt2 than gpt"
"Unable to read pre-trained model using BertModel.from_pretrained "
"Potential bug with gradient clipping when using gradient accumulation in examples"
"seq2seq model with transformer"
"Using hidden states from BERT (Similar to using precomputed hidden states in GPT2 model \"past\" argument)"
"bert-base-multilingual-uncased vocabulary not consecutive"
"Supress long sequence tokenization warning"
"Any idea how to use pytorch-transformers for Entity Linking?"
"RuntimeError:  Invalid index in gather at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:469  (GPT2DoubleHeadsModel)"
"Pretrained GPT2 mdoels does not load unk special symbol"
"BERT with sequence pairs & padding"
"Is XLA feature existed in current repo?"
"Running the pytorch.distributed.launch example of Glue hangs at evaluation"
"Multi_Head Attention in BERT different from Transformer?"
"Running on GPU?"
"How do a put a different classifier on top of BertForSequenceClassification?"
"How to make a new line when using gpt2 to generate lyrics?"
"Can't GPT-2 set special_tokens? (or unk tokens)"
"Can't get attribute 'Corpus' on <module '__main__' from 'convert_transfo_xl_checkpoint_to_pytorch.py'>"
"can somebody share an example of how to use GPT2 model for multiclass classification problem with fine tuning Language model ?"
"How can I use only one layer transformer via this repository?"
"GPT2 Sentence Probability: Necessary to Prepend \"<|endoftext|>\"?"
"Order of inputs of forward function problematic for jit with Classification models"
"run_classifier.py missing from examples dir"
"inconsistency of the model (XLNet) output  / related to #475 #735"
"XLNet / sentence padding"
"BertTokenizer.save_vocabulary() doesn't work as docstring described"
"Logic issue with evaluating cased models in `run_squad.py`"
"inconsistent between class name (Pretrained vs PreTrained)"
"the execution order of `scheduler.step()` and `optimizer.step()`"
"Fine-tuning approach for Bert and GPT2 classifiers"
"Intended Behaviour for Impossible (out-of-span) SQuAD Features"
"When I set fp16_opt_level == O2 or O3,  I can not use multiple GPU"
"\"mask_padding_with_zero\" for xlnet"
"fail to download vocabulary behind proxy server"
"puzzling issue regarding evaluation phase"
"if cutoffs=[], convert_transfo_xl_checkpoint_to_pytorch.py has a bug"
"Tokenizer not found after conversion from TF checkpoint to PyTorch"
"Efficient data loading functionality"
"GPT2 Tokenizer got an expected argument `skip_special_tokens`"
"GPT2 Tokenizer got an expected argument `skip_special_tokens`"
"Getting embedding from XLM in differnet languages"
"Customize BertTokenizer and Feature Extraction from Bert Model"
"wrong generation of training sentence pairs. method: get_corpus_line, in finetune_on_pregenerated.py"
"Adding new tokens to GPT tokenizer"
"Minor bug in evaluation phase in example code for SQUAD"
"Issue in running run_glue.py in Roberta, XLNet, XLM in latest release"
"Unable to load custom tokens"
"mnli results for BERT"
"Issue using Roberta"
"Very bad performances with BertModel on sentence classification"
"BUG: run_openai_gpt.py bug of GPTTokenizer and GPTDoubleHeadsModel"
"Error in converting tensorflow checkpoints to pytorch "
"BUG: run_openai_gpt.py load ROCStories data error"
"reproducing bert results on snli and mnli "
"simple example of BERT input features : position_ids and head_mask "
"Initialising XLMTokenizer "
"GPT2 774M weights released!"
"Example in OpenAIGPTDoubleHeadsModel can't run"
"Can't load the RobertaTokenizer from AutoTokenizer.from_pretrained interface"
"Has anyone reproduced RoBERTa scores on Squad dataset?"
"`run_squad.py` not using the dev cache"
"LM fine-tuning for non-english dataset (hindi)"
"ru language"
"Support for Tensorflow (& or Keras)"
"Missing tf variables in convert_pytorch_checkpoint_to_tf.py"
"Unable to get hidden states and attentions BertForSequenceClassification "
"can this project select the specific version of BERT?"
"Index misplacement of Vocab.txt BUG BUG BUG"
"Getting tokenization ERROR while running run_generation.py "
"hwo to get RoBERTaTokenizer  vocab.json and also merge file"
"Xlnet for multi-label classification"
"RuntimeError: Creating MTGP constants failed. at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/THC/THCTensorRandom.cu:34"
"ProjectedAdaptiveLogSoftmax log_prob computation dimensions error"
"\u2753 Why in `run_squad.py` using XLNet, CLS token is not set at the end ?"
"No such file or directory: '..\\\\VERSION'"
"Problem with mask token id in RoBERTa vocab"
"fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN2at7getTypeERKNS_6TensorE"
"Performing MRPC task after Fine Tuning"
"some words not in xlnet vocabulary ,especially name "
"modifying config"
"Support multiprocessing when loading pretrained weights"
"Missing RobertaForMultipleChoice"
"Writing predictions in a separate output file"
"evaluate bert on Senteval dataset"
"Wrong documentation example for RoBERTa"
"Roberta semantic similarity"
"How to get pooler state's (corresponds to CLS token) attention vector?"
"sample_text.txt is broken (404 ERROR)"
"Changing the _read_tsv method in class DataProcessor"
"using BERT as pretraining with custom classifier"
"keeping encoder fixed from pretrained model but changing classifier"
"Can we get a 1.1.1 release so that AutoRoberta is included?"
"Implement the QuickStart but got an error when  using BertForMaskedLM to predict a masked token"
"[Help] How to do mean/max pooling to get sentence embedding?"
"Does RoBERTa needs input_type_ids as Bert ?"
"No parameter which is presented in documentation"
"Wrong parameter name in documentation"
"Tons of warnings on use of TransfoXLModel. masked_fill_ input dtype torch.uint8 should be changed to torch.bool"
"Using pretrained XLNET for long sentences"
"PyTorch library dependency"
"Extracting Features Example"
"XLNet resize embedding size ERROR"
"UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1176: character maps to <undefined>"
"Bert initialization"
"cannot import name 'RobertaConfig"
"Fine-tuning (BERT & RoBERTa) base outperforms large"
"Output of BertModel does not match fixed feature vectors extracted from the last hidden layer  "
"mems output in XLNet"
"How to split consecutive numbers?"
"GPT2 Tokenizer decoding fails when the added tokens include a space"
"Schedulers cause memory accumulation across folds in cross-validation?"
"Cannot import DistilBert classes"
"loss explosion"
"Can't Using Binarization Script for DistilBERT"
"FP16_Optimizer is not an Optimizer when fp_16"
"Why still using old implementation of apex fp16"
"where can i assign step in function lr_lambda of Class WramupLinearSchedule?"
"How to finetune GPT2"
"Attention values occasionally exceed 1 in BertModel"
"GPT2-large fails to load the tokenizer"
"Closing bracket is missing in token_counts.py for DistilBERT"
"What is the relationship between `run_lm_finetuning.py` and the scripts in `lm_finetuning`?"
"Idea to improve DistilBERT"
"About distilled the SQuAD?"
"How to load pretraind XLM model"
"Problem with optimizers after migration"
"--seed does not change the fintuning results of the xlnet model"
"Large Memory Layers"
"[Help] how to make a constrained text generation"
"Dependency errors when trying to use gpt2 using pytorch hub."
"Roberta for NER task"
"ImportError: cannot import name 'DistilBertModel'"
"How to add new pre-trained model pytorch-transformers"
"Attribute errors with pytorch_transformers tests"
"How to use BERT or word embedding for e-commerce product classification. "
"Can't get GPT2tokenizer to load correctly"
"apex fp16 FusedLayerNorm type issues"
"Write with Transformer doesn't show 774M model?"
"May I get the details of Bert pre-train procedure?"
"How to install previous versions of pytorch-transformers "
"DistilBERT training is killed because OOM"
"DistilBERT baseline"
"DistilBERT Loss Function Choice and further query on extending to GPT2."
"'DistilBertModel' object has no attribute 'init_weights'"
"Convert RoBERTa to TF checkpoint"
"XLnet output attentions doesn't work"
"Using do_eval from run_glue.py uses the cached result"
"BertEncoder head_mask not subscript-able error when not passed"
"Roberta tokenizer fails on certain unicode characters"
"how to use 'spiece.model' to create the xlnet_tokenizer"
"Finetuning BertModel to extract textual features for VQA shows bad results"
"how to get distilbert-base-uncased-distilled-squad?"
"How to finetune DistilBERT on custom data?"
"RoBERTa/GPT2 tokenization"
"How to fine-tune xlnet on SQuAD with the parameter setting provided in the paper?"
"Distributed device ordinal question"
"Learning word-pieces garble the predictions"
"Can't trace any model with pytorch-transformers 1.2"
"the best way to cut the upper layers"
"convert_roberta_checkpoint_to_pytorch.py 514 max position?"
"How to set the token_type_ids in XLNet correctly?"
"run_squad.py predictions"
"Finetuning distilbert-base-uncased"
"How to fine tune small dataset?"
"LSTM returns nan after using the pretrained BERT embedding as input "
"Fine-tuned RoBERTa models on CPU"
"Cut off sequences of length greater than max_length= 512 for roberta"
"Is there any sample code for fine-tuning BERT on sequence labeling tasks, e.g., NER on CoNLL-2003?"
"How to set the weight decay in other layers after BERT output?"
"RuntimeError: Gather got an input of invalid size: got [2, 3, 12, 256, 64], but expected [2, 4, 12, 256, 64] (gather at /opt/conda/conda-bld/pytorch_1544199946412/work/torch/csrc/cuda/comm.cpp:227)"
"Hi there, is bert-large-uncased-whole-word-masking-finetuned-squad trained for Squad 1.0 or 2.0?"
"Citing DistilBERT"
"[RuntimeError: sizes must be non-negative]  : XLnet, Large and Base"
"Remove duplicate hidden_states of the last layer in BertEncoder in modeling_bert.py"
"Bert output last hidden state"
"Question on the position embedding of DistilBERT"
"class DistilBertForMultiLabelSequenceClassification()"
"How to deal with oov tokens with pretrained models"
"Unable to load DistilBertModel after training"
"Can't reproduce XNLI zero-shot results from MBERT in Chinese"
"\u2753 How to finetune `token_type_ids` of RoBERTa ?"
"Quick questions about details"
"Roberta for squad"
"Issue in fine-tuning distilbert on Squad 1.0"
"how to finetuning with roberta-large"
"ModuleNotFoundError in distillation/scripts/binarized_data.py"
"Special tokens / XLNet"
"Can pytorch-transformers be used to get XLM sentence embeddings for multiple languages?"
"unconditional generation with run_generation.py"
"Different performance between pip install vs. download zip code"
"breaking change"
"KnowBert"
"model_type for gpt"
"R-BERT implementation"
"Why you need DistilBertModel class?"
"Running XLNet on Squad"
"Write With Transformer adding spaces?"
"examples/lm_finetuning/simple_lm_finetuning.py crashes with cublas runtime error"
"Could you please implement a Adafactor optimizer? :)"
"Training time increased from 45 min per epoch to 6 hours per epoch in colab "
"Cannot install the library "
"XLNet tokenizer returns empty list instead of string for some indexes"
"SequenceSummary / quenstion regarding summary types"
"run_generation.py  'encode' error for gpt2 and xlnet"
"Offsets in original text from tokenizers"
"Error running openai-gpt on ROCstories"
"different results shown each time when I run the example code for BertForMultipleChoice"
"Fine-tune distilbert-base-uncased under run_glue"
"Accuracy not increasing with BERT Large model"
"How to use pytorch-transformers for transfer learning?"
"could you add an option to transfer variables from float32 to float16 in GPT2 model to reduce model size and accelerate the inference speed "
"BERT returns different embedding for same sentence"
"get NaN loss when I run the example code run_squad.py"
"How long does it take? (BERT Model Finetuning using Masked ML objective)"
"ModuleNotFoundError: No module named 'pytorch_transformers.modeling' using convert_pytorch_checkpoint_to_tf.py"
"Write with Transformer: Please, add an autosave to browser cache!"
"No language embedding weights in pre-trained xlm models. "
"'Default process group is not initialized' Error"
"connection limit of pregenerate_training_data.py"
"FineTuning using single sentence document"
"start_position=0 in utils_squad.py when span is impossible"
"Is training from scratch possible now?"
"GPT2 Tokenizer Decoding Adding Space"
"Evaluation result.txt path suggestion"
"TransfoXLLMHeadModel compatibility with pytorch 1.1.0"
"MemoryError on run_lm_finetuning.py"
"traced_model "
"Fine Tuning GPT2 on wikitext-103-raw"
"cannot import name 'XLNetForMultipleChoice' but python can import"
"Where are BERT's pretrained Embeddings loaded?"
"What is the best CPU inference acceleration solution for BERT now?"
"\u2753 Why the criterion of XLNet LMHeadModel use ignore_index = -1 ?"
"Rectified Adam + LARS"
"Getting an unexpected EOF when trying to download 'bert-large-uncased-whole-word-masking-finetuned-squad' model."
"max_len_single_sentence should be max_len - 2 for RoBERTa"
"Dataset format and Best Practices For Language Model Fine-tuning"
"Which model is best to used for language model rescoring for ASR"
"mask_tokens sometimes masks special tokens "
"Planned support for new Grover 1.5B models?"
"Best loss"
"Redundant sep_token_extra option for RoBERTa Fine-tuning"
"RoBERTa : add_special_tokens=True"
"In BertForSequenceClassification, why is loss initialised in every forward?"
"How to preprocess my own data to use RoBERTa of Multiple GPUs"
"How to predict missing word [MASK] using Robert"
"BertTokenizer provides wrong encode function for Japanese BERT"
"A sequence with no special tokens has been passed to the RoBERTa model. This model  requires special tokens in order to work. Please specify add_special_tokens=True in  your encoding."
"BertForQuestionAnswering output to predict text"
"Why does padding affect the embedding results for XLNet? Pre-padding returns different embeddings than post-padding. Which one should be used?"
"Using pytorch-transformer to reimplement the \"Attention is all you need\" paper"
"parameter never_split not added in BasicTokenizer's tokenize"
"How to build a Text-to-Feature Extractor based on Fine-Tuned BERT Model"
"A Micro BERT"
"RuntimeError: expected scalar type Half but found Float"
"Sequence Classification pooled output vs last hidden state"
"Loading errors for BERT base on GPU with PyTorch 0.4.1 "
"Is the UI code for https://transformer.huggingface.co open source? "
"pytorch-transformers returns output of 13 layers?"
"Typo in modeling_bert file"
"Optimize XLNet model to generate embedding of long documents"
"Extending `examples/` to TensorFlow"
"Why is the vocabulary of token_type_ids and input_ids shared? "
"Size mismatch when loading pretrained model"
"Examples in Colab"
"AttributeError: 'RobertaTokenizer' object has no attribute 'add_special_tokens_sentences_pair'"
"RobertaTokenizer documentation is off with the new transformers library"
"Errors when using fp16 with traced models"
"Ram utilisation of DistilBERT"
"Use PyTorch's GELU activation"
"Urgent: RoBERTa-Large-MNLI does not work for 2-way classification anymore"
"Custom models: MixUp Transformers with TF.Keras code"
"SQUAD: V2 referenced at top of Readme; V1 referenced in usage instructions"
"wwm-bert lm_finetune "
"run_tf_glue.py breaks when changing to a glue dataset different from mrpc"
"GPT and BERT pretrained models in French"
"Support for SuperGLUE fine-tune/eval?"
"How to contribute to \u201cWrite with transformer\u201d?"
"Chunking Long Documents for Classification Tasks"
"Why the RoBERTa's max_position_embeddings size is 512+2=514?"
"Is there any plan for Roberta in SQuAD?"
"Why add the arguments 'head_mask' and when to use this arguments"
"Model does not train when using new BertModel, but does with old BertModel"
"Tried to import  TFBertForPreTraining in google colab"
"considerd to add albert?"
"cannot import name 'TFBertForSequenceClassification'"
"Is it save the best model when used example like run_glue?"
"Error when calculate tokens_id and Mask LM"
"TFDistilBertForSequenceClassification - TypeError: len is not well defined for symbolic Tensors during model.fit()"
"TransfoXLCorpus requires pytorch to tokenize files"
"Confusing tokenizer result on single word"
"how to train RoBERTa from scratch"
"Issue with `decode` in the presence of special tokens"
"TFTransfoXLLMHeadModel doesn't accept lm_labels parameter"
"\u2753 How to use cached hidden states in run_generation ?"
"Built-in pretrained models location"
"Bert's keyword argument 'output_all_encoded_layers' does not exist anymore?"
"With GPT-2 is it possible to get previous word prediction?"
"Change gpt2 language model loss function"
"Masking of special tokens in masked LM finetuning."
"Generate Variable Length Text With GPT2"
"XLM add new models"
"Defining Models in TF 2.0 and Extending Them"
"Is it possible to modify the parameters in GPT-2?"
"How to speedup BERT eval"
"GPT-2 Training on non-english text"
"Batched BertForNextSentencePrediction with variable length sentences"
"migrate BertForQuestionAnswering from pytorch-pretrained-bert not produce the same result"
"How to use model.fit in GPT2 TF Model"
"Adding New Vocabulary Tokens to the Models"
"Instruction for Using XLM Text  Generations"
"run_glue.py - Import Error"
"How to install transformers with pytorch only?"
"How to replicate Arxiv-NLP but for different subject?"
"question for one parameter matrix in transformers/GPT2"
"ALBERT Model Incoming?"
"Option to upload a trained model from gpt-2-simple to use with Write With Transformer"
"Problem loading trained keras model"
"ELECTRA Model"
"GPU Benchmarking + Accumulated Optimizer for TF2"
"Replace TensorboardX with Pytorch's built in SummaryWriter"
"Problem with word prediction with GPT2"
"AttributeError: 'BertOnlyMLMHead' object has no attribute 'bias'"
"Fine-tune specific layers"
"How to return  bert self attention, so that i can do visualization??"
"GPT2 Tokenizer "
"Which model should I use for machine translation?"
"how to do next word prediction in xlnet?"
"Input length is not equal to output length?"
"TF2 Mixed Precision, XLA, Distribution"
"TFBertForSequenceClassification - Feeding List of InputExamples"
"RuntimeError: cublas runtime error : resource allocation failed"
"XLNet - Finetuning - Layer-wise LR decay"
"Performance degradation with new version of this library (inference)"
"integer representation ambuiguty in tokenizer"
"Can't replicate Language Model finetuning"
"Installation example #2 fails: cannot import name 'glue_compute_metrics'"
"nn.Transformer"
"xlm-mlm-100-1280 model is not available for download"
"DistilBert for Tensorflow doesn't work"
"questions on checkpoint and 'training_args.bin' in run_lm_finetuning.py"
"when running run_squad.py it is showing no progress . stuck after feature building"
"how to get word embedding vector in GPT-2"
"Imports for Roberta conversion appear to be outdated"
"`decoder` without bias in BertLMPredictionHead"
"How can I use a TensorFlow 2.0 model for Named-Entity-Recognition (NER)? (using TFBertForTokenClassification )"
"Visualizing the Inner Workings of Attention "
"bert ids"
"How is it possible to furthur tune gpt-2(or gpt) in a seq2seq manner?"
"Multilabel Classification with TFBertForSequenceClassification"
" RuntimeError: storage has wrong size: expected -1451456236095606723 got 1024"
"Scores using BertForNextSentencePrediction are not Interpretable."
"How much GPU memory is needed to run run_squad.py"
"Plan for Albert?"
"Write with Transformer: Changing settings on Mobile?"
"Bug when finetuning model on Squad"
"Bug in CTRL generation"
"'LayerNorm' object has no attribute 'cls'"
"data loader for varying length input"
"RuntimeError: Error(s) in loading state_dict for BertModel:"
"Much slower for inference, even when traced?"
"bert-large-uncased-whole-word-masking-finetuned-squad or BertForQuestionAnswering?"
"How can I get the transformers' parameters?"
"Does run_lm_finetuning.py finetune the entire BERT / Xlnet architecture"
"Error while fine-tuning model for GPT2"
"improve final answer extraction in utils_squad.py"
"Can you please share the pre-processed text dump of the bookcorpus and wikipediacorpus?"
"Excessively Long text_b Raises Unnecessary Warnings in `encode_plus`"
"Is encode_plus supposed to pad to max_length? "
"RuntimeError: unexpected EOF, expected 7491165 more bytes. The file might be corrupted."
"FR: Tokenizer function that can handle arbitrary number of sequences"
"model.to(args.device) in run_glue.py taking around 10 minutes. Is this normal?"
"How to load a different domain BERT-based pre-trained model?"
"Issue with XLNet pretrained model"
"What is the best way to handle sequences > max_len for tasks like abstract summarization?"
"Fine-tuning with run_squad.py, Transformers 2.1.1 & PyTorch 1.3.0 Data Parallel Error"
"Seq2Seq model with HugginFace"
"GPU Usage?"
"CalledProcessError"
"/pytorch/aten/src/THC/THCTensorScatterGather.cu:100: void THCudaTensor_gatherKernel(TensorInfo<Real, IndexType>, TensorInfo<Real, IndexType>, TensorInfo<long, IndexType>, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 3]: block: [4,0,0], thread: [319,0,0] Assertion `indexValue >= 0 && indexValue < src.sizes[dim]` failed."
"Main and train for CTRL model"
"Unable to import TF models"
"Predefined token classification "
"Accuracy drop in finetuning roBERTa"
"Changelog"
"Downloading model in distributed mode"
"When to support Albert?"
"Why the codes of training BERT from scratch are deprecated "
"Question on AllenNLP vocabulary and huggingface BERT out of sync"
"Understanding run_glue in distributed mode"
"Alignment of tokens - 'extract_features_aligned_to_words' from fairseq roberta?"
"Training GPT or GPT-2 from scratch"
"Question about hidden states in GPT2"
"Hight CPU and low GPU on XLNet"
"Plan to support UniLM ?"
"why xlnet requires a long prompt for short inputs while Bert does not ? "
"'BertForSequenceClassification' is not defined   'DUMMY_INPUTS' is not defined"
"Add vocabulary gives sequence length warning"
"run_ner.py file with Distill Bert"
"Why the output of DistilBertModel is inconsistent with BertModel?!"
"Penalize high confident false negative classifications?"
"Behavior of Masked-LM BERT, dependence on masked token"
" Fine-tune RoBERTa on WikiText-2"
"A couple of noob-to-transformers questions"
"Should the option to run on TPU in run_glue.py use some sort of xla data parallelizer ? "
"Type of model for each GLUE task"
"Running CTRL Model On Google Colab Environment"
"Where is pytorch-pretrained-BERT?"
"the num_labels in run_squad"
"Adding new tokens to uncased tokenizers - case insensitivity is lost"
"Q / Note: BERT Masked-LM fails to predict last token in sequence if it is not punctuation"
"Is it possible/is there a plan to enable continued pretraining?"
"training BERT from scratch for native language PT-BR? Without init weight"
"There is not space after generating an 'special token' and the next word using gpt2."
"GPT2 not in modeltype"
"Does the function of  'evaluate()' change the result?"
"Tuning BERT on our own data set for multi-class classification problem"
"unable to parse E:/litao/bert/bert-base-cased\\config.json as a URL or as a local path"
"Compatibility between DistilBert and Bert models"
"Finetuning OpenAI GPT-2 for another language."
"[CLS] & [SEP] tokens missing in documentation"
"training BERT on coreference resolution "
"The implementation of grad clipping is not correct when gradient accumulation is enabled"
"ALBERT: will it be supported?"
"How to add the output word vector of bert to my model"
"error load bert model \uff1anot found model file"
"TFRobertaForSequenceClassification fails on TPU on Transformers >2.0.0"
"Pytorch Transformers no longer loads SciBert weights, getting `UnicodeDecodeError`. Worked in pytorch_pretrained_bert"
"Can we generate multiple possible sentences using GPT?"
"GPT2 attention mask and output masking"
"Why the output is same within a batch use BertForSequenceClassification?"
"use gpt2 as a seq2seq model"
"evaluating on race dataset with checkpoints fine tuned on roberta with fairseq"
"distilled gpt2 to be added to run_generation and run_lm_fintuning"
"seq2seq with gpt2"
"Is there a computation/speed advantage to batching inputs into `TransformerModel` to reduce its number calls"
"How does arg --vocab_transform help in extract_distilbert.py?"
"Question answering for SQuAD with XLNet"
"AdamW requires torch>=1.2.0"
"Sequence to sequence with GPT model "
"Using HuggingFace pre-trained transformer to tokenize and generate iterator for a different text than the one it was trained on"
"Error when trying to reuse hidden states in CTRL"
"Make benchmark more flexible (TF or PT)"
"Using HuggingFace TransfoXLLMHeadModel() with custom Torchtext vocabulary"
"How to use BERT for ENTITY extraction from a Sequence without classification in the NER task ?"
"_tokenize() got an unexpected keyword argument 'add_prefix_space' in CTRL"
"Issue in Cost Function"
"None in openAi-gpt tokenization"
"Support for gpt2-medium, gpt2-large and distilgpt2 in pytorch-pretrained-bert 0.6.2"
"Show pretrained model and config file download address directly in README.md & doc"
"failed to download pretrained weights"
"Can the prefix for GPT-2 conditional sampling be very long (longer than context window size)?"
"How can I get the probability of a word which fits the masked place?"
"Slight different output between transformers and pytorch-transformers"
"CUDA error: device-side assert triggered(pretrained_model.cuda())"
"run_generation.py example for a batch"
"Add T5 model"
"Format problem when training DistilBert"
"AttributeError: 'BertForPreTraining' object has no attribute 'classifier'"
"'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
"tokenization slow"
"Fine-tuning BERT using Next sentence prediction loss"
"--cache_dir argument in run_lm_finetuning.py not used at all"
"What is currently the best way to add a custom dictionary to a neural machine translator that uses the transformer architecture?"
"Loading pretrained RobertaForSequenceClassification fails, size missmatch error"
"Perm Mask in XLNet"
"cannot import name 'RobertaForTokenClassification'"
"Loading from ckpt is not possible for bert, neither tf to pytorch conversion works in 2.1.1"
"How to initialize AdamW optimizer in HuggingFace transformers?"
"Training DistilBert - RuntimeError: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237"
"AttributeError: 'CTRLTokenizer' object has no attribute 'control_codes'"
"Installation error :Command \"python setup.py egg_info\" failed with error code 1"
"how can I pre-training my own model from the existed model or from scratch"
"Add Transformer-XL fine-tuning support."
"Why DistilBertTokenizer and BertTokenizer are creating different number of features?? "
"How to use custom built Torchtext vocabulary with HuggingFace TransfoXLLMHeadModel?"
"How to compute loss with HuggingFace transformers?"
"how to use  BertForMaskedLM"
"Maximum length of out put generated in run_generation.py is of length (1021 ) despite changing position id length and length parameter"
"Error while importing RoBERTa model"
"Undefined behavior"
"distilroberta-base unavailable in pip install transformers"
"Changing LM loss function"
"ALBERT"
"Custom language text generation"
"How to set local_rank argument in run_squad.py"
"Missing required argument 'mode' in run_ner."
"No way to control ID of special chars e.g. mask IDs"
"Can I load a CTRL model that was fine-tuned using the Salesforce code?"
"Missing a line in examples/distillation/README.md"
"Parallel data preprocessing for distillation"
"How to fine tune xlm-mlm-100-128 model."
"How is the interactive GPT-2 implemented?"
"How to fine-tune CTRL?"
"BERT multi heads attentions"
"Tokenizer.tokenize return none on some utf8 string in current pypi version"
"Problem with restoring GPT-2 weights"
"Moving model from GPU -> CPU doesn't work"
"Allowing PR#1455 to be merged in the master "
"Question: Token sequence length longer maximum sequence length"
"How to load trained model of distilbert"
"Quick Tour TF2.0 Training Script has Control Flow Error when Replacing TFBERT with TFRoberta"
"Is HuggingFace TransfoXLLMHeadModels trainable from scratch?"
"BertModel.from_pretrained is failing with \"HTTP 407 Proxy Authentication Required\" during model weight download when running behing a proxy"
"possible issues with run_summarization_finetuning.py"
"Any example of how to do multi-class classification with TFBertSequenceClassification"
"\ud83c\udf1f BART"
"i want to use bert pre-trained modle in a text classification problem which the text with Multi-label. But\uff0cthere are some problems ."
"Download assets directly to the specified cache_dir"
"Error when creating RobertTokenizer for distilroberta-base"
"Wrong Roberta special tokens in releases on GitHub"
"Access denied to pretrained GPT2 model"
"Unpickling errors when running examples"
"OpenAIGPTDoubleHeadsModel Not working (even with the official example...)"
"request for a Bert_base uncase model.bin file"
"fine tuning bert and roberta_base model"
"Can't export TransfoXLModel model"
"T5"
"ALbert Model implementation is finished on squad qa task. but some format is different with huggingface.(specify on albert qa task)"
"TFXLNet int32 to float promotion error"
"TFXLNet Incompatible shapes in relative attention"
"Invalid argument error with TFRoberta on GLUE"
"add_tokens() leading to wrong behavior"
"Why do I get 13 hidden layers?"
"Best practices for Bert passage similarity. Perhaps further processing Bert vectors; Bert model inside another model "
"When I uesd gpt2, I got a error"
"Interpretation of output from fine-tuned BERT for Masked Language Modeling."
"loss is nan, for training on MNLI dataset"
"unable to import from utils_squad "
"Regression Loss"
"bugs with run_summarization script"
"The id of the word obtained by tokenizer.encode does not correspond to the id of the word in vocab.txt"
"CTRL does not react to the \"seed\" argument"
"transformers module doesn't work with torch compiled on Cuda 10.0?"
"Scheduler documentation blocks subtly wrong"
"How to mask lm_labels and compute loss? --- Finetune gpt2: masking the lm_labels with '-1' and padding increase the perplexity a lot! "
"How to train from scratch"
"Hello, how to upload a .ckpt file in TFBertForSequenceClassification?"
"Can we fine tune GPT2 using multiple inputs?"
"run_generation.py Runtime error"
"BUG for XLNet: Low GPU usage and High CPU usage, very low running speed!"
"GPT2 text generation repeat"
"Exceeding max sequence length in Roberta"
"loss is nan, for training on MNLI dataset"
"glue_convert_examples_to_features not working if no task is provided"
"Roberta embeddings comparison"
"resize_token_embeddings doesn't work as expected for BertForMaskedLM"
"Threads running on evaluation?"
"TFBertForSequenceClassification.from_pretrained ERROR"
"\ud83c\udf1fNew model addition: VL-BERT"
"BART"
"Out of Memory (OOM) when repeatedly running large models"
"F1 socre is zero while loss is about 0.12xx when using run_ner.py to fine tuning bert model"
"How to stop wordpiece-tokenizing in BertTokenizer?"
"How to use gpt-2-xl with run_generation.py"
"Released OpenAI GPT-2 1.5B model"
"gpt2 generation crashes when using `past` for some output lengths"
"How to calculate memory requirements of different GPT models? "
"input token embedding issues"
"Subtokens in BPE in GPT2"
"Out of Memory Error (OOM) only during evaluation phase of run_lm_finetuning.py and run_glue.py"
"How to add weighted CrossEntropy loss in sequence classification task?"
"Is it okay to define and use new model by using only a part of full GPT model blocks? Or anyone tried to do so?"
"Is it possible fine tune XLNet?"
"the BertModel have the class \"BertForTokenClassification\", why XLNetModel don't have the class"
"NameError: name 'DUMMY_INPUTS' is not defined"
"'RuntimeError: CUDA error' is occured when encoding text with pre-trained model on cuda"
"Roberta Positional Embeddings Not Masked"
"Perplexity for (not-stateful) Transformer - Why is it still fair to compare to RNN?"
"GPT2 - XL"
"BERT: Uncased vocabulary has 30,552 tokens whereas cased has 28,996 tokens. Why this difference?"
"[XLM-R] by Facebook AI Research"
"Error when Fine-tuning XLM on SQuA"
"For HuggingFace GPT2DoubleHeadsModel, is there a way to directly provide a hidden state for an input? "
"pip install transformers not downloading gpt2-xl"
"Extracting the output layer of HuggingFace GPT2DoubleHeadsModel"
"Could you support albert?"
"Multi GPU dataparallel crash"
"Problems when restoring the pretrain weights for TFbert"
"Dose the file /examples/run_lm_finetuning.py provide a demo to pre-train a BERT"
"    model = GPT2LMHeadModel.from_pretrained(args.model_path) try loads in json format"
"How to measure similarity of words?"
"Unclear documentation for special_tokens_mask"
"\"Write with Transformer\" source code?"
"a BertForMaskedLM.from_pretrained error"
"Invalid argument with CTRLModel"
"BertForNextSentencePrediction is giving high score for non similar sentences ."
"BertForMultipleChoice QuickTour issue with weights?"
"transformers vs pytorch_pretrained_bert giving different scores for BertForNextSentencePrediction"
"token indices sequence length is longer than the specified maximum sequence length "
"MNLI: BERT No Training Progress"
"Confused by GPT2DoubleHeadsModel example"
"RuntimeError: Connection timed out in Single node Multi GPU training"
"Add an LSTM and CNN layer on top of BERT embeddings for sentiment analysis task"
"Exact and F1 score do not increase when fine-tunes XLM on the SQuAD dataset"
"Exact and F1 score do not increase when fine-tunes XLM on the SQuAD dataset"
"run_glue.py RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:3"
"pip cannot install transformers with python version 3.8.0"
"RuntimeError: CUDA error: device-side assert triggered"
"Extracting First Hidden States"
"Whether it belongs to the bug of class trainedtokenizer decode?"
"XLMForSequenceClassification - help with zero-shot cross-lingual classification"
"Why do language modeling heads not have activation functions?"
"NameError: name 'DUMMY_INPUTS' is not defined - From TF to PyTorch"
"LM Fine-tuning for XLNET?"
"computing self-attention for tokens in a sentence"
"Best way to fine tune GPT-2 in order to create a custom text generator?"
"Using multiple inputs for GPT-2"
"Trouble running fine tuned language model script"
"CUDA out of memory on loss.backward when fine-tuning GPT2 (117M)"
"Is there a way of finetuning DistilGPT2?"
"Generated text makes no sense. Trying to auto-generate sentences like https://transformer.huggingface.co/doc/distil-gpt2"
"how to use convert_pytorch_checkpoint_to_tf2.py "
"Issue testing run_squad.py example"
"Regarding Fine-Tuning for Abstractive Summarization"
"How to get all layers(12) hidden states of BERT?"
"GPT2 tokenizer is so slow because of regex.findall"
"NotImplementedError when using TFDistilBertModel"
"GPT2 tokenizer is so slow because of sum()"
"Where is Model2Model PreTrainedEncoderDecoder in run_summerization_finetune"
"Parallell compute failing for finetuning GPT2 using GPU"
"Model parallelism support?"
"Error started happening today: ImportError: cannot import name 'get_linear_schedule_with_warmup'"
"resize_token_embeddings not implemented for TFBert* "
"BPE error when fine-tuning a CTRL model"
"xlm-mlm-17-1280 model masked word prediction"
"\"This tokenizer does not make use of special tokens.\" warning"
"summary_type value  of SequenceSummary is incorrect"
"CUDA runtime error (59) : device-side assert triggered "
"Can't run convert_roberta_original_pytorch_checkpoint_to_pytorch.py"
"The acc of RACE is always low by roberta model"
"XLNet model params for Question answering "
"run glue problem"
"Some questions about the abstractive summarization code."
"multitask learning "
"XLM Masked Word Prediction"
"Conversion of [Model]ForSequenceClassification logits to probabilities"
"save all 12 layers outputs for each token"
"How do I train OpenAIGPTDoubleHeadsModel from scratch?"
"tensorflow2.0 does not has mean, but reduce mean"
"Run bert for multi-classification but loss never decrease"
"BertForTokenClassification for NER . what is the conclusion of  this output ?"
"How to fine-tune BERT on a large training dataset?"
"Pre-training a smaller version of BERT on own data"
"Understanding feature creation "
"Disparitry with Fairseq Roberta implementation for predicting the mask token"
"Clarifications about the Quick-tour of the fine-tuning scripts?"
"Can the HuggingFace GPT2DoubleHeadsModel either for regular language modelling or solving multiple-choice questions? or is it only for solving multiple-choice questions?"
"Is this a bug? "
"run_squad hangs for small max_seq_length"
"question about 'add_prefix_space' of encode method"
"XLNetForSequenceClassification and CLS token"
"F score 0 in combining RoBERTa and BiLSTM "
"Wrong definition of the `logging_steps` parameter at the `run_lm_finetuning.py`"
"GPT2 Tokenizer Special Token ID Bug"
"save_pretrained on CamembertTokenizer"
"Using GPU for gpt2-xl"
"Is there a straightforward way to classify documents at the sentence level, while using surrounding sentences for context?"
"error on bert.fit for Squad dataset"
"`overwrite_cache` argument in `run_lm_finetuning.py` not used at all"
"Tokenizing/Loading Data for GPT-2 (1 example per line)"
"Distilling GPT2 with gives OOM"
"Is the usage of scheduler described in README correct?"
"Classify entities - run_ner script "
"Can GPT2DoubleHeadsModel be used for regular next token prediction task without adjusting its head?"
"Methods get_input_embeddings() and set_input_embeddings() appear in documentation but not available."
"Typo in Documentation for GPT2LM Output \"past\""
"run_summarization_finetuning.py"
"Documentation error in GPT2Tokenizer"
"lm_fine-tuning on small dataset of 3 documents"
"Training transformer XL from scratch with my own dataset"
"Passing inputs to TFGPT2LMHeadModel results in error: 'TensorSliceDataset' object has no attribute 'shape'"
"Bart Tokenizer treat symbols in a word as a new word. "
"XLNet is getting slower when enabling mems"
"Some Questions about XLNet "
"How to perform common sense reasoning task with GPT-2?"
"Any plan to include BART and T5? "
"Truncating GPT2 past"
"run_squad.py not running"
"CTRLTokenizer not consistent with the fastBPE tokenizer used in Salesforce/CTRL"
"FileNotFoundError when running run_squad.py"
"Step restarts from step 0 when reload from an existing checkpoint?"
"TypeError: convert_examples_to_features() got an unexpected keyword argument 'sequence_a_is_doc'"
"Need a Restore training mechenisim in run_lm_finetuning.py"
"How to process ARC dataset with HuggingFace GPT2"
"Mask probability in run_lm_finetuning.py"
"configuration of the optimizer "
"BERT bertviz"
"Using model output by transformers (v2.0) in older versions (0.4.0 or 1.0.0) "
"Using GPT-2 XL"
"Can I use HF XLNet to make a Model that Predicts Backwards?"
"Download model too slow, is there any way"
"attention_mask added, not multiplied ... is this correct?"
"how to output specific layer of TFBertForSequenceClassification, or add layer?"
"access to the vocabulary"
"Load output file from fine-tuned bert language model"
"Abruptly model training was stopped"
"NER - sciBERT weights not initialized."
"Wrong paraphrase in the TF2/PyTorch README example."
"git@github.com: Permission denied (publickey) when fetching"
"When using the Bert model"
"Expected object of scalar type Byte but got scalar type Bool for argument #2 'mask'"
"Should I use `attention_mask`?"
"Can i train my own text corpus"
"word or sentence embedding from BERT model"
"Benchmark not replicable"
"the output type of TFBertModel is weird"
"BertForMultipleChoice"
"run_squad.py crashes during do_eval"
" get_linear_schedule_with_warmup Scheduler"
"Do we need to add [CLS] and [SEP] for BertForMaskedLM ?"
"run_ner.py --do_predict inference mode errors. Right data format?"
"Improving model saving and resuming"
"Problems with running 'run_lm_finetuning.py' with bert"
"TFBertModel ValueError: Tried to convert 'dims' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor"
"Did the underlying pre-trained models change somehow?"
"How to increase model saving checkpoint from 50 to 1000?"
"XLMForTokenClassification"
"Trouble running 'bert-base-multilingual-cased'"
"AlbertPreTrainedModel class is not available in release v2.2.0"
"Bert Tensor Dimensions"
"How to persist cloud-based transformers"
"Changes to S3 Roberta / RobertaForSequenceClassification"
"Albert Hyperparameters for Fine-tuning SQuAD 2.0"
"How can we view different versions of documentation?"
"'convert_tf_checkpoint_to_pytorch.py' file is missing"
"Modify position_embeddings from pre_trained model"
"AlbertForQuestionAnswering "
"Transformers for WebNLG tasks "
"add special tokens"
"run_squad.py for tf"
"Fine Tuning Bert for Q&A"
"Possible error in the HuggingFace Transformers documentation?"
"Will you add XLNet text-generation feature ?"
"When training QA models, albert-xxlarge-v2 uses much more GPU mem than Bert-large"
"Facing AttributeError: 'DataParallel' object has no attribute 'resize_token_embeddings'"
"Worse F1 on squad2 with finetune+distil distilroberta-base than just finetune"
"Why is the weight of linear layer tied to the input embeddings in OpenAIGPTLMHeadModel?"
"XLnet output_attentions=True raises an exception"
"ALBERT is missing from AutoClasses"
"How to get a spiece.model from customize chinese vocab.txt in Albert xlnet ?"
"Training masked language model with Tensorflow"
"Wrong tokenization in Transformer-XL documentation"
"GPT2: how to construct batch for Language Modeling"
"Where I could find the vocab.json for XLNet"
"Can we use tf.keras.mixed_precision.experimental.set_policy ?"
"tf.keras.mixed_precision.experimental.Policy"
"[ALBERT]: 'AlbertForMaskedLM' object has no attribute 'bias'"
"Expand run_lm_finetuning.py to all models"
"Reason for using einsum in xlnet?"
"Changing the docs as per Pytorch v1.1+"
"How to output the vectors of the last four layers of BERT_Model."
"What is the real parameters to weight the triple loss (L_{ce}, L_{mlm}, L_{cos}) in DistilBert?"
"[CamemBERT] Add CamembertForQuestionAnswering"
"GPT-2 finetuning with run_lm_finetuning.py script"
"How to use GPT-2 text generator in spanish"
"FileNotFoundError: [Errno 2] No such file or directory: 'data/dump.txt'"
"[CamemBert] Tokenizer function add_tokens doesn't work"
"Camenbert length Tokenizer not equal config vocab_size"
"save as tensorflow saved model format and how to inference?"
"How to convert the ALBERT tfhub model to pytorch model?"
"Is it possible to fine-tune models on TPUs using TensorFlow?"
"[ALBERT] : ValueError: Layer #1 (named \"predictions\") expects 11 weight(s), but the saved weights have 10 element(s)."
"How to convert a tf2 pre-trained model to pytorch model?"
"Does GPT2LMHeadModel need <|startoftext|> and <|endoftext|> tokens?"
"Tokenization differs for different intepreter instances"
"[CamemBERT] Potential error in the docs"
"gpt-2 generation examples"
"cannot import name 'WEIGHTS_NAME'"
"Typo in modeling_albert.py for mask_token"
"Any workaround to extend the embeddings on TFGPT2DoubleHeadsModel?"
"run_lm_finetuning.py script CLM inputs and labels preparing"
"Doubts on modeling_gpt2.py"
"error"
"how to select best model in run_glue"
"run_squad with xlm: Dataparallel has no attribute config. "
"Meaning of run_lm_finetuning.py output"
"XLM-R Support"
"How do I load a pretrained file offline?"
"UnboundLocalError: local variable 'extended_attention_mask' referenced before assignment"
"Missing xlm-mlm-100-1280"
"Tokenization in quickstart guide fails"
"Changing the number of hidden layers for BERT"
"ModuleNotFoundError: No module named 'git'"
"[CamemBert] About SentencePiece training"
"Missing \"do_lower_case\" action for special token (e.g. mask_token)"
"Crosslingual classification with XLM, loss does not converge"
"Find dot product of query and key vectors "
"cannot import name 'get_linear_schedule_with_warmup' from 'transformers.optimization'"
"`distilroberta-base` link missing"
"Automatically allocates memory in GPU, always OOM when create TFALBERT model"
"How to run a batch of data through BERT model?"
"BertForSequenceClassification' object has no attribute 'bias"
"TypeError: argument of type 'PosixPath' is not iterable (in modeling_utils.py)"
"[ Structure of LM vocab trained from scratch ]"
"CPU RAM out of memory when detach from GPU"
"Save model for tensorflow serving"
"XLMWithLMHeadModel forwarding questions"
"The generation script could fail when there's a double space in the prompt"
"Accessing roberta embeddings"
"How to structure text data to finetune distilGPT2 using tf.keras.model.fit()?"
"Text Generation in Hebrew"
"How to average sub-words embeddings to obtain word embeddings?"
"Encoding special tokens"
"ImportError: cannot import name 'WarmupLinearSchedule'"
"ALBERT how to obtain the embedding matrix?"
"CUDA out of memory for 8x V100 GPU"
"Write With Transformer: PPLM document is stuck"
"\"Only evaluate when single GPU otherwise metrics may not average well\""
"How can I get similarity matching ?"
"Help with converting fine-tuned PT model to TF checkpoint"
"Use run_lm-finetuning on tpu"
"AssertionError in official example "
"Error msg when running on the colab"
"When I use albertModel, it prints the following repeatedly."
"How to save a model as a BertModel"
"Can't get gradients from TF TransformerXL model forward pass"
"The added tokens do not work as expected"
"about the special tokens"
"Understanding output of models and relation to token probability"
"which special token is used to predict the score in roberta?"
"Unclear how to decode a model's output"
"How to pretrain BERT whole word masking (wwm) model?"
"Is there any way to treat the whitespace characters same as other characters when tokenizing?"
"Having trouble reproducing SQuAD 2.0 results using ALBERT v2 models"
"RobertaTokenizer runs slowly after add _tokens"
"I am running bert fine tuning with cnnbase model but my project stops at loss.backward() without any prompt in cmd."
"Error in TFBertForSequenceClassification"
"unable to load the downloaded BERT model  offline in local  machine . could not  find config.json and  Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] |"
"Could not run run_ner.py based on XLNET model"
"XLM model masked word prediction Double Language "
"Running run_lm_finetuning.py within python"
"Split models to multiple GPUs"
"Couldn't reach server at '{}' to download vocabulary files."
"Encoder-decoders in Transformers "
"Could convert_pytorch_checkpoint_to_tf2.py convert any pytorch model to tf2?"
"Finetune and generate text with BertForMaskedLM"
"BertModel.from_pretrained() doesn't accept pathlib.PosixPath anymore"
"\"Write With Transformer\" interface returning 502 on gpt2/xl model"
"Transformers for Tabular data extraction - e.g., wikitables"
"Is there a way to evaluate models during training in Multi-gpu setting"
"DistilmBERT training/distillation dataset"
"Model2Model: RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool"
"Where is extract_features.py and run_classifier.py ?"
"In which directory the downloaded roberta-base models will be stored on linux server conda environment"
"`bert-base-uncased` tokenizer broke around special tokens in v2.2.1"
"Refactor functionality of run_squad and squad_utils into XXXForQuestionAnswering"
"Is there support for TensorflowJs?"
"is the tokenization broken for bert?"
"Tokenization in C++"
"encode_plus not returning attention_mask and not padding"
"About Summarization "
"return_tokens_mapped_to_origin not working"
"Fine-tuning distilled GPT-2"
"master branch examples/run_squad.py: missing --predict_file argparse argument"
"the docs pretrained models is missing"
"Recommended way for creating distillBERT container and serving "
"RoBERTa tokenization: Why do we call 'all_special_tokens' in each tokenize loop?"
"RoBERTa tokenization: Why do we call 'all_special_tokens' in each tokenize loop?"
"RoBERTa/GPT-2 tokenization: Why we call all_special_tokens for each token in split_all_tokens?"
"BertAbs decoder_input_ids"
"AlBERT UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
"Special Tokens are Split by BPE"
"End-Task Distillation with DistilBERT"
"How to find the  corresponding download models from Amazon?"
"gpt-2 implement issue"
"Low ROUGE scores for BertSum"
"Adding model type to config.json"
"pad_to_max_length param is not supported in PreTrainedTokenizer.encode "
"PreTrainedEncoderDecoder on tensorflow"
"How to do the further pretraining ?"
"using run_squad.py for predict and specifying config_name as path, config.json not found"
"CUDA error at 'cublasSgemm' when using the pretrained BERT"
"How to structure input data for training TFGPT2LMHeadModel using model.fit() in TF2.0?"
"Small run_squad nit: eliminate trailing \"_\" in \"best_predictions_.json\" when no prefix"
"RobertaTokenizer token type issue"
"run_squad.py for SQuAD2.0 have bad f1 score"
"Should I always use bert as a teacher to distillation distilbert as a student?"
"Pretty sure patch in Pull Request #1313 is incorrect"
"Conda version is not the latest"
"sts-b task score is far worse than other GLUE tasks"
"Unit of the prediction scores of a language model"
"T5Tokenizer: Using cls_token, but it is not set yet."
"RuntimeError: CUDA error: device-side assert triggered when using Roberta"
"summarization code is incomplete"
"Output diverging on different GPUs using same prompt?"
"About QuestionAnswering on SQuAD2.0 Dataset"
"Fine-tuning GPT2 or BERT and adding new vocabulary?"
"Error while saving Pretrained model for Encoder and decoder"
"XLNet fine-tuning speed (Multi-label classification)"
"How to output labels for GLUE test set"
"How to add traditional features for transformers?"
"run_ner.py example fails"
"weights not initialised in pre-trained Roberta"
"PRs error which occurs many times in the last days"
"Segmentation fault when GPT2-chinese import transformers"
"Transformers Encoder and Decoder Inference"
"```glue_convert_examples_to_features``` for sequence labeling tasks"
"```glue_convert_examples_to_features``` for sequence labeling tasks"
"training a new BERT tokenizer model"
"Fine-tuning TF models on Colab TPU"
"T5 - Finetuning of an EncoderDecoder Model"
"XLM run_squad errors with size mismatch"
"Return overflowing tokens if max_length is not given"
"Error while loading Pretrained Enocder and Decoder transformers "
"When i run the script run_tf_ner.py, i got ValueError: Expected floating point type, got <dtype: 'int32'>."
"tokenizer of bert-base-uncased gives an incorrect split"
"Can we remove force_download=True from tests?"
"Need pretrained XLNET on Squad which can be loaded from_pre_trained"
"Trouble loading Albert model"
"what is the most efficient way to store all hidden layers' weights?"
"The code used to be clean..."
"Supoort loading model weights from a single file."
"Removing redundant model weights"
"Readme installation/test order can lead to confusion when running example unit tests"
"TFDistilBertModelTest.test_pt_tf_model_equivalence thrown while merging after PR"
"How to load the finetuned model for retraining from checkpoints in run_squad.py?"
"BertTokenizer / CamemBERTokenizer `decode` behaviour ?"
"Training dataset is not available"
"Recently added pipelines tests should be marked as slow"
"NER pipeline missing start/end"
"Extract features aligned to tokens from a BertForQuestionAnswering model"
"Four tests fail when running the full test suite"
"AttributeError: 'Sst2Processor' object has no attribute 'tfds_map'"
"Documentation link broken"
"bias weights not used in T5Model"
"Untrainable dense layer in TFBert. \"WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\""
"HuggingFace transformers documentation webpage is blank?"
"run_ner.py load checkpoint issue"
"problem in the doc, in the  \"Quick Start\" GPT2 example"
"AlbertTokenizer behavior doesn't match docs"
"How to do_predict on run_glue?"
"BertModel sometimes produces the same output during evaluation"
"Only the Bert model is currently supported"
"Imports likely broken in examples"
"Does Pre-Trained Weights Work Internally in pytorch? "
"Bad F1 Score for run_squad.py on SQuAD2.0"
"Run_tf_ner.py error on TPU "
"adding special tokens after truncating in run_lm_finetuning.py "
"AttributeError: 'GPT2LMHeadModel' object has no attribute 'generate'"
"Gpt2/xl Broken on \"Write With Transformer\" site"
"Does the calling order need to be changed?"
"where is the script of a second step of  knwoledge distillation on SQuAD 1.0?"
"Help with finetune BERT pretraining "
"Do anyone have solution for this"
"Maybe some parameters are error in document for distributed training ?"
"Loading sciBERT failed"
"[ALBERT]: Albert base model itself consuming 32 GB GPU memory.. "
"BertTokenizer custom UNK unexpected behavior"
"Do Hugging Face GPT-2 Transformer Models Automatically Does the Absolute Position Embedding for Users?"
"Train custom NER model with new Pipeline"
"Is there any efficient way to convert BERT outputs to fit token-level tasks?"
"How do you handle large documents?"
"A question about BERT position embedding."
"RunTimeError in \"run_summarization\": expected device cuda:0 and dtype byte but got device cuda: 0 and dtype Bool"
"Why cosine similarity of BERT, ALBERT, Robert is so big, almost near 1.0?"
"Model2Model inference"
"run_ner.py RobertaForTokenClassification.from_pretrained \"size mismatch for classifier.bias\""
"Can I use run_lm_finetuning.py for training models in an uncovered language?"
"Repetition penalty work falsely in case the logit of the token is negativ"
"Why are you getting just the last encoder states in the summarization code?"
"[CLS] token /  is used as the aggregate sequence representation for classification tasks"
"Non-Deterministic Behavior in BertTokenizer"
"What's the exact name of BERT large in results ( GermEval 2014)?"
"pytorch_pretrained_bert giving different scores for BertForNextSentencePrediction "
"Bug: Tokenization of Special Tokens"
"Can I use BERT / gpt-2 for text generation"
"Is there a uncased gpt2?"
"How can I read my bert model by using transformers?"
"help: couldn't find such vocabulary files at this path or url"
"how to do a simple multi-classifier by bert 2.0,training set ,and label set all lines"
"Bert Decoder using is_decoder and encoder_hidden_states"
"I am getting repetitive output when running \"python run_generation.py\""
"Where does the pre-trained bert model gets cached in my system by default? "
"How to make FP16 quantization on gpt/xl?"
"run_generation.py gives TypeError when using xlnet due to empty dict being passed as token"
"load_and_cache_examples crashes on windows"
"BERT adapted to time series"
"Learning Rate is not being updated by the Scheduler"
"What does 'output of the embeddings' mean?"
"relativeattentionbias.weight in block 0 EncDecAttention of T5 Model not in original tf model. Where do we get it from?"
"XLNet and RoBERTa embeddings"
"TypeError: Expected Operation, Variable, or Tensor, got None while saving tensorflow model"
"Dropout rates to be updated in all ALBERT v2 configs"
"Summarization ROGUE scores don't equal that of the paper ..."
"Bert cross attention"
"\"Reformer: The Efficient Transformer\" looks awesome. I'd love to see it in the library."
"How to finetune PreTrainedEncoderDecoder"
"How to run bert without checkpoints"
"Feature Request: Pipeline for Query/Document relevance"
"Why does the BertForQuestionAnswering sample code duplicate the [CLS] token?"
"CamembertForQuestionAnswering"
"Trouble fine tuning BERT language model "
"GLUE Benchmark Hyperparameters"
"transformers command not found after installing transformers using pip"
"GLUE benchmark score for XLNet_base_cased?"
"Quickstart BERT Example: Assertion Error"
"Confusion about the target_mapping parameter of the xlnet model"
"CTRL - RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
"Why albert has a print statement during forward?"
"Finetuning on several tasks "
"How to fine-tune PreTrainedEncoderDecoder on new dataset?"
"upgrading new transformer doesn't work"
"How Can I load the google bert model(ckpt)? "
"Load the google bert model(ckpt) from TFBertForPreTraining error"
"Clarification regarding past/layer_past in GPT-2"
"Pipelines: add PoS support"
"Encounter an \"index out of range problem\""
"What is the \"could not find answer\" warning in squad.py"
"RuntimeError: The size of tensor a (30524) must match the size of tensor b (30522) at non-singleton dimension 2 --- run_lm_finetuning.py"
"Fine-tuning BertAbs on new dataset?"
"Is the position of the scheduler.step() correct?"
"Classification of sentence pair with two different languages"
"finetune transformer"
"errors encountered with run_lm_finetuning.py"
"how to use distilledgpt2"
"clarification on output"
"The method os.rename() in file_utils.py  make a permissionError"
"Different usage between BertModel and AlbertModel"
"Pre-trained model returns different outputs(random outputs)"
"Can't load finetuned model properly."
"Pipelines support"
"What dataset was used for the NER results reported in the docs for bert/roberta-large-cased and distilbert-base-uncased models?"
"Unable to download community models"
"run_squad_w_distillation update"
"Pretrained model installation issue"
"ALBERT pretrained models uses wrong type of GELU activation"
"Model2Model quickstart attention_mask dimensionality problem"
"unable to use distilbert multilingual model"
"Distilbert predicting mask"
"import Error from official example caused by fastprogress"
"Batch size affecting output."
"BertForTokenClassification can not from_pretrained the fine-tuned model?"
"Pretrained Model not available"
"weird resize during the initialization in the PreTrainedModel"
"BERT's Embedding/Vocab Size in Code is Different from Provided Pretrained Config"
"Can't download models or model config"
"Error in pipeline() when model left as None"
"Typo in XLM moses pipeline."
"What is the difference between T5Model, T5WithLMHeadModel, T5PreTrainedModel?"
"How to use transformers-cli serve , how to set up on the server side?"
"greedy beam search generates same sequence N times"
"Albert to torchscript is not working"
"Unclear documentation for indice masking"
"Is there a way to reduce the vocabulary size?"
"Bug Transformers 2.3.0 - ValueError: invalid literal for int() with base 10: 'pytorch'"
"[Albert] SentencePiece Error with AlbertTokenizer"
"Is any possible for load local model ?"
"[DistillBERT] tokenizer issue of multilingual-cased"
"convert tf ckpt to pytorch_model.bin, load back model(TFBertModel), will loss params"
"Tokenize whole sentence vs. tokenize words in sentence then concat"
"ALBERT model does not work as expected"
"Padding part output in BERT NER task is not [PAD]?"
"It occurs error when python run_lm_finetuning.py "
"T5_INPUTS_DOCSTRING correct!?"
"How can I fine-tune XLM for sentence classification?"
"make test problem"
"Generating text with fine-tuned TFGPT2LMHeadModel in python. "
"DistilBertForSequenceClassification returning nans"
"is pytorch-pretrained-bert still being maintained in the future?"
"loss_fct = CrossEntropyLoss(ignore_index=-1) for BERT/RoBERTa MaksedLM"
"porting XLM-Roberta to tensorflow 2.0"
"Error occurs in XLMRobertaModel when token_type_ids is given."
"RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows."
"Reproducibility problem with DistilBERT paper "
"Tokenizer methods and padding"
"Evaluation not working on distilbert-base-uncased-distilled-squad"
"Error when running run_generation.py"
"Installation of Transformers without Sacremoses"
"ROBERTa model wrong padding for token_type_ids field if return_tensors=True"
"Fine-tuning pretrained BERT model using own dataset but with same training task"
"For Hugging Face transformer's hidden_states output, is the first hidden state tensor that is returned the out of the embeddings?"
"TF2 version of Multilingual DistilBERT throws an exception  [TensorFlow 2]"
"How to use GPU to do inference ?"
"How to run the \"run_lm_finetuning.py\" with my own corpus?"
"GPT-2 XL PyTorch Quantization for use on a Cloud Server"
"Error in BertForMaskedLM with add_tokens"
"How pipeline can use a  ner finetuned model from a local directory ?"
"Using T5"
"Pytorch T5 does not run on GPU"
"Using Transformer Library for code prediction"
"ALBERT tokenizer : local variable 'tokenizer' referenced before assignment"
"help..."
"DistilBertTokenizer defaults to tokenize_chinese_chars=True"
"TFDistilBERT ValueError when loading a saved model and running model.predict(), same with any sequence classification model in tensorflow"
"ImportError: No module named 'transformers'"
"Implement Layer-wise Relevance Propagation (LRP) for prediction explanation"
"BERT add_token function not modify bias size"
"[closed] cls token in XLM"
"model.generate should support past as an input"
"Removing pretrained layers?"
"Import issues in run_squad_w_distillation"
"Finding the right keras loss and metric for SQuAD"
"\"config.json\" does not include correct \"id2label\" and \"label2id\" after finetuning on NER task"
"NER Pipeline Issue"
"Model trained on Wikipedia Articles"
"UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 13: character maps to <undefined>"
"Masked tokens are -1 not -100?"
"GPT2 text generation produces different results w and w/o `past`"
"Using Model2Model with Albert"
"How to load tf1 BERT checkpoints and sentencepiece model from local folder?"
"RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
"Trouble fine tuning distilbertmodel"
"mistake, closing"
"[announcement] Community effort for storing models metrics in one place. Anyone can help to gather results"
"Perform MultiLingual Name Matching "
"BERT and cross entropy"
"BertTokenizerFast.encode() ignores max_length"
"AttributeError: 'BertForTokenClassification' object has no attribute 'named_configeters'"
"Discrepancy in results ( BertModel) between pytorch_pretrained_bert and transformers"
"XLMRobertaTokenizer is a wrong tokenizer for XLMRoberta"
"ModuleNotFoundError: No module named 'model_bertabs' AND RuntimeError: CUDA error: device-side assert triggered"
"Saving full tensor output of hidden states instead of truncated output in lm_finetuning.py script"
"Getting started with the new 'FeatureExtractionPipeline' feature"
"Error in AlbertForMaskedLM with add_tokens and model.resize_token_embeddings"
"T5 Masked LM -- pre-trained model import?"
"How to use transformers to  convert  batch sentences into word vectors???"
"Save only Bert Model after training a Sequence Classification Task/ LM finetuning Task. "
"Type of Training file needed for finetuning"
"Does calling fit() method on TFBertForSequenceClassification change the weights of internal pre-trained bert?"
"Descriptions of shared models and interaction with contributors"
"https://s3.amazonaws.com/models.huggingface.co/xxx/pytorch_model.bin failed or can not open at xxx/.cache/xxxxxxxxxx"
"Tokenizer encoding functions don't support 'left' and 'right' values for `pad_to_max_length`"
"Will you release the pre-train script for T5?"
"Error when running demo script in T5Model"
"How to get the output in other layers from Bert?"
"[Question] Add extra sublayer for each layer of Transformer"
"Updating the issue template, directing general question to SO"
"SentencePiece Error with AlbertTokenizer using google pretrained chinese model"
"Gradient accumulation"
"DistilBERT accuracies on the glue test set."
"Universal Sentence Encoder"
"[Question] Help needed to understand how torch.distributed.barrier() works"
"Finetuning TFDistilBertForQuestionAnswering on SQuAD"
"squad convert example to features potential bug"
"Dynamic Quantization on ALBERT (pytorch) "
"Unable to generate ALBERT embeddings of size 128"
"AlbertDoublehHeadsModel"
"SQuAD convert_examples_to_features skipping doc tokens when they exceed max_seq_length"
"unsupported operand type error in tokenizer.batch_encode_plus"
"fast gpt2 inference"
"EnvironmentError OSError: Couldn't reach server"
"Model not learning when using albert-base-v2 -- ALBERT"
"CTRL tokenizer has no special tokens to indicate EOS"
"Quantized model not preserved when imported using from_pretrained()"
"Prediction on NER Tensorflow 2"
"why this implementation didn't apply residual and layer norm?"
"Model upload and sharing - delete, update, rename...."
"Architectures for Dialogue"
"Optionally convert output of FeatureExtraction pipeline to list"
"question about tokenizer changes original sequence length"
"Bert perform way worse than simple LSTM+Glove"
"Finetuning ALBERT using examples/run_lm_finetuning.py"
"Why isn't BERT doing wordpiece tokenization?"
"Bert TPU fine-tuning works on Colab but not in GCP"
"Is RoBERTa's pair of sequences tokenizer correct with double </s>"
"is RoBERTa-base.json in s3  wrong?"
"always occur error:AssertionError"
"GPT2TokenizerFast object has no attribute 'with_pre_tokenizer'"
"glue_convert_examples_to_features in  glue.py runs to  errors"
"Invalid argument:  assertion failed: [Condition x == y did not hold element-wise:] [x (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 128]"
"XLM-Roberta checkpoint redundant weight"
"How to start a server and client to get feature vectors"
"what's the structure of the model saved after fine-tuning ?"
"Attibute Error\uff1a\u2018NoneType\u2019 object has no attribute 'seek' and OSError"
"PyTorch 1.2 has released API 'torch.nn.Transformer'\uff0cso it's better to modify the source code with the official  python API"
"The accuracy of XLNet"
"how can i download the model manually?"
"run_lm_finetuning.py regenerates examples cache when restored from a checkpoint, is this intended?"
"run_glue.py, CoLA : MCC goes to 0, in some hyperparameter cases"
"What is the f1 score of Squad v2.0 on bert-base? I only got f1 score 74.78."
"RuntimeError: The expanded size of the tensor (449) must match the existing size (2) at non-singleton dimension 2.  Target sizes: [4, 2, 449].  Tensor sizes: [1, 2] while using ALBERT"
"RAM leakage when trying to retrieve the hidden states from the GPT-2 model."
"changing the attention head size in MultiBert"
"Transfer Learning on Text Summarization Model"
"load  tf2 roberta model meet error"
"Xlnet, Alberta, Roberta are not finetuned for CoLA task"
"Trouble fine tuning multiple choice"
"unexpected keyword argument 'encoder_hidden_states' when using PreTrainedEncoderDecoder"
"XLNet: Incorrect segment id for CLS token"
"Can not upload BertTokenizer.from_pretrained() from an AWS S3 bucket"
"glue.py when using mrpc and similar data does not work"
"Upload CLI: on Windows, uniformize paths/urls separators"
"Bug in the command line tool: os.DirEntry not supported in Python 3.5"
"Bad Results with Albert"
"run_ner.py huge discrepancy between eval and predict (or \"dev\" and \"test\" evaluation modes)"
"Finetuning my language model"
"Error in fine tuning Roberta for QA"
"XLnet memory usage for long sequences"
"Missing module \"startlette\" when calling transformers-cli"
"Question answering pipeline fails with long context"
"Adaptive Attention Span for Transformers"
"TF Models have no attribute .train() or .eval()"
"summarization codes "
"Adding scibert in the list of pre-trained models?"
"Document which heads are pretrained and which aren't"
"Documentation markup for model descriptions"
"tokenizer.add_tokens not working"
"QA pipeline run-time error when there is no answer"
"How to merge TFDistilBertForSequenceClassification with another tf.Keras model"
"Pipeline error when creating a model without a model card json file (on Windows)"
"BertModel output the same embedding during Evaluation"
"Why does the hidden state of the same input token change every time I call the same GPT2 model?"
"Albert on QQP inference"
"Question about Architecture of BERT for QA"
"Pad token for GPT2 and OpenAIGPT models"
"CamembertTokenizer cannot be pickled"
"Details on T5's current integration status "
"Gradient checkpointing with GPT2DoubleHeadsModel"
"Get Warning Message: Unable to convert output to tensors format pt"
"AttributeError: 'Tensor' object has no attribute 'transpose'"
"batch_encode_plus not working for GPT2, OpenAI, TransfoXL when returning PyTorch tensors"
"ImportError: cannot import name 'TFDistilBertModel'"
"Scrambled dimensions on output of forward pass"
"BERT LOSS FUNCTION"
"XLNet run_squad.py IndexError: tuple index out of range"
"How to load locally saved tensorflow DistillBERT model"
"glue.py: AttributeError: 'numpy.str_' object has no attribute 'text_a'"
"Question Answering with Japanese"
"run_lm_finetuning.py for GPT2 throw error \"Using pad_token, but it is not set yet.\""
"Using a Model without any pretrained data"
"loss function error when running run_lm_finetuning.py file"
"XLNET SQuAD2.0 Fine-Tuning - What May Have Changed?"
"Add keyword arguments to batch_encode_plus() to match encode_plus()"
"Using Transformers for a Sequence with Multiple Variables at Each Step"
"Inconsistent values returned by batch_encode_plus and encode_plus "
"PPLM with Tensorflow"
" 'Embedding' object has no attribute 'shape'"
"Multiple token IDs for same token"
"Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated"
"How to get .ckpt files for tensorflow  DistilBERT model"
"models and tokenizers trained with pytorch_pretrained_bert are not compatible with transformers"
"is SOP(sentence order prediction) implemented?"
"bert-base-uncased have weird result on Squad 2.0 "
"Fine tuning XLMRoberta for Question Answering"
"Best weights/models after fine-tuning gpt2"
"Trouble fine tuning Huggingface GPT-2 on Colab \u2014 Assertion error"
"Bug in consecutive creation of tokenizers with different parameters"
"Does loss  function in the run_tf_ner.py takes logits or probabilities?"
"How to add a fc classification head to BertForQA to make a MTL-BertForQA model?"
"Issue with my profile on the upload/share models webpage"
"TFCamembertModel"
"distilbert_multilingual_cased model for multiple language"
"German Bert tokenizer does not recognize (some) special characters (!,?,...)"
"Issue about pipeline of sentiment-analysis"
"Hardware requirements for BERT QA inference"
"how can i finetune BertTokenizer?"
"Regarding distlbert uncased model's size"
"Input file format for examples/run_lm_finetuning.py"
"AutoModel fails to load FlauBERT with `output_hidden_states`"
"get_linear_schedule_with_warmup method can't be found in optimization.py"
"Missing `do_sample` argument for run_generation example"
"Albert language model fine tuning not running run_lm_finetuning.py "
"DistilBERT does not support token type ids, but the tokenizers produce them"
"run_lm_finetuning.py on bert-base-uncased with wikitext-2-raw does not work"
"How to make transformers examples use GPU?"
"What is the input for TFBertForSequenceClassification?"
"Load from tf2.0 checkpoint fail"
"Can't pickle local object using the finetuning example."
"DistributedDataParallel for multi-gpu single-node runs in run_lm_finetuning.py"
"TypeError: apply_gradients() missing 1 required positional argument: 'clip_norm'"
"a problem occur when I train Chinese distilgpt2 model"
"Weights of FlaubertForQuestionAnswering not initialized from pretrained model"
"How to add Dense layer on top of TFBertForSequenceClassification model?"
"error while training distilbert multilingual model"
"DistilBertForMaskedLM is not passing ignore_index to loss fct nn.CrossEntropyLoss"
"Error when running run_lm_finetuning.py"
"Is transformers ovewriting tokenizer?"
"Bert and Roberta models cannot be converted to TFLite"
"sequence labeling for sentences and not tokens"
"add TinyBERT?"
"convert_tokens_to_ids(self, tokens)\u4e2d\u7684ids.append(self.vocab[token])\uff0cKeyError"
"XLM Roberta token_type_ids bug with batch_encode_plus"
"RuntimeError: expected dtype Float but got dtype Long - run_lm_finetuning.py"
"Attention Mask for TFXLM Model doesn't work"
"QuickStart code error"
"Masked LM and TFBertForSequenceClassification"
"Error for run_lm_finetuning.py (CUDA error: device-side assert triggered)"
"Save model wrapped in Keras"
"Version 2.4.1 breaks run_lm_finetuning.py, version 2.3.0 runs fine"
"Development Infrastructure for ML Projects"
"T5 "
"XLM-Roberta mask filling error"
"do_lower_case strips accents!"
"PreTrainedEncoderDecoder keeps giving me the same next token"
"Albert language model fine tuning not running run_lm_finetuning.py"
"TFAlbertModelTest::test_pt_tf_model_equivalence  -> Fatal Python Error on Mac"
"default output of BertModel.from_pretrained('bert-base-uncased')"
"Sentence pair classification"
"Saving tokenizer vocabulary throws error when passing file name instead of directory."
"Multi-text files support for run_lm_finetuning"
"Cannot reproduce SQUAD Example"
"TFRoberta output with attention_mask changes in version 2.3.0 vs 2.4.1"
"Loss is calculated on all tokens, including padding, in the LM fine-tuning example"
"Add albert-base-v3 to pretrained models?"
"Adapter-BERT is missing in transformers library?"
"why take the first hidden state for sequence classification (DistilBertForSequenceClassification)"
"Model download: tf-xlm-roberta-large \"tf_model.h5\" file missing "
"The prediction output is random"
"export to onnx issue"
"How to generate different suggestions with GPT2 or XLNet like Write With Transformers?"
"How to load a pretrained TF model using AutoModel?"
"embedding index getting out of range while running gpt2-xl model "
"Using fast tokenizers with pipelines"
"Pipeline for text classification"
"configuration from custom config file not working "
"Pipelines- if initial model download is interrupted, everything is ruined"
"Flaky TF pipelines test on CircleCI"
"RoBERTaMultiChoice does not work with `roberta-large`"
"Features proposals to simplify training Tensorflow model"
"ERROR:CUDA out of memory when using GPT2 tour"
"SequenceSummary: config.summary_activation = 'relu' would be ignored"
"Distillation code loss functions"
"SQuAD preprocessing not working for roberta (wrong p_mask)"
"Is there any way that I can extract the hidden output from the self-attention layer?"
"Is there any way that I can directly feed the hidden output of the embedding layer into each of the transformer's layer?"
"tiny issue with distilbertconfig docs"
"You must specify an aggregation method to update a MirroredVariable in Replica Context."
"Probably a bug in XLMRobertaTokenizer"
"output padding different to zero in hidden layers with attention mask"
"Reduce the CamemBERT dimensions"
"CircleCI doesn't run slow tests"
"Can't load pre-trained Flaubert model"
"Support DeepSpeed for language modeling finetuning"
"TFBertModel.from_pretrained('neuralmind/bert-base-portuguese-cased') -> TypeError"
"Multiple Choice BERT, SWAG task, failure to test "
"How to get longer output for summary?"
"How to use a batch size bigger than zero in Bert Sequence Classification"
"How can I finetune the BERTModel on my own corpus?"
"PreTrainedModel.generate do_sample default argument is wrong in the documentation"
"Repository with recipes how to pretrain model from scratch on my own data"
"GPT2LMHeadModel with variable length batch input"
"Albert multilingual"
"ImportError: cannot import name 'GradientAccumulator'"
"CUDA out of memory issue in the middle of training in run_language_modeling.py (say after 1000 steps)."
"bugs in xlnet XLNetLMHeadModel"
"GPT-2 language model: multiplying decoder-transformer output with token embedding or another weight matrix"
"binarized_data.py in distillation uses incorrect type casting"
"Why only use the hidden state of last token of last layer is used for predicting the next word?"
"OOM risk in RobertaTokenizer/GPT2Tokenizer"
"BERT generating prediction in 120sec approx using squad 2.0 in prediction.json"
"Reusing states for sequential decoding in BERTForMaskedLM"
"Installation Error - Failed building wheel for tokenizers"
"'distilbert-base-cased-distilled-squad' was not found error"
"Failing slow AutoModelTest/BertForPreTraining"
"Failing slow RobertaModelIntegrationTest"
"Getting value of [UNK] labels"
"Pretrained TFAlbertForMaskedLM returns seemingly random token predictions"
"A small model for CTRL "
"Fine-tuning the model using classification tasks"
"cannot find model in model name list"
"when will add XLMRobertaForQuestionAnswering package"
"Error reported when running ''run_language_modeling.py\" file"
"BART/T5 seq2seq example"
"Add `masked_lm_labels` argument to `TFAlbertForMaskedLM`"
"PreTrainedEncoderDecoder does not work for LSTM "
"is right?"
"Post-padding affects the Bert embedding output"
"DistilBERT distilbert-base-cased failed to load"
"PreTrainedTokenizer returns potentially incorrect attention mask"
"What does the variable 'present' represent?"
"UserWarning: The number of elements in the out tensor of shape [1] is 1 "
"How to get the matrix that is used to combine output from multiple number of attention heads?"
"from_pretrained making internet connection if internet turned on"
"How can I run NER on ALBERT?"
"ValueError: too many dimensions 'str'"
"RoBERTa has a token_type layer (just a cosmetic issue)"
"Explanation of the results derived from fine tuning"
"how to get \"xlnet-base-cased-pytorch_model.bin\" original 'last modified' date?"
"How  to run TFBERT model in disable_eager_execution() mode "
"Error with run_language_modeling.py training from scratch"
"FileNotFoundError when python runs setup.py for sentencepiece"
"No prediction for some words (BERT NER) when run on GPU"
"Evaluation and Inference added to run_glue.py"
"Load Pretrained Model Error in Inherit Class"
"Regarding attention size returned by the model"
"Getting: AttributeError: 'BertTokenizer' object has no attribute 'encode'"
"Pipeline Loading Models and Tokenizers"
"Allow import of model components, e.g. `from transformers import TFAlbertMLMHead`"
"BertModel' object missing 'save_pretrained' attribute"
"save_pretrained doesn't work with GPT2FastTokenizer"
"Language modeling example script missing the next sentence predicion"
"RobertaTokenizer different than fairseq for 'world'"
"Pre-trained BERT-LM missing LM Head - returns random token predictions"
"Convert BERT to RoBERTa"
"squad_convert_example_to_features does not work with CamembertTokenizer"
"documentation for TF models mentions non-existent methods"
"Help needed with interpretation of the MLP class"
"Model I am using Roberta"
"`PreTrainedTokenizerFast.build_inputs_with_special_tokens` doesn't add the special tokens"
"missing \"para\" attribute in ARC dataset for multiple choice question answering model"
"How to train with variable number of candidates for multiple choice selection?"
"How to train a LM with a custom Dataset?"
"Breaking-change behavior in BERT tokenizer when stripping accents"
"Fast tokenizers ignore `add_special_tokens=False`"
"Error arises when using pipeline with community model"
"Loading tensorflow first and then loading transformers errors"
"DistilRoberta Model fine tuning on Squad dataset"
"Masked LM implementation details"
"What does ## mean in the bert vocab?"
"Getting the same results when evaluating Model2Model with different encoder inputs."
"New tokenizers issue in NER demo"
"OpenAIGPTDoubleHeadsModel throws CUDA OOM with large number of candidates"
"BERT model breaks during FP16 Apex training on the latest update (2.5.0) - due to gelu function"
"pipeline(\"sentiment-analysis\")() can't handle more than 2 sentences"
"fp16 is not compatible with the current activation code when pytorch is less than 1.4.0"
"output padding different to zero in embedding layer"
"On masked-lm labels and computing the loss"
"Fast tokenizers padding and prefix space"
"Some questions about change the BertForSequenceClassification"
"Change the model type after fine-tuning?"
"BertTokenizerFast ignores `pad_to_max_length`"
"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
"Migrating from `pytorch-pretrained-bert` to `pytorch-transformers` issue regarding model() output"
"Distillation throws CUDA out of memory even with available GPU memory"
"Bias in `BertLMPredictionHead ` is added twice"
"Switching from argparse to Typer"
"Python Tokenizer batch_encode_plus doesn't pad input if asked to do so."
"Length of special_tokens_mask doesn't align with the input_ids"
"Bug in transfo_xl function call"
"Getting the output of the from the forward function of the GPT-2"
"GPT2 always has largest attention on first token?"
"XLMRobertaTokenizer vocab size"
"unreadable codes in for utils_glue"
"Question about output pipeline(feature-extraction)"
"Cannot install Transformers version >2.3.0 with pip on CentOS"
"Strange bug when Finetuning own pretrained model (with an even stranger solution)"
"`AutoModel.from_pretrained` sends config kwargs to model"
"How to generate BERT/Roberta word/sentence embedding?"
"run_ner.py example"
"Train TFXLNetForSequenceClassification model failed."
"Too many bugs in Version 2.5.0"
"No optimizer steps when gradient_accumulation_steps smaller than epoch_iterator length"
"Trying to Use AlbertTokenizer With my own custom Vocab file"
"unk_token not set when loading TransformerXLTokenizer.from_pretrained() from a save_pretrained() "
"Changing the loss function in BERT"
"BART : How can I train and evaluate BART on CNN/DM dataset"
"Output of pipeline feature extraction"
"[Benchmark] Pipeline for question answering"
"batch_encode_plus with pad_to_max_length is not padding the output"
"Generation with gpt-2"
"Latest version of transformers available via conda?"
"Use my own pretrained BERT model"
"Can GPT2LMHeadModel do batch inference with variable sentence lengths?"
"BART : host `bart-large-cnn`"
"Why do I run example/run_ner.py with no output"
"language_modeling.py doesn't continue from last global step"
"This class and module cannot be found"
"AttributeError: 'Tensor' object has no attribute 'size'"
"How to add data to  pretrained model."
"run_tf_glue with AdamW optimizer and distributed training"
"Forward pass with GPT2 using both past and attention_mask as an input leads to dimension error"
"Loading custom weights for BERT in pytorch"
"Cannot use `transformers.GradientAccumulator` with `tf.function`"
"Wrong logic in `batch_encode_plus()`"
"AttributeError: 'Model2Model' object has no attribute 'prepare_model_kwargs' in 2.5.1"
"Paragraph re-ranking using MS MARCO dataset"
"Knowledge distillation from internal representation GPT2"
"Issue with Makefile"
"How can I use the this result?"
"[docs] Provide a barebones GPT-2 colab notebook"
"Set specific hidden_size for ClassificationHead"
"Regarding attention received by the distilbert model"
"Should be able to turn off logging"
"Is ALBERT the right implement from paper ?  "
"Fast tokenizers don't properly tokenize special tokens"
"Fast tokenizers calculate wrong offsets when special characters are present"
"How to init a subclass of BertForTokenClassification"
"Bad word list for text generation"
"Should weight distribution change more when fine-tuning transformers-based classifier?"
"Add LM capabilities to TFTransfoXLLMHead"
"XLM-RoBERTa can't add new tokens."
"Documentation and code mismatch in BertForMaskedLM forward method"
"No speed diference when doing prediction between BERT and ALBERT"
"Problem with using pretrained BertTokenizer for Korean"
"No Causal Attention Masking in GPT-2 LM Finetuning Script"
"load_tf_weights_in_bert : 'BertModel' object has no attribute 'bias'"
"Predict the next word in sentence context from the list of possible words in Russian"
" Chinese BERT model can be used represented by words instead of character"
"Finetuned BERT model does not seem to predict right labels/work properly?"
"Only enable some labels in BERT fine-tuned NER"
"Training TFBertForSequenceClassification with custom X and Y data"
"XLNet multiple sentence modeling token type ids"
"Bart CUDA not working"
"Docker Hub automatic tests failing"
"Why there is no TransfoXLForSequenceClassification class?"
"Memory error : load 200GB file in run_language_model.py"
"Disabling Eager Mode Prevents Loading Pre-Trained BERT"
"Error when runnig run_tf_ner.py"
"Fast tokenizers can't `encode_plus` a list of ids; slow tokenizers can"
"Cuda error during evaluation - CUBLAS_STATUS_NOT_INITIALIZED "
"Fast tokenizers fail when the input is just spaces"
"\u010b in gpt2"
"wrong 'label2id' and 'id2label' in config when loading from pretrained"
"Getting different topk results when using past + attention mask for more than 1 sentence"
"BART BartForSequenceClassification example"
"Some question about training BERT after change the Vocab.txt size"
"why BertModel' object has no attribute 'bias'"
"Keras layers should override get_config to be JSON-serializable"
"BART -- RuntimeError: expected device cuda:0 but got device cpu"
"BertTokenizer.save_pretrained() ignores do_lower_case"
"BART: <mask> token ID is outside vocab bounds"
"BART FP16"
"Making past and mems variables have batch size as their first output dimension."
"A better way to process extended_attention_mask in BertModel.forward()"
"NER tutorial: run_tf_ner.py reports an entity number not matching the one in the .txt files"
"BartTokenizer and 'bart-large-cnn' out of sync"
"Load pretrained roberta model from fairseq?"
"Performance Issue about pretrained bert migration from tensorflow to pytorch"
"Converting tf weights:  AttributeError: 'GPT2Model' object has no attribute 'zeLoss'"
"BART: move boilerplate code inside encoder/decoder"
"Cant import my pretrained bert model from NVIDIA/DeepLearningExamples/"
"links of the model's pretrained weights"
"GPU memory getting out of bound "
"Missing `missing_keys` when loading from saved base model checkpoint"
"[Question]: Why does model.__call__ return the loss too?"
"Padding changes model outputs (even with attention_mask)"
"How to train a distilled gpt2"
"BART.generate: possible to reduce time/memory?"
"Fresh macOS install errors out on import"
"seed parameter for model generate()"
"i want to browse and store image in data base in tkinter can u give me suggessions?"
"NER: some issues in PyTorch Lightning example"
"urgent - ROBERTA on WSC"
"Does this project have this function ?"
"Inference is slow with "
"wrong configuration of ALBERT xlarge"
"Reason for speedup"
"[BERT] Implementation of the sliding window for long sequences"
"padding and attention mask does not work as intended in batch input in GPT2 language model "
"Can we use GPT-2 sentence embedding for classification tasks?"
"issues while modifying modeling_roberta.py file"
"Semantic Code Retrieval using Transformers"
"Do we have a whole-word-masked version of BERT?"
"Quick tour TF 2.0 "
"Get the CNN/Daily Mail Data for BART"
"How can I assign a specific gpu when using examples/run_language_modeling.py?"
"GLUE test set predictions"
"Pretraining QA corpora from scratch with sentence pairs"
"I can not import transformers"
"The implementation of GPT2 masked attention mechanism will cause errors when the model was trained after some iterations."
"How can i use pipeline with pretrained automodel ?"
"About the examples document of bert with SQuAD 2.0"
"`Failed to build tokenizers` when installing 2.5.1 version."
"Knowing the specific data set used for DistilBertForQuestionAnswering"
"Beam search sometimes fails this assert error"
"I want to import the model path on my owm computer?"
"Provide comprehensive guide & best-practices for run_language_modeling.py"
"Where is the default download address for pre-trained weight"
"Error reported when fine tuning on my dataset using ''run_language_modeling.py\""
"TF GPT2 Language model can't be created with from_pretrained() for specific shortcut name"
"Attention mask always returns array of ones for CamembertTokenizer.batch_encode_plus"
"UnboundLocalError: local variable 'tokenizer' referenced before assignment"
"where is the position emdeddings in bert for training a new model from scratch ?"
"More details about DistilBERT experiment setting."
"Pipeline for Question Answering: How to return multiple correct answers output?"
"Error loading pretrained bert-base-multilingual-cased"
"How to tokenize word to characeter"
"Why the pre-trained models will be downloaded each times?"
"torch.distributed.barrier() have NCCL error"
"Problem with PreTrainedTokenizerFast and return_offsets_mapping"
"Strange behaviour after using BertTokenizer.add_tokens()"
"An Error report about pipeline"
"train dev test split with BERT"
"[TorchHub]Repo's layout is not compatible with TorchHub anymore since 2.0"
"How to encode a batch of sequence?"
"seems TFBertForSequenceClassification cannot load tf1.x model?"
"Get word seperator char for tokenization"
"pad error in BertTokenizer.batch_encode_plus "
"How do you do inference in production?"
"Using FP16 on BartModel"
"UnicodeDecodeError when loading BART from fairseq checkpoint"
"Why is the seq_len dimension hard coded to be the first dimension of BERT's input?"
"batch_encode_plus cannot work properly"
"bug in run_glue"
"Implement Electra"
"very slow performance on transformer 2.5.0 versus 2.3.0"
"Great job advice"
"Model name 'distilbert-base-german-cased' was not found in model name list."
"TFAlbertMainLayer cannot be imported from the transformers library."
"[BART] test_generate_fp16 fails after PR#3140"
"when I install transformers, it appeared this error"
"train model from scratch with big data"
"Finetuning before feature extraction"
"how can i distill xlm-roberta model , just like distill roberta model , any suggestion ?thanks a lot  "
"Tremendous slowdown in multi-node distributed training"
"Cannot Achieve Reproducability with Tensorflow Transformer Models"
"Model fail to revert to generation_mode=False after generation"
"how to finetune with PreTrainedEncoderDecoder"
"how to use TFBertModel to load a Bert, which the path is from own computer."
"Install error , Win10,anaconda3,python3.5,pytorch"
"What is the most effective way to use BERT , ROBERTA , GPT-2 architectures as frozen feature extractors ?"
"Return token span from NerPipeline"
"Is there a way to evaluate GPT-2 model during fine-tuning process for accuracy and fluency?"
"Unexpected output from feature extraction pipeline"
"Dockerhub images huggingface/transformers_cpu for version 2.5.1 has version 2.5.0 installed"
"GPT-2 attention_mask reshaping uses input_ids first dimension"
"a lot of examples in doc can't run successful"
"NER Pipeline returns null"
"BertForPreTraining should compute only <MASKED> prediction_scores"
"Installation error: can not find Rust compiler"
"Getting output of any hidden layer "
"ImportError: cannot import name 'BartForConditionalGeneration'"
"Error in loading Albert model"
"Error in loading  albert-base-v2"
"Loading DistilBertModel with AutoModel gives 12 layers  "
"Add sample softmax possibility to TransfoXL model for TransfoXL training"
"GPT2 -- build_inputs_with_special_tokens lacking BOS and EOS tokens."
"GPT2Tokenizer doesn't include BOS or EOS token"
"KeyError in GLUE data tokenization with RoBERTA"
"Mismatch in the accuracy figures"
"how does masked_lm_labels  work ?"
"TextClassificationPipeline does not work with pretrained BERT model"
"output value of  XLNetModel changes for the same input "
"pipelines.ipynb mask should be [MASK]"
"TF BERT not FP16 compatible?"
"Error loading finetuned bert model AttributeError: 'NoneType' object has no attribute 'endswith'"
"Cubla Error on DistilBert"
"CUDA Error when running run_language_modeling.py"
"run_tf_ner.py doesn't work with unlabelled test data"
"Finetuning T5 Model"
"transformers.PreTrainedTokenizer.tokenize does lower case work all the time and discards space and tab. Want this changed."
"No Module named Transformers"
"Reproducing SQuAD v1.1 with xlnet-base cased?"
"Bug? NaN loss after training for a while using for BERT Encoded sentences."
"License information by model"
"Downloading mlm-17-1280 community model"
"Some community models are broken and can't be downloaded"
"RuntimeError: CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 14.73 GiB total capacity; 13.33 GiB already allocated; 575.88 MiB free; 13.38 GiB reserved in total by PyTorch)"
"TF Camembert not improving over epochs"
"Generate all possible sentences using a fine-tuned GPT-2 model"
"GPT2TokenizerFast does not preserve special tokens' ids after a save and load."
"Why does huggingface bert pooler hack make mixed precission training stable?"
"BERT pretrained checkpoints"
"Add example code for CRF heads "
"closed"
"RobertaTokenizer doesn't have 'batch_encode_plus'"
"test_resize_tokens_embeddings does not inspect `get_output_embeddings`"
"Data Processor should not include in the package"
"Can't save DistilBert model."
"[BART] test_dummy_inputs fails on GPU"
"When I used the add_special_tokens function in the BertTokenizer, it assigns 2 different tokens with the same ID. Is this done on purpose?"
"gpt2 - convert examples to features(tensorflow 2)"
"minor website fix"
"Model conversion from PyTorch to TF2 doesn't work properly for ALBERT"
"Finetuning of T5 on SQuAD 1.1 including code examples"
"\ud83d\udc1bBugs in run_tf_ner.py"
"adding --fp16 to run_language_modeling and increase batch size but cuda out of memory error"
"\ud83d\ude80 Feature request Multimodal BERT Models"
"Cannnot Import from transformers"
"Supported language information by model"
"Trying to train a GPT2 from scratch"
"added_tokens.json is used for splitting texts"
"[Bart]example---BartForConditionalGeneration"
"AdamW in HuggingFace is different from AdamW in Pytorch"
" cannot import name 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING'"
"Add custom rules for sampling from GPT-2 Generator"
"Problem with running Transformer Notebook: How to train a language model"
"XLNet model on S3 not set up correctly? "
"Unused function in squad metrics"
"Reading files takes for ever in language modeling"
"Where is the code of Bart fine-tuning?Thanks"
"Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_selec"
"I want to create a tokenizer which the vocab file in my computer"
"Confusion in understanding the output of BERTforTokenClassification class from Transformers library"
"Problem saving and/or loading fine-tuned model"
"Error ImportError: cannot import name 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING' from 'transformers' (C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\transformers\\__init__.py)"
"how to use transformers to get all pretraining model names in transformers hub"
"How to detokenize a BertTokenizer output? "
"TFXLMRoberta impossible to load base and large model with pretrained weight ?"
"Same probability from fine-tuning custom pre-trained LM"
"can't import TFBertModel from transformers"
"`run_language_modeling` fails with community model (BioClinicalBERT)"
"Import error in example script `run_language_modeling.py`"
"run_lm_finetuning on multiple training files"
"Special tokens to pre-trained BART model"
"Save models after each epoch"
"Failure to load checkpoints saved during distributed training"
"Write With Transformer returning a 502 on gpt2/xl model"
"NER pipeline usage examples"
"Fine-tuning with BertForSequenceClassification on custom dataset yields a model that outputs only the label with highest support in training set"
"ImportError: cannot import name 'TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING'"
"WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: True"
"Error : forward() got an unexpected keyword argument 'inputs_embeds'"
"SyntaxError when fine-tuning ALBERT on NER"
"Question Answering pipeline not working"
"Docstring cannot be build anymore"
"Issue in generating samples for text generation"
"CircleCI ExamplesTests::test_run_squad failing"
"TFAlbertForMaskedLM Decoding Error"
"Inversion of a mask in newer pytorch versions"
"Finetuning FlauBERT with hugging face's Transformers : WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: True"
"Do I need to pad non-fixed examples or does run_language_modeling.py already takes care of that? "
"Tests for more examples"
"Sphinx build for documentation fails when tensorflow is installed"
"setup.py succeeds, then can't import transformers"
"missing import in BartForConditionalGeneration example"
"which iterator to use for different hugging face transformer models for solving multiple choice questions?"
"Finetuning GPT-2 "
"TypeError when using Feature Extraction Pipeline with XLM roberta"
"CUDA error: CUBLAS_STATUS_ALLOC_FAILED When running language modeling using bert-base-cased"
"How to load BertforSequenceClassification  models weights into BertforTokenClassification model?"
"REALM"
"XLM-ROBERTA"
"masked_lm_loss in BertForMaskedLM model"
"Bert Batch Encode Plus adding an extra [SEP]"
"Distil-BART?"
"No grad feature in model parameters"
"[Bart] when output_paste=False BartForConditionalGeneration raises confusing error"
"reproducing the performance of XLM-ROBERTA on MLQA dataset on the zh language"
"how to get activation weights of a pretrained model?"
"Adding mbart-large-cc25"
"Isort installed from github branch does not correspond to circle ci isort"
"Argument \u201cnever_split\u201d not working on bert tokenizer"
"why isn't AlbertForMultipulChioce in modeling_albert?"
"Why GPT2 train loss and topK accuracy both decrease?"
"Issue loading custom tokenizer for fine-tuning gpt2"
"bug in run_glue.py"
"Bart.generate requires config.output_past=True"
"Unexpected ZeroDivisionError when calling model.prune_heads"
"TypeError: sequence item 0: expected str instance, NBProgressBar found"
"Error when training with distributed training on 4/8 Nvidia v100."
"pretrained EsperBERTo"
"Error on fine-tuning XLM like model on SQUaD like dataset"
"T5 Summarization"
"Quick Tour TF2.0 error: dataclasses.FrozenInstanceError: cannot assign to field 'label'"
"forward() got an unexpected keyword argument 'output_all_encoded_layers'"
"KeyError: 'answers' error when using BioASQ dataset using Huggingface Transformers"
"Impossible to use T5 11b"
"How to extract \"contiguous tokens\" from `NerPipeline` results?"
"model name '../data/bert_models/chinese_finetuned_lm/pytorch_model.bin' was not found in model name list . Creating an empty model card."
"Recommended preprocessing steps for english sentences in GPT2"
"unable to completely load T5 pretrained model; missing/unexpected keys"
"resize_token_embeddings error for Transformer-XL"
"T5 for summarization: pipeline x T5ForConditionalGeneration different results"
"Metrics are coupled to the run_glue.py tasks."
"How to trace the BertForQuestionAnswering"
"Mean reduce over last hidden state"
"Evaluation of labelled test set?"
"can not init tokenizers from third party model , on albert model"
"Tokenizers: setting bos_token_id = 0 and adding language_pair_codes"
"Language model fine tuning using scibert as the base model"
"Regarding distilbert-multilingual-uncased model"
"tokenizer cannot load form model on disk"
"transformers pipeline in GCP cloud functions"
"BART run run_train.sh RuntimeError: expected device cuda:0 but got device cpu"
"How can I use masked_lm_labels correctly?"
"[Benchmark] QUAERO French Medical Corpus for Named Entity Recognition"
"T5 fine tune for seq2seq generation"
"DistilBert not giving hidden states"
"Summarization pipeline max_length parameter seems to just cut the summary rather than generating a complete sentence within the max length"
"wrong parameters order in TFTransfoXLMainLayer _update_mems call"
"Different outputs in using convert_roberta_original_pytorch_checkpoint_to_pytorch.py"
"Does the BART model support Chinese? Having the pre-trained Chinese model?"
"Dict in the first positional arguments"
"cased -> uncased in BERT GLUE example"
"Reason behind the layers taken for distilbert-multilingual"
"when I run transformers in Docker container, it appeared this error"
"How to fine tune T5 like for translation tasks?"
"Evaluation - Output False Positive and False Negative Sentences"
"min_length parameter in default pipeline summarization produces output smaller than min_length"
"Cannot load model in tranformers"
"Issues with using SciBERT for Summarizer"
"Transformers and BERT: dealing with possessives and apostrophes when encode"
"Wrong tokenization for distilbert-base-multilingual-cased"
"batch_encode_plus with pad_to_max_length but no max_length is not padding the output"
"CTRL generates French text when I want English texts"
"After enable fp16, torch.save model has error"
"Why is there not a SequenceClassification model for GPT-2?"
"Why isn't there a SequenceClassificationModel for GPT-2 (and some other models)?"
"\ud83d\udc1b Summarization pipeline : T5-base much slower than BART-large"
"RobertaTokenizer corner case with empty string"
"Filling more than 1 masked token at a time"
"How can u make sure that my transformer model should only one GPU, though the serve has multiple GPU cards."
"training GPT2 from scratch : implement causal attention mask?"
"The tensorflow implementation of T5ForConditionalGeneration runs much slower than the pytorch one. GPU utilization is 30%"
"Mismatch of loss shape in document with output of TransfoXL"
"Out of memory error while training GPT2-large on 8x32GB Nvidia Volta"
"Choosing between adding frequent out of vocab words and doing further pretraining."
"Feature Request: Fill Mask more than 1 token"
"default of weight_decay  for run_language_modeling.py "
"Add code to pretrain T5 model from scratch"
"how can i run gpt2 model on tf serving ?"
"ValueError: You have to specify either input_ids or inputs_embeds!"
"Failing to load saved TFBertModel"
"How to get top 10 possible set of words to calculate Top-K accuracy and MRR?"
"Cased model + `--do_lower_case` in documentation?"
"Custom collate function that pads only to the longest sequence?"
"Reinitializing layers in BERT"
"Translation pipeline bug after 398 characters"
"Summarization pipeline - Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/modelcard.json'"
"Wrong Mask LM prediction with BertForMaskedLM"
"Can't evaluate official TensorFlow NER model"
"How can I track the performance of my GPT-2 model during finetuning?"
"\u2753 How to run pipeline (summarization) in FP16 mode ?"
"Allow token regression in ForTokenClassification models"
"Bertabs metrics lower than paper"
"Chatbot QnA feature for given text corpus"
"How can I judge whether is in the dictionary?"
"\u2753Adding new tokens to pre-trained tokenizer"
"Weird summarization results - the summary is longer than the input"
"Exception: process 0 terminated with signal SIGKILL"
"Unable to serialize/save TF2.0 RobertaSequenceClassification model to saved model format"
"Any Ideas on how to generate in bulk with CTRL?"
"\u2753 In BART, why forcing the first token to BOS ?"
"Has anyone used the run_language_modeling.py to train a gpt2 in a different language? is it possible?"
"Loading pre-trained ELECTRA checkpoint to HuggingFace"
"How to train BART text summarization with your own data?"
"TypeError while loading the model built from scratch using transformer "
"Wrong tokenizer configuration in sentiment-analysis pipeline"
"gpt2-medium fine-tuned model.generate joins words and sentences together without space or newline"
"Does anyone have the XLNet (and ALBERT) NER performance on CONLL-2003 "
"run_generation.py with empty input"
"How to use GPT2DoubleHeadsModel?"
"Requesting model for TFAlbertForQuestionAnswering"
"Bug in variable name in NER"
"Is it possible to use multiprocessing for pipelines?"
"Can't update the train_batch_size and eval_batch_size for the training image in a docker container"
"BertSelfAttention have not Add and Norm layer, why???"
"cannot import name AddedToken"
"How to use Huggingface pytorch bert to generate the prediction TSV file from the test set of a GLUE task?"
"How can I pinpoints Logs directory to Google Drive while finetuning GPT-2 model, which helps in visualizing data via tensorboard?"
"Extending XLM Roberta for Question Answering"
"Deserialize BERT Sequence Lassifier Quantized Model & Inferencing Issue"
"Can not set different token and model dir in `run_glue.py`"
"Bug in ElectraForTokenClassification"
"Would the weights for the main body of the pertained GPT2Model and pertained GPT2DoubleHeadsModel be identical?"
"Problem with https://transformer.huggingface.co/doc/gpt2-xl"
"Queries about the Notation and Model training of T5 and ELECTRA sentiment classification."
"Distributed training on multiple GPU nodes is slower than on single GPU node"
"inconsistent tokenize output"
"TransfoXLLMHead doesn't shift labels internally when called for loss"
"Text Generation with XLNet is very Slow"
"cannot determine what will be the cardinality of the output after applying glue_convert_examples_to_features [TF  2.2.0rcx]"
"Zero shot multilingual BERT"
"How can i conditional fine-tuning with GPT2?"
"how to use transformers with gpu"
"loading from tf_ckp and this showed up: AttributeError: 'BertCrf' object has no attribute 'bias' ."
"Unable to load german BERT model"
"Disable @torch.no_grad() for model.generate() ? "
"DistributedSampler can't shuffle the dataset"
"How to get multiple answers from the context using BertForQuestionAnswering"
"Has anyone used run_language_modelling.py to train a gpt 2 from scratch? "
"Fine-tuning for paraphrasing tasks"
"ValueError: Cannot reshape a tensor - TFBertForSequenceClassification"
"Checking that the LM actually trained "
"OOM error when resuming training from a checkpoint"
"Loading pipeline(\"summarization\") failed"
"Fine tuning XLMRoberta for Question Answering"
"How i take an OpenAIGPTDoubleHeadsModel from run_language_modeling.py script? "
"Pipeline for text generation"
"Seq2Seq: decoder hidden_states shape not tested"
"Tokenizer Encode More than 2 inputs"
"Turning off Verbosity on QA model using Pipeline"
"text generation like lorem ipsum but human readable"
"Slow training time on BERT pretraining on multiple gpu compare to single gpu"
"Question about whitespace filtering in squad data processor "
"ImportError: cannot import name 'HfArgumentParser' from 'transformers'"
"Etract all last hidden states of the input dequences for Question and answering Bert"
"uss"
"How to speed up the transformer inference?"
"Deprecation warning due to invalid escape sequences in Python 3.7"
"Dealing with class imbalance "
"Why does `examples/translation/t5` test on newstest2013 rather than newstest2014?"
"Quick question difference output of Bert models compared to Electra"
"Summarization pipeline fails to initialize"
"PPLM Write With Transformer demo not working "
"long text classification"
" Input format for a BertTokenClassification task"
"Issues in Training GPT-2 Model from Scratch (Text Generation-Identifying Epoch Value-Perplexity Calculation)"
"Text generation with Transformer-XL stops at <eos> token."
"Getting error AttributeError: 'BertOnlyMLMHead' object has no attribute 'bias' when giving TF path"
"Cannot find the script"
"Why the first item of the config.json of bert is \"architectures\": [\"BertForMaskedLM\"]"
"Making Simple whitespace tokenizer and then using that tokenizer to make a language model from scratch?"
"OpusNMT/MarianMT Machine Translation Models"
"Problem when Converting a Fine-tuned Checkpoint from TF to PyTorch using ALBERTxxlargev1 Model"
"language modeling other models"
"Importing horovod.tensorflow crashes AlbertTokenizer but not BertTokenizer"
"Longformer, a scalable transformer model for long-document NLP tasks"
"Convert pytorch-pretrained-bert to new version (transformers)"
"How to fine tune EncoderDecoder model for training a new corpus of data ?"
"Why force tokens in Bart decoding"
"In just fouth blocks of the code of the colab notebook \"01-training notebook\", it just failed."
"Inconsistencies and possible bugs in different tokenizers"
"Is there a classical transformer model in the project?"
"Using run_glue.py on external datasets for fine-tuning a RoBERTa classification model --> Is this possible?"
"Getting large alloc error while evaluating bert-base on NER task"
"Calculated offsets are wrong"
"Error when using run_generation.py to generate texts with long prompts, specifically for models -XLM and Openai-GPT"
"Clarification about GPT2LMHeadModel lm_head weights"
"Calculated offsets are wrong in squad.py"
"Using fill-mask pipeline to get the \u201cscore\u201d for a result it didn't suggest"
"Roberta Tokenizer crashes when tokenizing  empty string in 2.8.0"
"run_glue.py example doesn't work for distilbert models"
"Pre-trained BART performance on XSum lower than expected"
"T5 prediction using fine-tuned model"
"A bug in the padding of input examples in the NER fine-tuning example"
"How to speed up getting answers?"
"Aborted (core dumped) or Kernel dies"
"What are the GPU RAM requirements of popular models?"
"Tokenizers Notebook  Issue"
"getting random results when running run_glue"
"lowercasing on LM with cased models"
"[readability] Consolidate prune_heads logic to PretrainedModel."
"[readability] consolidate examples/summarization/bart  and examples/summarization/t5"
"ModuleNotFoundError: No module named '__main__.utils_summarization'; '__main__' is not a package"
"Can't install transformers in conda environment"
"Faster mask computation "
"AlbertModel output is not HalfTensor when using apex fp16"
"The issue I met when do the NER task for Universal language by using XLM-R"
"i want help to create saved model(.pth) from Pytorch Dump(pytorch_model.bin) if possible!"
"Transfo-XL cannot generate long texts. Using run_generation.py to generate texts"
"Cutom tokenizer not loaded in AutoTokenizer"
"Different output encode and encode_plus"
"Decoding predictions for masked language modeling task using custom BPE"
"Reproducing squad score with TFXLMRoberta?"
"list index out of range error when I execute a command with examples/run_glue.py"
"Roberta (and BERT) tokenization converts \"do not\" to \"don't\""
"Share more details on fine-tuning GPT-2 on WikiText-2 ?"
"Electra for question answering"
"Bug in run_glue "
"'pad_to_max_length' in Pipeline should be set to True by default"
"How properly apply a tokenizer map function to a Tensorflow batched dataset?"
"TFT5: get_input_embeddings() and get_output_embeddings()"
"How to use fine-tuned BART for prediction?"
"Bug in optimization_tf   create_optimizer "
"Write with transformers demo hardware"
"ValueError: Unable to set proper padding strategy as the tokenizer does not have a padding token. "
"How to do parameter sharing between two BERT models"
"Cannot convert RoBERTa to tflite model"
"Summarisation tuning"
"Tokenization issue with RoBERTa and DistilRoBERTa."
"unable to load model 'bert', tensor 'input_ids': the model expects 1 dimensions but the model configuration specified 2 dimensions"
"ImportError: cannot import name 'HfArgumentParser' from 'transformers'"
"bert summarizer module import error"
"Tokenizer could accept a string tensor"
"torchscript tests fail with RuntimeError: normal_ expects std > 0.0, but found std=0"
"Call to torch.pow() passing integer as exponent isn't per PyTorch docs"
"T5 Translation Error "
"How to reduce random summary generation of BART Summarization models?"
"ImportError: cannot import name 'MODEL_CLASSES' from 'run_glue' "
"When will ELECTRA pretraining from  scratch will be available?"
"Replace `config.output_hidden_states` parameter with function argument `output_hidden_states`"
"Replace `config.output_attentions` parameter with function argument `output_attentions`"
"No longer able to fine-tune GPT2 using provided examples"
"Problem trying to run AlbertForMaskedLM on Colab TPU: TypeError: can't pickle torch._C.ScriptFunction objects when calling xm.send_cpu_data_to_device(model, dev) "
"Pretrain From Scratch using Google TPU"
"How to find a correct place of original word from the list of predicted words from GPT-2 model?"
"pytorch lightning examples doesn't work in multi gpu's with backend=dp"
"encode_for_summarization function did actually add CLS and SEP to separate sentences"
"Allow one to return encoder attentions in seq2seq generation"
"\u2753 Summarization example : Why no shuffling ?"
"Can not import DataCollatorForLanguageModeling"
"FileNotFoundError: [Errno 2] No such file or directory: 'mnli/dev_matched.tsv'"
"run_bertology: AttributeError: 'NoneType' object has no attribute 'abs'"
"ImportError: cannot import name 'DataCollatorForLanguageModeling' in run_language_modeling.py"
"How to fine-tune DialoGPT with your own data?"
"NLQ application"
"How to only use part of pretrained model "
"RuntimeError: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237"
"T5 shows weird behaviour in a sanity check of its pre-training task."
"how i pass from an AutoModelWithLMHead to a GPT2DoubleHeadsModel ?"
"BERT TF-Lite conversion not working in TensorFlow 2.2.0"
"GPT2 generations with specific words"
"Type Hints for modeling_utils.py"
"ImportError: cannot import name 'DataCollatorForLanguageModeling'_File \"run_language_modeling.py\""
"Unknown Device when training GPT2 with TPUs in Colab"
"New run_language_modeling.py continuing trainng"
"xlm-roberta (large/base) : run_language_modeling.py cannot starting training"
"run_language_modeling.py line 251: checking if it is a directory"
"New run_language_modeling.py does not save vocab.txt and tokenizer_config.json"
"LineByLineTextDataset limits the total number of examples to 50000 documents"
"Using the default trainer args"
"\u2753 DistilBert test perplexity based on WikiText-2: ppl is too low?"
"How can i finetune an encode-decoder combination?"
"Loading a TF pretrained model into BertForSequenceClassification module"
"How to feeding hidden state vectors from one transformer directly into a layer of different transformer"
"Pytorch 1.5 DataParallel"
"What should I do if I want to change the padding idx in pytorch bert(Huggingface)?"
"[Benchmark] Parameter settings of XLM-R on NER tasks"
"Decoding output sequences from TFGPT2Model"
"XLNetLMHeadModel: target mapping with num_predict > 1 and labels not working"
"How to input hidden state vectors from GPT2Model directly into mc_head of the GPT2DoubleHeads Model?"
"Trainer: distributed eval"
"BertConfig.to_json_file('config.json') saves  \"num_labels\"  as  \"_num_labels\" on Google Colab"
"ImportError: cannot import name 'DefaultDataCollator'"
"Many tests fails with PyTorch 1.5.0"
"After reading the tutorial I can use the BertModel to extract word embedding but how to use it extract the sentence embedding?"
"In run_xnli.py, output_dir seems to be used in place of tokenizer_name"
"run_xnli doesn't execute"
"BertForSequenceClassification is not optimum"
"GPT2 is not fully torch.jit.trace-able"
"RuntimeError: Error(s) in loading state_dict for BertForTokenClassification"
"model.multiple_choice_head( ) function for Hugging Face GPT2 models"
"AttributeError: 'LambdaLR' object has no attribute 'get_last_lr'"
"Question regarding glue examples"
"There are some warnings when I used AdamW and pytorch1.5."
"`BertTokenizer.from_pretrained()` not working with native Python `pathlib` module"
"run_language_modeling, RuntimeError: expected scalar type Half but found Float"
"Error on dtype in modeling_bertabs.py file"
"Problem with downloading the XLNetSequenceClassification pretrained xlnet-large-cased"
"Sized Fill-in-the-blank or Multi Mask filling with T5"
"Weights from pretrained model not used in GPT2LMHeadModel"
"xlnet large "
" When I run `import transformers` , it reports an error. But I had no problem with pyrotch-transformers before. Is it a TensorRT problem?"
"Can't install transformers from sources using poetry"
"MLM Loss not decreasing when pretraining Bert from scratch"
"why the accuracy is very low when we test on sentences one by one of the words input?"
"Using the T5 model with huggingface's mask-fill pipeline"
"Run Multiple Choice failure with ImportError: cannot import name 'AutoModelForMultipleChoice'"
"Language generation not possible with roberta?"
"RuntimeError: Creating MTGP constants failed. at /opt/conda/conda-bld/pytorch_1549287501208/work/aten/src/THC/THCTensorRandom.cu:35"
"Toy size models versions for faster experimentation and testing"
"Question about the output linear weight  "
"How do I convert T5 checkpoint to pytorch bin to utilize fine-tuned model from finetune.py?"
"TF BART ?"
"\ud83c\udf1f Transformer Lite"
"Maybe it is a bug for Roberta vocab file."
"ALBERT with Masked Language Model Input Processing from SavedModel"
"Fast Tokenizers do not work when `return_offsets_mapping=True, return_tensors=\"pt\"`"
"fairseq-preprocess, Why are the number of rows in src and trg different?"
"Report minimum system requirements for each architecture."
"Attribute error while using run_language_modeling.py with Transformer-XL"
"test_lm_head_model_random_*_generate fail on GPU"
"Fast Tokenizers: `batch_encode_plus` error"
"Text Generation generated <|endoftext|>"
"String Format should be more pythonic"
"Pass existing tensorboard SummaryWriter to Trainer. "
"T5 Tokenization of unique masked tokens (<extra_id_1>) is incorrect"
"NER Pipeline with CamemBERT not showing entities"
"TFBert: Out of memory error when acting on a strided slice of input."
"Training a new language model with custom loss and input representation"
"CamembertForSequenceClassification not initialized from pretrained model"
"TF2 - how to access intermediate layers of pre-trained bert model?"
"\ud83d\udc1b Saving TF model : Expected Operation, Variable, or Tensor, got None"
"BertForSequenceClassification producing same output during evaluation "
"torch num_samples=0 error on XLMnet @ run_language_modeling.py"
"GPT-2 models are unpickable"
"Add BPE dropout to tokenizers"
"Email Alerts: Run failed: Torch hub integration"
"GPT2LMHeadModel Documentation Mismatch for labels"
"Why is the pooler output used for sequence classification (if it does not represent the input semantic well)?"
"How to run squad for chinese dataset? "
"Dropout training"
"Cannot Load bert-base-japanese tokenizer"
"KeyError ' '- run_ner.py - Transformers 2.8.0"
"Why GPT2LMHeadModel's loss mismatches accuracy?"
"Is there pre-train bert or xlnet from scratch code ?"
"No CORS Policy for Write With Transformer Endpoints"
"Native integration with pytorch/serve"
"How can I continue finetuning from checkpoint using the NER script?"
"Save T5 model as H5 and convert it to model.json to be used in TensorflowJS"
"Issue with non-text files and bertabs example"
"How to using vocab other language in run_ner.py?"
"[docs] bad RST in PretrainedModel.generate docstring"
"Using 'ner' task in pipeline with a non default model gives me entities as \"LABEL-6\" , \"LABEL-8\"  instead of \"I-ORG\" and \"I-LOC\""
"Load of pre-trained t5 model from Tf to HuggingFace"
"tokenizer.encode_plus stopped returning `attention_mask`"
"Examples link to the master branch which seems to be ahead of the pip install version?"
"Zero-shot PPLs vary wildly across gpt2 model sizes"
"Why is masking ID optional?"
"feature-extraction pipeline is second last layer better representation than the last hidden layer"
"Use roberta-med or scibert for fillmask"
"Not able to reproduce same CoLA result as huggingface defualt"
"InvalidArgumentError: Incompatible shapes: [5,20] vs. [5,18] [Op:Less]"
"GePpeTto!"
"Character level models?"
"Finue-tuning T5 model"
"Negative dimension when initialising the XLNetModel"
"Defaults models for different pipelines"
"Masking in Bert"
"AttributeError: 'NoneType' object has no attribute 'abs' when run example/run_bertology.py"
"model from path 16-bits training:True but float16 false"
"BERT as encoder and a transformer as a decoder."
"`eos_token_id` breaks the `T5ForConditionalGeneration`"
"ValueError: You are attempting to pad samples but the tokenizer you are using (GPT2Tokenizer) does not have one."
"convert pytorch_model.pt [pretrained Bert model] to pytorch_model.onnx (ONNX)"
"Is it possible to have high perplexity of some individual sentences, as compared to overall testing corpus perplexity?"
"No decoder specific error message for T5Stack.forward"
"Model output should be dict"
"Parse in tensorflow strings as well as normal strings"
"GPT-2 past behaves incorrectly when attention_mask is used"
"problem about change from pytorch-pretrained-bert to transformers"
"jplu/tf-xlm-roberta-large showing random performance"
"Why does ALBERT use einsum in PyTorch implementation while in TF one it does not?"
"Cannot loading SpanBERT pre-trained model "
"Error: `cannot import name 'TFBertForMaskedLM'`"
"Cannot set max_position_embeddings to any desired value in T5Config"
"How to make run_language_modeling.py work for transformer-xl?"
"\ud83d\ude80 An advice about changing variable name from \"attention_mask\" to \"adder\""
"New model addition: Blender (Facebook chatbot)"
"Use finetuned-BART large to do conditional generation"
"tokenizer.batch_encoder_plus do not return input_len"
"Error in Calculating Sentence Perplexity for GPT-2 model"
"DistilBertForQuestionAnswering returns [UNK]"
"Fine-tuning T5 in Tensorflow"
"Config File"
"How to pre-train BART model"
"Embedding index getting out of range while running camemebert model"
"num_samples=0 when using pretrained model"
"[Marian] Readme parser defaults to porting oldest model"
"[Marian] @-@ symbol causes strange generations"
"[Marian] Multilingual models require language codes"
"run_generation.py - use of < redirect for input text file only reads first line - solution requested"
"Cannot use camembert for question answering"
"Add a sampling_transform callback to generation for arbitrary probability-warps"
"Tapas"
"No name 'AlbertMLMHead' in module 'transformers'"
"How to position encode a sentence?"
"Encoder/Decoder generation"
"ImportError: cannot import name 'AutoModel' from 'transformers'"
"Make ElectraPreTrainedModel importable"
"args.output_dir seems like been ignored"
"Not able to import certain packages"
"[Marian] Key-Error for some languages"
"Is it possible to get document embeddings using GPT-2? If so, how?"
"New model request: MobileBERT from Google "
"How to apply Torchtext convenience classes to prepare data for a Transformer?"
"Bug: can not use pretrained BERT on multiple GPUs with DataParallel (PyTorch 1.5.0)"
"Language model training with NSP using run_language_modeling.py"
"AutoTokenizer not able to load saved Roberta Tokenizer "
"[Proposal] Small HfAPI Cleanup"
"KeyError with a fine-tuned model"
"Issues with RobertaTokenizer and unicode characters"
"How to get output from BERTNextSentencePrediction by passing One Sentence and Next Sentence getting predicted"
"BertForTokenClassification, logits on sequence tagging task "
"Perplexity in T5/BART"
"GPT2Tokenizer.decode() slow for individual tokens"
"[examples] text_classification/run_pl.sh error"
"Need correction in GPT-2 perplexity equation based on computation"
"TPU Trainer's PerDeviceLoader has no len()"
"Bart now enforces maximum sequence length in Summarization Pipeline"
"[Reformer/Longformer] Add cross-attention layers for Encoder-Decoder setting"
"2.9.0 Padding Bug"
"Reformer tokenizer not working"
"Problem with BertTokenizer using additional_special_tokens"
"Strange difference between NER results in 2.8.0 and previous versions"
"MarianMTModel: Runtime Errors"
"How can I mute the info from BertTokenizerFast.encode_plus"
"[infra] make a tiny \"distilroberta-base\" to speed up test_pipelines.py and test_examples.py"
"Error importing Reformer model"
"Only self-attention returned when output_attentions = True for BERT"
"distilbert-base-uncased"
"RuntimeError: expected device cpu but got device cuda:0"
"Reformer self attention mask value not converted in apex half precision"
"Structuring Dataset for Fine Tuning GPT-2 with Song Lyrics?"
"sentiment analysis pipeline provides option to output the original rating than positive/negative tags"
"[Request] NER Scripts on CoNLL 2003 dataset "
"Encoder - Decoder loading wrong weights and missing decoders"
"How to specify a TPU IP for run_tf_glue.py?"
"'utf-8' codec can't decode byte 0x80 in position 229888: invalid start byte"
"Vaswani's Transformer (Encoder-Decoder) decoding methods"
"Model call with `inputs_embeds` does not match that with `input_ids`"
"Strange Behaviour using Transformer pipline with FLASK"
"I am training a bert model for question classification task, now it is a binary classifier. 0 for Descriptive question and 1 for not descriptive. While testing I am getting the following error :-"
"NER Models not using BIO tagging scheme"
"Sentiment Analysis Pipeline is predicting incorrect sentiment in 2.9"
"[DOCS] BertModel documentation doesn't mention the usage of tokenizers for creating input_ids"
"KlDivBackward"
"Evaluating a checkpoint"
"FileNotFoundError when running distributed Trainer"
"Wrong returns of API:get_special_tokens_mask()"
"training script for any tensorflow based pre-trained model for question-answer"
"Use BART for longer documents"
"Could you help with a huggingface/transformers pretrained models download link?"
"Default code snippet for shared model doesn't work because of different model and tokenizer types"
"T5 fp16 forward yields nan"
"TAPAS: Weakly Supervised Table Parsing via Pre-training"
"No crossattention layers in decoder from pretrained encoder-decoder model"
"Error Training T5 Transformer Translation Model"
"Issue with XLNet using xlnet-base-cased"
"why squad.py did not reproduce squad1.1 report result?"
"How to perform Agglomerative clustering on Albert model last hidden state output, it gives tuple of tensors for each sequence in a sentence. I am processing multiple sentences from a document, How to achieve this ?"
"Latest Version Of HuggingFace is Not Using GPUs"
"Multiple token prediction with MLM"
"why Building wheel for tokenizers (PEP 517) ... error?"
"MPNet: Masked and Permuted Pre-training for Language Understanding"
"Finetuning error: RuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel: Missing key(s) in state_dict: \u201ctransformer.h.0.attn.masked_bias\u201d, \u201ctransformer.h.1.attn.masked_bias\u201d"
"MobileBert form google-research added"
"RobertaForQuestionAnsweringModel "
"how can I make BERT word embedding faster?"
"run_generation.py GPT2 model only using 1st GPU, OOM error"
"How to use DistilBertTokenizer in C++"
"AutoModel.from_pretrained with torchscript flag raises a TypeError: __init__() got an unexpected keyword argument 'torchscript'"
"Uploaded models not appearing on website"
"Train model from scratch with Tensorflow"
"\ud83d\udc1b Trainer on TPU : KeyError '__getstate__'"
"Cache directory"
"Can't set attribute 'device'"
"Extractive Text Summarization"
"[bug in run_glue.py] GlueDataset with no local_rank when init"
"Unable to load weights from pytorch checkpoint file"
"Wrong dimensions of input to loss function, multilabel classification"
"can't load checkpoint file from examples/run_language_modeling.py"
"Support multitask learning"
"[docs] XLNetLMHeadModel example in documentation does not produce the right probabilities"
"Discrepancy in the generation of T5 here vs the original code "
"TracerWarning on modeling_gpt2.py:147 when using Torchscript"
"Ability to specify directory of tensorboard logs in BART finetuning example"
"RobertaForSequenceClassification for BERT"
"Not getting expected results for freshly-pretrained BERT "
"DistilGPT2 Finetuning with GPU"
"GPT2 checkpoint breaks on new transformers version (2.9.1)"
"Trainer and Colab TPU: Training loss isn't declining"
"LayerNorm not excluded from weight decay in TF"
"Trainer doesn't calculate eval loss for models with lm_labels parameter"
"Trainer crashes on TPU at eval time when prediction_loss_only is True "
"Can not reproduce article numbers"
"Has anyone successfully used TF2.0 to load pre-trained transformer-XL (wt103) weights and reproduce their sota results"
"[pipelines] Failing @slow test for TF Summarization"
"past functionality broken in release 2.9.0 and 2.9.1"
"demo website: i info icon should link to resource about parameters"
"run_squad with early stopping on a validation set"
"Save pretrained MarianTokenizer"
"Speed on various cards"
"Error on instantiating pipeline.summarizer"
"MNLI finetuning results affected by values of max_steps"
"max_qa_length is needed for funetune on multiple-choice problems"
"Unknown task fill-mask"
"Need clarity on training Albert from scratch"
"Issue with lr during training from scratch"
"Finetuning BERT classifier on a non-GLUE dataset in GLUE format"
"[Bart/Marian] ignore output_attentions when invoked through AutoModel"
"[docs] AutoModelWithLMHead(model_name, **kwargs)"
"[PretrainedTokenizer] is  <unk> a special token?"
"MarianMTModel translate {tgt}-{src}"
"BertTokenizerFast does not load custom vocab created from Tokenizer library"
"the special token of XLNet"
"MarianMT = How to return 5 best candidates for a translation.  "
"Wrong model or tokenizer for MarianMT"
"Training TFBertForQuestionAnswering on custom SquadV1 data"
"Trainer is missing sampler.set_epoch for distributed mode"
"Pipeline for question generation"
"BertWordPieceTokenizer cannot be pickled"
"Run Language Modeling on 8 TPU cores doesn't seem to terminate"
"Summarization Fine Tuning "
"Request to add MobileBert"
"Pipeline for Conditional Generation (T5 type models)"
"Tensorflow NER Training script Not working"
"Get BERT sentence encoding"
"GPT2 perplexity rolling/striding way for evaluating a document."
"TypeError: add_() takes 1 positional argument but 2 were given"
"Scaling text classification / reusing models"
"BERT Tokenization problem when the input string has a \".\" in the string, like floating number"
"How to change transformers model embedding layer weights"
"BERT and other models pretraining from scratch example"
"Lack of funetune examples for T5 model"
"How to extract the best candidate after token classification?"
"mbart config.json missing"
"\ud83d\udc1b Weird learning rate with TPU Trainer"
"albertModel object has no attribute bias"
"BERT Fine-tuning problems"
"Reformer training error"
"Issues with the EncoderDecoderModel for sequence to sequence tasks"
"model.save() does not save keras model that includes DIstillBert layer"
"Generation with EncoderDecoder Model"
"TF Beam Search generation seems to be flaky sometimes"
"[Questions & Help] The loss doesn't decrease correctly while training BERT from scratch"
"\u2753 Warning : This overload of addcdiv_ is deprecated"
"Value matrix of self-attention"
"Bug - TFBertForSequenceClassification on SQUaD data"
"DMOZ - web page classification / multi-language"
"get output from a particular layer of pre-trained transformer (xlnet) "
"Problems About Using the Run_language_modeling with Tf2."
"FastTokenizer add_special_tokens also adding individual characters for multi character tokens"
"Pretrained Transformer-XL gives unreasonable result on WikiText-103"
"ProjectedAdaptiveLogSoftmax.log_prob raises Exception"
"batch_encode_plus returns same lengths when enable pad_to_max_length"
"Request for hosting model files in a Virtual Hosted-Style S3 buckets"
"Tokenizer encode to have an option to overflow from left"
"\u2753 [TPU] [Trainer] Moving model to device before setting optimizer slow the training "
"Trying to add support for GPT2 as decoder in EncoderDecoder model"
"Bug using Roberta models in QA Transformers pipeline."
"Can't find vocabulary file or is corrupted for MarianTokenizer"
"tokenizer.vocab has not changed after using add_tokens"
"How to load a pruned Albert model with from_pretrained()?"
"Windows: Can't find vocabulary file for MarianTokenizer"
"Cannot load reformer-enwik8 tokenizer"
"Incorporate HuggingFace 'nlp' library in examples"
"\u2753 [BART] Why Decoder Layer Normalization is applied only at the last layer ? "
"python run_glue.py with the AttributeError: 'NoneType' object has no attribute 'seek'"
"Tokenize something with a \".\" in between Decode these ids, you will find it mismatch"
"Pre-trained electra-large model doesn't converge when fine-tuned on SST-2"
"Pipelines do not control input sequences longer than those accepted by the model"
"How to finetune ELECTRA on glue?"
"SummarizationPipeline crashes"
"Hard-coded force_download in run_squad forces expensive community download "
"FillMaskPipeline crashes when executed on TPU"
"AttributeError: 'SummaryWriter' object has no attribute 'add_hparams'"
"ValueError: TracedModules don't support parameter sharing between modules"
"Couldn't reach server GPT-2 "
"\u2753 How Linear layer difference between TF2 and PT are handled ?"
"sometimes loss starts with nan when running \"Quick tour TF 2.0 training and PyTorch interoperability\" script"
"How to train a custom seq2seq model with BertModel"
"[marian] possible memory leak problem while translating & extracting internal representations"
"How to use its own custom Optimizer (GLUE Example)"
"Using DistillBert to train Bert (run_languge_modeling.py) for some languge from scratch"
"Can't reproduce export to onnx with custom bert model"
"Codecov migration to marketplace app"
"Error in Longformer attention mask using apex mixed precision"
"Tokenizers bug: version 2.10 doesn't honor `max_len` when instantiating a pretrained model"
"Minor correction in Roberta Model docs, Roberta doesn't use NSP"
"MMBT doesn't inherit from nn.Module"
"How to speed up inference step in BertQuestionAnswering?"
"resize_token_embeddings not implemented for TFGPT2LMHeadModel"
"InvalidArgumentError while using GRU layer in custom training loop"
"'use_fast=True' results in 'TypeError' when trying to save tokenizer via AutoTokenizer"
"RuntimeError: The size of tensor a (1025) must match the size of tensor b (1024) at non-singleton dimension 3"
"Automatically setting number of LSH buckets in Reformer may give invalid value"
"seems that run_ner.py cannot handle the situation when example length exceed max_length? "
"pass lowercase to fast tokenizer"
"Example script for SQuAD question answering unable to reproduce the claimed performance"
"XLnet loss and accuracy not decreasing"
"Albert Tokenizer hangs"
"implementation of transformers for abstractive summarization task"
"Decoding with DistilmBERT to generate text in different languages"
"changing config.axial_pos_shape for 'ReformerModelWithLMHead' when fine-tuning "
"\u2753 [BART] Why using bias for LM head if not trained ? "
"\u2753 [BART] Different embedding sizes between pre-trained / fine-tuned checkpoint"
"bert embedding make OOM in albert"
"cannot import name 'TFElectraModel' from 'transformers'"
"Typo in GPT2 documentation "
"Transformers' trainer sequence classification problem"
"Onnx notebook problem"
"OSError: Model name 'transfo-xl-wt103' was not found in tokenizers model name list (transfo-xl-wt103). We assumed 'transfo-xl-wt103' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.bin', 'vocab.txt'] but couldn't find such vocabulary files at this path or url."
"Using whole word masking on training LM from scratch"
"How to save tokenize data when training from scratch"
"Provide simple way to train a new translation model from scratch"
"T5Model in fp16 still yield nan with more complex examples"
"Help Wanted: Predict Next Two Tokens"
"[Model hub web parsing MD code error]"
"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
"KeyError: \"Unable to open object (object 'bias:0' doesn't exist)\""
"KeyError when loading a trained EncoderDecoder model"
"AttributeError: 'Namespace' object has no attribute 'to_json_string'"
"ImportError: cannot import name 'AutoModelForQuestionAnswering' from 'transformers "
"Functionality for addressing imbalance data points?"
"Which models can be using for encoder-decoder?"
"Inconsistency in how Electra doing sentence level prediction"
"How to deal with summarization task to long sequences input? "
"Key error while evaluating the Language Model finetuning"
"Use fill-mask pipeline to get probability of specific token"
"What does the output of feature-extraction pipeline represent?"
"run evalation after every epoch in Trainer"
"The new abstractions in /master are counterproductive"
"GPU memory usage"
"`train.jsonl` file missing in MM-IMDb task"
"Can't see logger output"
"How to use run_glue.py with tensorboard?"
"Model evaluated at each checkpoint, but results not in checkpoint file"
"Numpy format string issue in TFTrainer"
"tensorflow2_gpt2 Slow speed"
"03-pipelines.ipynb on Colab: error on \"Summarization\""
"How to generate prediction/answer from a custom model fined-tuned/trained for self-defined questions?"
"Error when loading a trained Encoder-Decoder model. "
"question-answering examples bug in pipelines document"
"Encode-Decode after training, generation gives the same results regardless of the input"
"Tokenization_utils doesn't work with Pytorch-Lightning on 2.10.0 version"
"--fp causes an issue when running example scripts in distributed mode"
"Add upcoming GPT-3 model"
"Assert message error in Reformer chunking"
"Write With Transformer: PPLM page is broken"
"run_tf_ner.py cannot run"
"End-to-end object detection with Transformers"
"run_tf_ner.py TFTrainer logdir cannot be none"
" Colab crashes due to tcmalloc large allocation"
"Cannot load labels from old models"
"Coversion between tokenizers"
"get_from_cache in file_utils.py gobbles up error in making url requests"
"QUESTION: How do I know what type of positional encoding to input during fine-tuning or pretrained BERT?"
"KeyError in Camembert in QuestionAnsweringPipeline"
"Gpt2 generation of text larger than 1024"
"Documentation for non-nlp experts"
"GPT-3"
"XLNet Generation appears to reference padding text in run_generation script"
"when I encode [unused1], return not one token"
"AutoModel.from_config loads random parameter values."
"Compressive Transformer"
"Same logits value for different input"
"Keyword errors on tokenizer.encode_plus"
"Gradient overflow issue when i try to train gpt2 with run_language_modeling in fp16 with 02. Any idea why that maybe happen?"
"TypeError: cannot create 'BPE' instances"
"Adding Neutral Score "
"Please add the functionality to save tokenizer model for run_language_modeling.py"
"Loading config file bug"
"SpanBert always predicts the same token"
"Transformer-XL: Input and labels for Language Modeling"
"NER example doesn\u2019t work with tensorflow"
"Why we need the init_weight function in BERT pretrained model"
"Why DataCollatorForLanguageModeling do not make attention_mask feature?"
"Fused_norm_layer_cuda"
"Tensorflow XLMRoberta Multi-Class Problem"
"Is transformers 2.11.0 compatible with tokenizers 0.8.0(-dev*)?"
"When using the Hugging Face Transformer, if I set my pad_token to be a different token then the default, do I need to train my model on that new pad_token as well?"
"How tokenizers work"
"Is the separation token absolutely necessary if I use GPT2DoubleHeadsModel with token_type_ids?"
"Wrong argument passed during TFRobertaClassificationHead initialization"
"Converting model to pytorch"
"Is there any need to fine-tune the already pre-trained GPT-2 models?"
"ImportError: cannot import name 'MODEL_WITH_LM_HEAD_MAPPING'"
"how to make a multi-task deep neural network baseline using huggingface transformers?"
"Can I save a word embedding from BERT and used again later for computational purpose(as BERT takes much more time). Just like in Glove. If yes then How? and is this good idea to do so?"
"About `do_basic_tokenize` behavior in BertTokenizer"
"Save & load sparse models from the models database"
"Albert pretraining loss not decreasing"
"[Feature request] Support batched conditional generation from GPT-2 "
"When I use TFBertEncoder in my laptop, I get an error.I can not build a model. Here is a simple examples."
"TFTrainer: Checkpoints not getting saved in `output_dir` but in {cwd}/checkpoint"
"Bart model for textinfilling"
"BertModel Inputs"
"Question Answering Modeling through Hugging Face Models"
"Extending run_language_modeling.py for XLNet"
"Can't find config.json"
"Perform evaluation on HANS with Trainer (like GLUE example)"
"How to use pretrained model for inference?"
"Why I can't generate phrases in batches if I include an attention mask? (GPT2)"
"QuestionAnsweringPipeline query performance"
"Hugging Face GPT-2 Tokenizer"
"Tokenizer.encode documentation not correct"
"Batching not speeding up Transformer-XL"
"Can I use TorchText Iterator output as the input_ids for Hugging Face Transformer?"
"bart-large-cnn model weights updated?"
"run_ner.py crashes with RoBERTa because of incorrect sequence length"
"run_tf_ner.py output_dir/saved_model empty"
"Fine-tuning of RoBERTa"
"KeyError in Pipeline Question Answering with LongFormer"
"GPT2TokenizerFast raises pad_token error even if not used"
" Cannot load pretrained model from repo."
"Issue with HANS evaluation"
"Model is running on special characters and word pieces for token classification"
"Bert (sentence classification) output is non-deterministic for PyTorch (not for TF)"
"Correcting path to pplm examples"
"The purpose of files merges.txt, special_tokens_map.json, training_args.bin and add_tokens.json"
"\ud83d\udc1b [BART] Pipeline OOM "
"Reformer hidden_size of output is doubled."
"Regarding generate method used in BART"
"\u2753 How to use Gradient Accumulator in TF_Trainer ?"
"question about tokenizer"
"Can I train question-answering on TPU using Huggingface"
"Question Answering Pipeline with big texts."
"Usage of \u0120 in BPE tokenizer"
"\ud83d\ude80 [Feature Request] Add self-contained browsable examples/notebooks in the docs "
"Onnx conversion for bert models with classification layers"
"\ud83d\udc1b run_ner.py runtime error linked to TPU training"
"Write With Transformer Request:"
"[ctrl] has broken code for pruning that is not tested"
"pip install -e does not always install the correct isort version"
"Invalid Argument for Onnxruntime Inference on GPT2"
"Albert pretrained weights change across runs. "
"Is albert lm finetuning with SOP in Pytorch supported?"
"TPU Training fails with --evaluate_during_training"
"NER pipeline: Inconsistent entity grouping"
"Question: Where do I find the Transformer model from the paper \"Attention is all you need\" ?"
"EncoderDecoderModel forwards return different values every time."
"Discriminative fine-tuning for new (added) words"
"Top-k sampling and top-p sampling for generating phrases on batches with GPT-2?"
"Onnx converted model has its output shape modified when compared to original (finetuned) model"
"How to remove token ?"
"`run_glue.py` fails with models `bert-base-cased`, `distil-bert-cased`, others"
"Why exclude LayerNorm.bias from weight decay when finetuning?"
"GlossBert adding"
"Why init specific layers rather than whole model  in BART"
"Any reason why BART does not have a ForTokenClassification variant?"
"[Bert Model] ValueError: not enough values to unpack (expected 3, got 2)"
"BUG while calculate LM loss in AlbertForMaskedLM"
"Multi-output regression support for Transformer models"
"[Benchmark] Add optimization notebook"
"remove words from vocabulary"
"Add support for Funnel-Transformer"
"Memory issue in Transformers"
"Add optimal model size and stopping time feature"
"TFXLMRobertaForSequenceClassification: call() got an unexpected keyword argument 'labels'"
"issue in pretraining language model with checkpoint "
"Tensorflow Glue example script for finetuning not usable with DistilBert"
"sentencepiece==0.1.92 causing segmentation fault"
"Add support for DeBERTa"
"Memory issues in Transformers"
"ROUGE_L score of summarization/t5 is very lower than that of paper."
"can anyone tell me how to do the pretraining of Reformer model on my text data?"
"how to extract several layers of BERT or GPT as a new model?"
"how to train mask model e.g Bert using WordPieceToken"
"Funnel Transformers"
"tokenizer.encode_plus stopped returning `attention_mask` and pad_to_max_length"
"KeyError in Camembert in QuestionAnsweringPipeline"
"Inconsistent number of vocab from pretrained T5Tokenizer and T5ForConditionalGeneration"
"ProphetNet"
"AutoModelForSequenceClassification not working with prunebert model"
"Previous commit introduces bug in `convert_pytorch_checkpoint_to_tf2.py`"
"encode_plus( ) function for the GPT-2 Tokenizer"
"\ud83d\udc1b [TFTrainer] `dataloader_drop_last` unused"
"Training RoBerta using transformers on masked language task giving weird results"
"\ud83d\udc1b TPU Training broken due to recent changes"
"\ud83d\ude80 Add early stopping to the trainer"
"How do I fine-tune hyperparameters for a model from Huggingface library"
"KeyError when using non-default models in Huggingface transformers pipeline"
"Error using inputs_embeds argument in TFXLNetModel"
"Latest version of transformers available via conda-forge?"
"[cleanup] Hoist ModelTester objects to toplevel"
"[How to] Carefully designing the head of a Transformer model?"
"TypeError: export() got an unexpected keyword argument 'use_external_data_format'"
"ModuleNotFoundError: No module named 'xml.sax'; 'xml' is not a package"
"Simple way to convert a Python tokenizer to a fast tokenizer"
"[generate] Increasing length_penalty makes generations longer"
"Pegasus for summarization ! "
"File is not found due to extension"
"Unexpected behavior encoding token_type_ids in GPT models"
"Documentation doesn't include instructions for applying BertModel to documents using GPU acceleration"
"Incorrect loss values calculated for TPU training."
"sentencepiece dependency must be a specific version. "
"BertTokenizer from own vocab meet problem"
"Training ELECTRA model on TPU with the help of Trainer or TFTrainer classes"
"Using LongformerForQuestionAnswering on large documents (40K+ characters)"
"name 'ElectraForSequenceClassification' is not defined"
"What is the different options for pooler_type in Bert config ?"
"T5ForConditionalGeneration fp16 issues"
"Dataloader in Trainer num_workers > 0"
"Unable to evaluate on fine-tuned bart for summarization"
"Trainer.evaluate does not support seq2seq models"
"GPTDoubleHeadsModel Unexpected node type: onnx:: Sub "
"Data used for training MarianMT models"
"How to use fine-tuned BERT to fill <mask>"
"RobertaForMaskedLM Failing for example code given on Hugging Face' documentation page"
"Memory leakage with bert-large-uncased-whole-word-masking-finetuned-squad"
"Issue with an inline code comment"
"unexpected keyword argument 'lm_labels' when using BertModel as Decoder with EncoderDecoderModel"
"Extending Encoder Decoder to GPT-2"
"How to implement differential learning rates and still ensure \"weight_decay\" = 0 for the the parameters it should?"
"Cannot load optimizer and lr_scheduler states with TPU training"
"GPT: Weights not being initialized"
"How to Paraphrase with GPT2?"
"Spanbert TACRED model not found, despite model card"
"Add Linformer model"
"Request: pretrained distilgpt2-medium, distilgpt2-large models"
"huggingface distillbert classification using multiprocessing"
"Run run_tf_glue.py has bugs"
"How to make my own dataset to use BART summarization?"
"keras"
"Why run_language_modelling.py does not use segment embeddings or language embeddings?"
"Getting very bad F1 Scores when training SQUAD v2.0 with robertadistil-base"
"FillMaskPipeline return word-piece "
"Word Embedding input to GPT-2"
"BertTokenizer: ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
"error while instantiating model"
"Save the Dataset for training GPT2"
"evaluating with trainer.py with TPU results in sudden RAM spike and crash"
"[TFTrainer] Tensorflow Warning : experimental_run_v2 is deprecated"
"Importing transformers causes segmentation fault when setting cuda device"
"\ud83d\udc1b TFTrainer not working on TPU (TF2.2)"
"\u2753 How to use TFTrainer on TPU ? Unable to destroy remote tensor handles"
"Accessing scores for the entire vocabulary in GPT2"
"Segmentation fault when loading pretrained file"
"Illegal memory access (cudaErrorIllegalAddress)"
"Cannot import transformers due to issue with signal.py (SIGKILL)"
"[\ud83d\ude80 Feature request] upload bart-base checkpoint"
"How can I print output attention on test data?"
"Cached TF files cannot be loaded with `from_pretrained` without internet connection"
"Add CRF layer after Transformer model"
"bart-large-xsum config task_specific_params['summarization_params'] wrong"
"Not able to reproduce XNLI results from Google's mBERT weights."
"Run error when run PPLM in example!"
"Latest merge [Benchmark] Memory benchmark utils #4198 fails at Windows"
"Multi class classification using Reformer Model"
"Error while trying to retrieve BERT embeddings for a custom task"
"The correct way to save and load pretrained MarianTokenizer?"
"Cannot save and load pretrained MarianTokenizer"
"\"AutoTokenizer.from_pretrained\" does not work when loading a pretrained MarianTokenizer from a local directory"
"How can I use tokenizer.encode_plus to input and encode 2 sentences - (query,answer) pair for training a BERT binary classifier?"
"\u2753 [TFTrainer] How to run on 8 TPU cores ?"
"TFTrainer does not consider number of epochs when calculating learning rate"
"ref #4733"
"How to use 16 token types in pretrained Albert/BERT?"
"After I resume learning, loss is greater than prev checkpoint"
"DataCollator problem"
"TypeError: function() argument 1 must be code, not str"
"How to cosume movement-pruning .h5 models in QnA pipeline"
"T5 model for classification doesn't work properly for large number of classes."
"How can I load the finetuned BART model to memory?"
"Examples tests improvements"
"Error when loading Flaubert model "
"More flexible wandb support for Trainer"
"What do the following parameters mean during the initialization of T5 model?"
"Non-deterministic training issue on GPU: TF-BERT"
"Tokenization+Transformers works with PyTorch but not TensorFlow on TPU"
"Modify BERT/BERT-descendants to be TorchScript-able (not just traceable)"
"Errors while running pytest"
"glue.py Data Processor Index Error for Large Data"
"\ud83d\udc1b [TFTrainer] Wrong number of optimization steps"
"how to get complete URLs to weights in 2.11.0"
"Converting to ONNX doesn't apply to all models"
"Colab session crashes on transformers"
"Several problems with named entites predicted with the ner pipeline"
"How do I pre-train the T5 model in HuggingFace library using my own text corpus?"
"01_how-to-train.ipynb broken"
"Why does the T5Tokenizer prepend and '_' to every token?"
"Image GPT"
"Is there a helper script to preprocess data for T5 for masked language modeling? "
"encode_plus wrongly tokenizing a symbol"
"Different output from model on CPU and GPU"
"Addition of VisualBERT"
"Can I training a bart model from scratch by transformers?"
"Training the BERTSUM model"
"\ud83d\udc1b [TF] `create_optimizer` wrong superposition of learning rate schedules"
"Update Conda Release"
"OpenAIGPTDoubleHeadsModel Don't have the \"labels\" attributes as it is described in the documentation"
"Loading Fine Tuned BERT Sequence Model after Training "
"Trainer evaluation doesn't return eval loss for question-answering."
"Is there a helper script to randomly mask spans of text for T5 pretraining?"
"How can I initialize RobertaForSequenceClassification empty?"
"Flaky tests sometimes caused by S3 failures"
"[Marian] Run predictions on GPU? RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select`  "
"Strange exception"
"GPU out of memory with Reformer enwik8 model"
"data_collator.py does not allow NoneType labels for test set predictions on Glue"
"An Implementation of ERNIE For Language Understanding (including Pre-training models and Fine-tuning tools) "
"UnboundLocalError: local variable 'next_tokens' referenced before assignment when using Generate()"
"AutoTokenizer.from_pretrained('facebook/mbart-large-en-ro') fails"
"Fine-Tuning GPT2"
"SLOW GPU test Failures"
"Cached feature files - Naming introduces confusion/model mixing"
"How to find and use fine-tuned model for GLUE-CoLA? "
"What does adjust_logits_during_generation (formerly prepare_logits_for_generation) do?"
"src/transformers/trainer.py relies on path to infer global training steps, skips training for glue example"
"Transformer-XL tokenizer cannot properly tokenize brackets"
"xlm-mlm-17-1280: after run model to get embeddings shape 20000"
"`facebook/bart-large-mnli` - random accuracy on MNLI"
"T5 special tokens not mapped to unique indices in vocabulary"
"Passing in own embeddings for image input"
"Scoring each word from the sentence using Pretrained LM"
"TFBertForSequenceClassification: TypeError: call() got an unexpected keyword argument 'labels'"
"Lite transformer"
"new tokenizer backend breaks old code"
"BertForMaskedLM \"labels\" is an unexpected keyword"
"Transformer pipeline loading model and tokenizer on every prediction request"
"Keras model created from individual Bert Layers has weights not shown in trainable_weights nor non_trainable_weights. model.summary() / utils.plot_model shows those weights as part of graph though"
"How to  save the created embedding of the text corpus."
"Why doesn't stride in squad_convert_example_to_features\u2018s encode_plus set to doc_stride?"
"output generate scores per hypothesis/token"
"Cannot import AutoModelForSeq2SeqLM "
"results for wikitext-2 clm using GPT-2 differ between paper and example code"
"Summarization Examples: Support label_smoothed_cross_entropy"
"[Tokenizer] batch_encode_plus method cannot encode List[Tuple[str]] with is_pretokenized=True"
"Add support for `encoder_hidden_states` and `encoder_attention_mask` in modeling_longformer"
"Load a T5ForConditionalGeneration's encoder into a T5Model"
"Trying to made a keras model with transformer layers defined in hf-transformers, keep running into `AttributeError: Tensor.op is meaningless when eager execution is enabled, when trying to make a keras model`"
"object returned by  RobertaTokenizerFast() class are not serializable,"
"When is 2.12 coming out?"
"Which Marian version was used to train the Helsinki-NLP/* checkpoints?"
"Is it possible to mimic trim_batch using new tokenizer strategies?"
"Unable to load the reformer pre-trained model, connection broken after X%"
"Trouble with PL Checkpoint loading after finetuning bart-large "
"Simplify LearnedPositionalEmbedding"
"[Benchmark] Jetson Nano DistillBERT SQuAD benchmark "
"Using segments ids in encoder-decoder model in generate function"
"batch_encode_plus() causes OOM, while encode_plus does not "
"tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(x)) returning a different result"
"Linformer"
"Can you release the code for Write For Transformer? "
"T5 Model : What is maximum sequence length that can be used with   pretrained T5 (3b model) checkpoint?"
"How to build Bimodel to search code snippets? [CodeBERTa]"
"Train RobertaModel from scratch for my dataset"
"BartConfig wrong decoder_start_token_id?"
"Train EncoderDecoder Models for question generation"
"How to predict on a batch?"
"TF2 support for Longformer"
"AttributeError: module 'tensorflow' has no attribute 'repeat'"
"run_language_modeling.py does not output vocab/config/etc files until training completes"
"gpt2.generate breaks on FP16 Apex training."
"Self documenting Payload instead of Tuples as output of Transformer"
"Embedding index out of range in self"
"BertAbs run_summarization.py example fails with errors"
"BertTokenizerFast.convert_tokens_to_string converts ids to string, not tokens to string"
"Does to T5 Transformer training scale to multiple GPUs?"
"BART(base) - Finetune Is this a bug ? Or I am doing something wrong?"
"Not Implemented Error "
"Multilingual MNLI model"
"Distilroberta Tokenizer and Encoder not aligning"
"RobertaTokenizerFast produces a different output than RobertaTokenizer"
"BertTokenizerFast does not support `pad_to_max_length` argument"
"[proposal] Move tests/utils.py to src/transformers/testing_utils.py so that examples can import"
"Cannot control wandb metadata when running examples/seq2seq/finetune.py"
"[examples] Verify marian and mbart BLEU scores with examples/seq2seq/run_eval.py"
"Set the number of times to evaluate per epoch when using Trainer"
"test_torch_fillmask failing on GPU"
"Finetune T5 on other Dataset, AssertionError: assert tokenized.input_ids.shape[1] == max_length"
"ValueError in T5 community colab notebook."
"BART finetune.py: model not learning anything"
"A question about the test accuracy of BERT-based-uncased model on the MNLI dataset"
"I have some problems with the \"bert-large-uncased\" model"
"Description of how to preprocess text corpus for roBERTa LM training"
"can't open file 'transformers-cli'"
"Segmentation fault when trying to load models"
"Bart: Instatiate lm_head once without wasting memory"
"Tokenizer batch_encode_plus unexpected behavior "
"Roberta's Positional Embedding Offset"
"save_pretrained on master results in tokenizers that cannot be loaded in v2.11"
"Is there a Longformer For Sequence Classification?"
"Model with fastest inference?"
"Saving and loading tokenizers with torch.save fails"
"run_squad.py :: ValueError: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
"Slow Integration Test for examples/seq2seq/finetune.py"
"Is summing of attention_mask intended?"
"Can we have a way for a tokenizer to transform word level or character level annotations?"
"The start and end position of BertForQuestionAnswering"
"No documentation for MMBT on official docs"
"T5ForConditionalGeneration fp16 nan loss"
"Request for inclusion of PEGASUS for text summarization by Google."
"Attempted relative import with no known parent package"
"Bert Abs not using GPU"
"Does BERT public an embedding file like glove.840B.300d.txt? "
"Links out of date under transformers/examples/README.md"
"AllenNLP SPECTER model"
"Transformer-XL not working with DistributedDataParallel"
"Reformer model axial.position.shape config not working"
"[{m}bart] Fix final_logits bias warning"
"Add option to keep tb_writer open after training is done."
"Better hyperparameter tensorboard logging in Trainer."
"Link to the example/summarization in doc is broken"
"XLNet with high CPU usage"
"Add \"labels\" functionality for all TF Causal LM and Masked LM models"
"BertForPreTraining and BertModel when loading TF checkpoints"
"BillSum dataset finetuning"
"Confuse by  \"All learning rates are 0\""
"Predefined tasks in T5"
"GPT2Tokenizer remove the ' ' (space) if it is at the end of text?"
"In Tensorflow the serving is very slow"
"Massive text generation slowdown when using repetition_penalty param on GPU"
"Upload model card with the CLI"
"Training with a large dataset"
"T5 Warning: embeddings are not initialized"
"T5 FP16: bad generations from my converted checkpoint"
"Do T5 have the next-sentence-predict loss?"
"Segmentation fault (core dumped) after importing transformers"
"Layer #0 (named \"roberta\") expects 0 weight(s), but the saved weights have 199 element(s)"
"Albert pooling dimension mismatches"
"Unable to load Longformer pretrained weights"
"New tokenizer code in transformer 3.0.0 is creating error with old code"
"Cannot reduce n_ctx for distil gpt2 from 1024 to 256"
"Unable to load pre-trained model/tokenizer when using Kubernetes"
"AssertionError: Padding_idx must be within num_embeddings"
" cannot import name 'AutoModelForSeq2SeqLM' from transformers"
"Positional and Segment Embeddings in BERT"
"Example: PyTorch Lightning returns missing attribute error (Token Classification)"
"bart-large-cnn training related information"
"BART fine tuning on gpu issue"
"Why T5 do not generate the whole next sentence as one of the pretrain loss?"
"model.generate source code"
"Training a GPT-2 from scratch in Greek-text, results in a low perplexity score of 7 after 15 epochs. Is it normal that score?"
"Windows: No matching distribution found for lightning_base"
"GPT2Tokenizer.save_pretrained does not work in v3.0.0"
"tokenizer started throwing this warning, \"\"Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\"\""
"Inference time difference between pipeline and with standalone model and tokenizer"
"Runtime for BERT and Roberta"
"Help with Debugging TF Common tests"
"How to interpret/act on this warning:  \"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM"
"How to interpret/act on this warning:  \"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM\"?"
"Colab session crash with XLA & Tranformers"
"High Quality EN-DE/EN-FR Translators"
"What to do about this warning message: \"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification\""
"Error Instantiating T5-11B from conributed models"
"Bart EncoderLayer masked_fill not working properly with pytorch 1.4"
"[Quick poll] Give your opinion on the future of \ud83e\udd17 transformers"
"WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss."
"How to use (and preferably finetune) BART for text infilling? "
"Can't load to predict a reproduced DistilBERT "
"MiniLM transformers inconsistent log posteriors in multiple runs"
"I want to load pre-trained model from file instead of file name"
"Squad2 processor error"
"\"Write With Transformer\" not generating text (502 Bad Gateway)"
"Benchmarking on TPU shows clearly wrong results"
"(TF) model.generate to tf.function for tf serving"
"Inconsistent tokenizer handling of max_len"
"\"Write With Transformer\" inserts a space whenever accepting a suggestion, even if a space doesn't belong there"
"Reformer language modeling using run_language_modeling.py: sentences didn't pad to max_length"
"Where did \"prepare_for_model\" go?  What is the replacement?"
"TF: inputs vs input_ids"
"Text Classification with PyTorch Lightning: 'dict' object has no attribute 'task'"
"The output to be used for getting sentence embeddings from BERT"
"Error while saving Longformer pre-trained model"
"How to batch encode sentences using BertTokenizer?"
"\ud83d\udc1b Can't use `AutoTokenizer` with `sshleifer/mbart-large-cc25`"
"Error while saving model: TypeError: ('Not JSON Serializable:', DistilBertConfig"
"BERT Huggingface trainer api: ValueError: expected sequence of length 128 at dim 1 (got 314)"
"[Reformer] combine reformer model with other tokenizers"
"Pre-Trained Model (ipuneetrathore/bert-base-cased-finetuned-finBERT) loads in PyTorch but not Tensorflow"
"Unable to use run_squad with xla_spawn.py on TPU"
"TFAutoModelForSequenceClassification: ValueError: Layer #1 (named \"classifier\") expects 2 weight(s), but the saved weights have 4 element(s)."
"Can't use AutoModelForCausalLM with bert"
"35 Model Hub entries fail AutoConfig"
"Seq2Seq: Option to not store whole dataset in memory"
"Possible breaking undetected change to \"data/processors/squad.py\""
"'Size' Error while loading t5-large model"
"Can't pickle tokenizers ..."
" can't get models directory after running python run_squad.py "
"Error using t5-base-cnn"
"Bert-extractive-summaizer importing issue"
"Tokenizers throwing warning \"The current process just got forked, Disabling parallelism to avoid deadlocks.. To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\""
"Better TPU Support in examples"
"Cannot train RoBERTa from scratch with multiple nodes and multiple GPUs"
"encoder_outputs are always the same when generating with different inputs"
"[ERROR] Tokenizer and TokenizerFast ??? "
"The inference speed of gpt2-xl has a gap between pytorch and tensorflow."
"What happened to https://huggingface.co/zero-shot/ ?"
"[ERROR] add_special_tokens = True not working in version 3.0.0"
"batch_encode_plus model output is different from tokenizer.encode model's output"
"licens"
"T5 Training on TPU doesnt use TPU"
"Write With Transformers"
"3.0.1 BertTokenizer batch_encode_plus() shows warnings \"Truncation was not explicitely activated but `max_length` is provided a specific value\""
"Why is `encoder_extended_attention_mask = None` when `config.is_decoder == False`"
"What's the correct way to use add_prefix_space for the fast RoBERTa tokenizer in 3.0.0/3.0.1?"
"T5 Masking:"
"TPU Trainer memory leak and memory requirements"
"Fail in some tests (with detailed description)"
"getting different model result from tokenizer vs tokenizer.encode function"
"Get prediction_scores from BART forward method"
"AdamW step device error"
"What should i do if I want a model class similar to BertForSequenceClassification?"
"How to fine-tune tinyBERT for question-asnwering"
"WARNING:transformers.tokenization_utils:Keyword arguments {'add_space_before_punct_symbol': True} not recognized."
"Tf to pytorch"
"3.0.1: \"unexpected keyword argument 'is_pretokenized'\" when using batch_encode_plus() w/ Fast Tokenizers"
"Tabert"
"Error in Loading bert-large-uncased-whole-word-masking-finetuned-squad"
"How-to-fine-tune-bert-for-question-answering?"
"How i can set the special token <|endoftext|> to an other id ?"
"Longformer - Compression"
"Easier way to download pretrained model files to local"
"Fine Tuning Using /question-answering/run_squad.py"
"MobileBert embedding vectors values"
"High F1 score. But poor accuracy during Inference due to tokenisation"
"t5-base translation_en_to_de BLEU lower than the paper"
"incorrect typehint for PreTrainedTokenizer.convert_ids_to_tokens() return value"
"Batching (TF)BertForQuestionAnswering deployment"
"[Feature Request] Extract Predictions from Trainer"
"Possibility to use WhitespaceSplit as pre_tokenizer instead of BPE/Sentencepiece?"
"Customize widget text-generation inference with prepended input"
"huggingface optimizer cannot de-serialize"
"Training TFBertForSequenceClassification with DataFrame instead of tensorflow_datasets"
"Roberta Large doesn't train for sentiment classification"
"[Docs] Incorrect links to models in the Summary of the Models page"
"Bug in Question Answering pipeline when question is weird (unanswerable)"
"Where is the documentation on migrating to the 3.0 tokenizer API?"
"\u2753 Why multiplying the output of T5 by some scalar before LM head ?"
"How to get gradient wrt to a word embedding layer pytorch?"
"Use pretrained bert withou embedding layers."
"BertEmbeddings code for position_embeddings and word_embeddings"
"Freeze the token embeddings for finetuning"
"Tokenizers save_pretrained doesn't work with custom vocabs (v3.0.2)"
"MBARTTokenizer set_lang logic will only work for src_lang=en_XX"
"Seperating premise and hypothesis in MNLI"
"OSError: Model name 'facebook/bart-large-cnn' was not found in tokenizers model name list"
"TypeError: 'BertTokenizer' object is not callable"
"On running finetune.py for seq2seq, the following error comes up: optimizer_step() got an unexpected keyword argument 'using_native_amp'"
"GPT2 past usage"
"Difference between AutoTokenizer.from_pretrained and BertTokenizer.from_pretrained"
"[Some weights or buffers of the PyTorch model TFGPT2LMHeadModel were not initialized] convert GPT2 pytorch to tensorflow model "
"Datasets & collators for NER"
"HF Trainer Segmentation Fault"
"KeyError Issue in Question answering"
"AttributeError: 'Tensor' object has no attribute 'ndim'"
"transformer dataset and masked LM"
"DPR model examples / notebook / pipeline"
"[MarianMT{"
"MarianMT: \"CUDA out of memory\" when translating many times with the MarianMT Model"
"[Benchmark] TFGPT2LMHeadModel is five times slower than GPT2LMHeadModel"
"Here maybe a bug, when we load staged checkpoint"
"OSError using FlauBERT"
"Is there an implementation of BERT architecture in PyTorch that I can modify here?"
"Duplicate grouped entities when using 'ner' pipeline"
"IndexError: index out of range in self"
"Did the run_language_model support TPU? "
"\ud83d\udc1b Bart Tokenization difference between 2.11.0 and 3.0.2"
"Should t5-small generate coherent text as summaries without finetuning?"
"TextGenerationPipeline breaks when used with device=0"
"Predictor in Streamlit Docker eating all memory and OOM"
"Inference widgets for self-hosted models?"
"Cannot reproduce roberta-large on SQuAD"
"Model doc failed"
"Support for Polyencoder and other retriever based models"
"How can I fine-tune on custom model"
"T5 has no module  ```torch_xla``` when using T5 fine-tuned on SQUADv2"
"test suite fails due to pytorch bug in torch.seed"
"Multiple Mask Tokens"
"Help with Using TFXLNet on custom embeddings"
"Can't get (global) attention probs using Longformer"
"T5 TorchScript (Trace) Conversion "
"Classification accuracy on validation set didn't improve while fine-tuning BERT"
"Bugs due to design choices in LongformerTokenizer"
"Wrong answers from Longformer model even on simple questions"
"T5 fp16 overflow in forward (T5DenseReluDense)"
"AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")  "
"\u2753 Difficulties to reproduce BART results on CNN/DM by fine-tuning bart-large"
"Truncated Outputs by t5 fine-tuned models"
"\"How to train a new language model from scratch\" colab stuck at training"
"Request for Support to Adapt a Model (Human Dignity Observatory: Non-Profit Project) "
"[PyTorch] Load and run a model CPU which was traced and saved on GPU"
"How do you connect Convolutional layers to Transformers?"
"pytorch_model.bin file is different after uploading to HuggingFace Models"
"SquadDataset should use version number in cache file name"
"Can't get BART to generate EOS token."
"Deepset model not loading using default code"
"Add indexes to grouped entity NER pipeline"
"Weird output when using unexpected model type for pipelines"
"How to produce customized attention mask for BertModel?"
"What is the decoder_input for encoder-decoder transformer in training time?"
"Is Writing With Transform open source?"
"How I can  predict missing letters in a sentence, like \" I want to b _ _  the car because it is cheap.\""
"Cannot import EvalPrediction from transformers"
"__init__() missing 1 required positional argument: 'logits'"
"How can I evaluate on GLUE but without fine-tune BERT. Just train the rest layers?"
"Add beta 1 and beta 2 option in `TrainingArguments` for `AdamW` optimizer."
"How to visualize the output of the encoder using T-sne plots? "
"How to generate sentences from Transformer's sentence embeddings?"
"help\uff1aOSError: Model name 'ctrl' was not found in tokenizers model name list (ctrl). We assumed 'ctrl' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
"Any insight to this mystery issue? Using the Keras functional API results in whole deleted weights/layers for transformer layers. "
"can't resume training from a saved checkpoint in run_glue"
"Span Mask Fill"
"For Roberta pretraining, how to enable large batch training using gradient accumulation?"
"Run Language Modeling on Colab TPU cores terminates"
"Attention heads attend equally after conversion from tensorflow checkpoint"
"QA Pipeline: Key Error due to predicting a token outside of allowed context"
"How to download Pre-trained T5 model?"
"ONNX export broken for QA models"
"facebook/bart-large-mnli input format"
"Extending vocabulary by a large size crashes RobertaTokenizerFast"
"generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was None."
"TypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of <function tf_if_stmt.<locals>.error_checking_body at 0x7f55400e3c80>, found return value of type <class 'tensorflow.python.keras.losses.MeanSquaredError'>, which is not a Tensor."
"Unable to finetune BERT on own dataset"
"Cannot preprocess WNUT'17 dataset for token-classification"
"Fix slow test_enro_generate"
"T5 ONNX Export Test Failing on GPU"
"Finetuning GPT2 with Custom Loss"
"t5 model card"
"[Feature request] Pass any Iterable to tokenizer.__call__()"
"Using pipeline('ner'), partial tokens returned when grouped_entities=True"
"TypeError: an integer is required (got type NoneType) while using  run_language_modeling.py"
"Unicode normalization for bert-cased models"
"TypeError: join() argument must be str or bytes, not 'NoneType'"
"FileNotFoundError: File not found when running run_squad.py to fine-tune the BERT on  SQuAD v1.1."
"How to use pytorch_model.bin to classify a single sentence?"
"google/reformer-enwik8 tokenizer was not found in tokenizers model name list"
"Where can I find raw code for char_to_token function."
" Unrecognized configuration class <class 'transformers.configuration_electra.ElectraConfig'>"
"Long BERT TypeError: forward() takes from 2 to 4 positional arguments but 7 were given"
"fail to run trainer.train() with huggingface transformer"
"Can't load `facebook/mbart-large-cc25` tokenizer"
"T5 fine-tuned model doesn't appear in the model hub"
"Problems with generating text using mbart-large-cc25"
"BART MNLI + yahoo answer in the model hub for inference API"
"BART/T5 eli5 in model hub"
"Feature request of Sparselty Gated Mixture-of-Experts and PowerNorm"
"When using \"transformers.WarmUp\" with tensorflow 2.0.0, warming up restart in each epoch!"
"Hello,I have this problem in running 'run_glue.py'!"
"How to get parameters from a Query."
"Confidence score prediction of pretrained models in extractive QA - similar to pipeline"
"Fine-tune BERT for regression problem"
"Bug in MiniLM-L12-H384-uncased modelhub model files"
"Error using DataParallel with reformer model: There were no tensor arguments to this function"
"[bart] decoder.last_hidden_state shape changes when passing labels"
"Error in conversion to tensorflow"
"Marian Conversion Script"
"Sentence-transformers model outputs different than when loaded in HuggingFace"
"Faster mBART finetuning"
"Can't load weights for GPT2 error"
"add attention_dropout, relu_dropout command line args to lightning_base.py"
"Print all next tokens of a sentence over a certain probability threshold."
"LongFormerAttention For AutoRegressive Models"
"Can I use the pretrained BERT-Base model directly for predict isNextSentence task?"
"Issue when load pretrained weights"
"GPT2 weights don't initialize from checkpoint"
"Hosted Inference API: Error loading tokenizer Can't load config"
"BERT-viewer is broken for russian?"
"While running finetune.py in seq2seq examples on a custom dataset, I am getting the following error. "
"TypeError: forward() got an unexpected keyword argument 'head_mask'"
"How to download original weights of gpt2"
"cast_bool_to_primitive breaks TensorFlow graph support."
"Additional layers to BERT "
"error with transformers 2.9.1 but not with 2.3.0, same code, why?"
"Migrate Marian models names to ISO-639-3 where possible"
"Vocab size mismatch on EncoderDecoder model from_pretrained"
"Reproducibility when using pretrained GPT2"
"ValueError: DataLoader with IterableDataset: expected unspecified sampler option,"
"OpenAI GPT NoPaddingTokenFastTokenizerMatchingTest test fails"
"ImportError: cannot import name 'BERT_PRETRAINED_MODEL_ARCHIVE_MAP' from 'transformers'"
"problem about Custom class inheriting from <TFBertPreTrainedModel>"
"Decode [UNK] from tokenizer"
"AttributeError: type object 'BertConfig' has no attribute 'pretrained_config_archive_map'"
"pip install error"
"Covid-19 - TPU V3-1024 - T5 11B:  Tensorflow to Pytorch conversion failed"
"Exception in device=TPU:1: 'ascii' codec can't decode byte 0xc2 in position 37: ordinal not in range(128)"
"[BartModel] Question for BartModel Output Shape when I pass the 'decoder_input_ids'"
"[seq2seq] organize commands into scripts/ subdir"
"issue with loading pretrained model - xlnet"
"Potential security vulnerability regarding Hosted Interface API?"
"not able to reproduce accuracy at the end of same epoch"
"Silenced error while downloading pretrained model"
"How to get probabilities from MarianMT models?"
"DataCollatorForLanguageModeling - Shift labels for left-to-right LM?"
"Seq2Seq: same MultiGPU test failing twice!"
"Reading transformer package from local codes and NOT the pip installed version"
"AttributeError: 'GPT2LMHeadModel' object has no attribute 'h'"
"How to finetune distillbart from distilbart-cnn-12-6 checkpoint using cnn_daily mail or gigawords dataset?"
"Benchmark: traceback does not describe real problem"
"examples/seq2seq/finetune.py and BART supports TPU"
"MT: automate/experiment with pruning embeddings"
"examples/seq2seq: add label_smoothing cross entropy option"
"Fix pack_dataset.py"
"Smaller output vocabulary for GPT-2"
"Is there any api for intermediate layer outputs?"
"How can I check the loss during pretraing huggingface/transformers"
"\ud83d\udc1b BART : Same representations for different `<s>` tokens "
"RobertaTokenizerFast unexpectedly quits when creating a TextDataset"
"Retrain/reuse fine-tuned models on a different set of labels"
"Word frequencies in TransfoXLTokenizer"
"ModuleNotFoundError: No module named 'torch_xla'"
"ImportError: cannot import name 'DataCollatorForPermutationLanguageModeling'"
"QA Pipeline: Key Error due to predicting a token in question"
"Incompatible tensor type when running BART on TPU"
"convert_roberta: AttributeError when converting CamemBERT model.pt to pytorch_model.bin"
"Add Fast Transformers - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
"how (if at all) are those models related..."
"Feed forward chunking for all pretrained models"
"code copy button on the website doesn't copy `...` lines"
"ALBERT tokenizer is not callable"
"How to get a language model score in BertModel?"
"InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array."
"Getting \"AttributeError: 'Tensor' object has no attribute 'numpy'\" while fine-tuning BERT for NER"
"Easy selection of a learning rate scheduler when initializing a Trainer"
"How does AdamW weight_decay works for L2 regularization?"
"Can't use BatchEncoding in the fit function"
"What is the difference between the function of add_tokens() and add_special_tokens() in tokenizer"
"Converting GPT2 logits to token ids directly"
"process stuck at LineByLineTextDataset. training not starting"
"Test on a new string of gpt2 fine tuned"
"Exporting T5 to ONNX"
"seq2seq: checkpoint callback seems messed up"
"module 'tensorflow_core._api.v2.config' has no attribute 'list_physical_devices'"
"NoneType error when using Trainer"
"Can't load weights of models"
"tensorflow\u8f6c\u4e3apytorch\u7684\u4e24\u4e2a\u6587\u4ef6\u5728\u54ea\u91cc\uff1f"
"Bert forward reports error on GPU; but runs fine on CPU"
"text classification reuse without classifier"
"BerTweet tokenizer issue"
"Loss becoming nearly zero in first 5K steps when training LM from scratch"
"run_squad example doesn't work with XLM model"
"ImportError: cannot import name 'MODEL_WITH_LM_HEAD_MAPPING'"
"[cleanup] much cruft in unittests"
"Transformer-XL: no mems are return when using 'return_tuple'"
"Training data format"
"dynamic masking for RoBERTa model"
"pipeline does not do truncation on long texts input, error message found"
"Albert pre-train from scratch convergence problem"
"how can I download T5-11B pretrained model?"
"Possible bug in preparing deocder_input_ids for T5 in seq2seq finetune.y"
"EncoderDecoderModel:  weight can not be init from the checkpoint"
"Trainer: exception raised when calling len() on IterableDataset"
"T5 Tensorflow: _shift_right returns wrong result"
"Store Predictions on CPU in Every Prediction Iteration (Trainer)"
"T5 pre training on different languages from scratch"
"bug in trainer.py line297"
"MbartDataset can support Summarization"
"Deebert Examples test failure"
"CI/Examples: ModuleNotFoundError: No module named '_sqlite3'"
"Fine tune T5 for paraphrase generation"
"install sentence-transformers on Linux by python error"
"[Benchmark]"
"When i train a GPT-2 which uses BPE, the computed perplexity is per sub-word right?"
"Cannot import DPRReader"
"convert_bart script should support mbart through command line."
"seq2seq/finetune.py can take config train_params through command line"
"Failed to save tokenizer with AddedToken in additional_special_tokens"
"tensorflow bert model can\u2018t  return all hidden_states "
"Error with batch_encode_plus"
"Is there an easy way to access the multiple choice head of the RobertaForMultipleChoice?"
"MarianMT - How to find out the actual names of the languages? - Only language-codes are available"
"slow from_pretrained failures"
"Test BART's memory  consumption"
"is_pretokenized seems to work incorrectly "
"Feed to forward new parameters as computed manually by update rule"
"examples/seq2seq/test_bash_script.py covers summarization"
"examples/seq2seq/test_bash_script.py :: actually learn something"
"CI: run tests against torch=1.6"
"examples/seq2seq:  add a dataloader that supports dynamic batch size"
"add gpt2 padding for tflite"
"Errors when creating a subclass of \"BertForTokenClassification\" "
"Errors while creating a subclass of BertForTokenClassification in run_ner.py file"
" Transformer layers + Functional API is failed but subclass is successful"
"Errors while using TFAutoModelForMultipleChoice and TFTrainer on winogrande dataset "
"new AutoModel classes in pipeline"
"lightning_base: new clarg: adam_betas"
"lightning_base: new clarg: lr_scheduler=polynomial_decay"
"Loading and running on CPU, the RoBERTa model traced/saved on GPU."
"Error in the RobertaTokenizer?"
"Cannot use the RobertaForMultipleChoice model for processing multiple choice questions with 4 options"
"model.roberta.from_pretrained() fails to change the parameters"
"Proposal: seq2seq tokenizers expose a prepare_seq2seq_batch method"
"\ud83d\udc1b Inconsistencies between BartTokenizer and BartTokenizerFast"
"Error when using np.where() during squad tokenization "
"ValueError raises when load Flaubert from pre-train with Transformers >=3.0.0"
"tf.saved_model.save is not worked on TFElectra* series."
"Finetuning German BERT for QA on biomedical domain "
"customize special tokens"
"i dont know what Tranier`s Dataset is."
"5 Slow test failures"
"mBART: incorrect <mask> token id"
"Use HFArgParser instead of Fire"
"Recursive error calling generate in forward"
"Weird Behavior on XLNetTokenizer after new tokens added"
"Where do the Masked Language Model perform mask on the input data"
"allenai/longformer-large-4096 unavailable"
"StopIteration error in RobertaForMultipleChoice"
"Is there any way that I can use the HuggingFace Transformers as Pyro models?"
"\ud83c\udf1f BigBird"
"namespace object has no attribute to \"enc_only\""
"Usage of Pytorch Native AMP in place of apex (Pytorch 1.6) in Trainer"
"No button for creating new post at the forum."
"Using control codes for finetuning"
"Is `guid` allowed to be None in `InputExample`?"
"\ud83d\udc1b Empty TypeError on BartTokenizerFast.decode(tensor)"
"Don't see how to use correct padding with QA pipeline"
"convert_pytorch_checkpoint_to_tf2.py AttributeError: cls.seq_relationship.weight not found in PyTorch model"
"problem about geting hidden_states using TFBertModel"
"Add decoding inputs to generate"
"Initializing XLMRobertaTokenizer using pretrained tokenizer expects serialized vocab"
"MBartTokenizerTrimmed to support truncated embeddings"
"How to combine the encoded representations of two transformers"
"frequent checkpoints have worse performance"
"StopIteration error when using HuggingFace Transformer models"
"Applying hugging face transformer in sequence labeling problem"
"Copyright date and owner not filled out in LICENSE file"
"Bug in language_modeling.py calling tokenizer.num_special_tokens_to_add"
"Question-Answering pipeline doesn't work anymore with long text"
"TOKENIZER: truncation not working for batch"
"\ud83c\udf1f Mirostat decoding algorithm"
"the documents for transformer don't work"
"tokenize cache for examples/language-modeling"
"\ud83d\udc1b T5 Tokenizer ignores \\n \\t characters and more than one whitespace together"
"Using BertWordPiece Tokenizer"
"Hidden State Embedding-Transformers"
"should mBART-large-en-ro have decoder_start_token_id by default?"
"OSError: Unable to load weights from pytorch checkpoint file. "
"run_squad.py eval metrics meaning"
"Padding Strategy Code missing an else case (maybe?)"
"RoBERTa ``tokenizer.decode`` does not produce the same sentence."
"[Benchmark]"
"\ud83d\udc1b Not adding `token_type_ids` when the model is `electra` (pytorch_lightning example)"
"My finetuned gpt2 model is taking wayy too long to generate samples, like 5-8 minutes"
"t"
"RoBERTa for QuestionAnswering "
"Why are the `device()` and `dtype()` functions in `modelling_utils.py` needed?"
"HANS Dataset: Incorrect `label_list` and `label`."
"Failing ONNX Export test"
"Failing XLMModelTest"
"Remove inconsistency between BertTokenizer and BertTokenizerFast "
"taeminlee/kogpt2 not working"
"Add support for truncation argument when calling a Pipeline"
"How to integrate the Pyro module with HuggingFace Transformers?"
"GPT2 crashing at loss.backward()"
"Some weights not initialized in pre-trained RobertaForMaskedLM"
"longformertokenizerFast gives error"
"Cannot fine tune my distilbart-cnn-12-6 model because of cuda memory"
"Issue with fp16_opt_level default"
"QA Loss Cleanup"
"Error while saving electra model in tensorflow \"savedModel\" format"
"torch\u6a21\u578b\u7684forward\u65b9\u6cd5\u91cc\u9762\u5185\u5bb9\u4e3a\u7a7a\uff0c\u7528Electra\u6a21\u578b\u65e0\u6cd5\u8bad\u7ec3\u3002"
"Tips for Tensorflow 2.0 NER task by using fit method."
"XLM-R has extremely low accuracy after fine-tuning on MNLI"
"Error when fine tuning GPT2 on GPU "
"Add `do_lower_case` handling to GPT2TokenizerFast and descendant tokenizers"
"Comparison different methods for benchmarking"
"memory benchmarking: should the cudnn kernels loading be included"
"benchmarking API: `no_` arguments, double negation, defaults"
"Can't load config for [community model]"
"ValueError: Unrecognized model identifier in facebook/bart-large-cnn."
"mismatch keys of glue tasks"
"losses does not decrease when trainning  with TFTransfoXLLMHeadModel  . "
"Discrepancy in the pad_token_id between the tokenizer and the model code of the T5"
"Documentation bug in GPT2Config"
"Implement DeLighT: Very Deep and Light-weight Transformers"
"LongformerForSequenceClassification has unused layers, making it unable to fine-tune with Data Distributed Parallel (required for gradient checkpointing)"
"Gradient Checkpointing with Transformers BERT model"
"Bart encoder with add_final_layer_norm"
"Incorrect tokenization for MarianNMT models in example script."
"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
"TF LMHead very slow: TFGPT2LMHeadModel is 7 times slower than Torch GPT2LMHeadModel "
"Bert Mesh Tensorflow"
"Unable to make inference from hosted api for a pretrained model that I uploaded. "
"Adding example with Finnish BERT fine-tuning for NER task"
"Deleting position IDS when fine-tuning BERT"
"How to access the parameters of the uppermost layer of the HuggingFace Transformers via \".modules()\"?"
"Reformer now requires PyTorch 1.6.0"
"Returning the attention heads using Longformer"
"TFRobertaMarkedLM model output issue"
"Using tensorrt model.engine Inference speed is relatively fast. Why is onnxruntime based on tensorrt as slow as CPU inference"
"Problem with converting XLM checkpoint to pytorch (missing merges.txt)"
"\ud83c\udf1f T5 V1.1"
"F1 decreases resuming from last saved checkpoint of fine-tuning"
"Why is the lm_head layer in GPT2LMHeadModel not a parameter?"
"inconclusive truncation strategies in encode_plus?"
"How to get word and sentence level embeddings from T5-11b"
"Fix/test convert_mbart.py"
"Question about BERT model size (transformer block number) "
"Redundant code"
"Default value of `n_tpu_cores` in lightning_base.py"
"solving `make quality` failures"
"Debug flag to `run_language_modeling` triggers error"
"collision between different cl arg definitions in examples"
"Error trying to import SquadDataset"
"Dataloader number of workers in Trainer"
"codecov invalid reports due to inconsistent code coverage outputs (non-idempotent test-suite)"
"TFBert runs slower than keras-bert\uff0c any plan to speed up\uff1f"
"num_beams error in GPT2DoubleHead model"
"Multi-gpu LM finetuning"
"Hi , I am having trouble locating the transformers/examples/summarization/bart/ file. I was wondering if it has been renamed or changed?"
"Text-to-SQL Query"
"Batched pipeline"
"OSError: Model name 'lonePatient/albert_chinese_small' was not found in tokenizers model"
"BertForPreTraining with NSP"
"Delete this line in label_smoothed_nll_loss"
"add tests/test_tokenization_reformer.py"
"delete unused tiny models"
"broken ONNX slow test"
"[CI] add manual workflow dispatch option to github actions runners"
"The default cache directory is lack of disk capacity, I need change the configure of the default cache directory."
"Is it necessary to provide attention_mask, or model will calculate itself?"
"ModuleNotFoundError: No module named 'transformers' on Google Colab"
"[Bart] Cannot use Bart decoder cache with torchscript"
"[testing] USE_CUDA default and intuitive skip decorators"
"Why is distillbart-cnn done with no teacher and distilbart-xsum has a teacher?"
"BartModel decodes sequence of incorrect length when decoder_input_ids is specified / Output shape mismatch due to when `use_cache` True/False"
"GPU memory consumption increases while training"
"Bug in squad example with XLNet"
"[TFTrainer] Error \"iterating over `tf.Tensor` is not allowed\""
"[s2s] pass max_length to config through command line"
"Can't load a saved tokenizer with AutoTokenizer.from_pretrained without saving Config as well"
"trainer/lightning_base: Arbitrary config updates through command line"
"FastTokenizer not returning batch_size for offset_mapping for short texts"
"Pegasus finetuning diary"
"[s2s] remove lr_scheduler redundancy"
"CUDA Out of Memory"
"hi"
"AttributeError: type object \"BartTokenizer\" has no attribute 'name'"
"seq2seq examples require pytest"
"Error while loading albert for token classification"
"Bug in the question answering pipeline"
"DPR retriever module"
"ZeroDivisionError with Reformer"
"[TF Longformer] Add Multiple Choice, Seq Classification Model"
"RuntimeError: Error while creating shape using tf-xlm-roberta-large"
"Slow Decoding Speed when using BertForLMModel"
"i have used t5_base for abstractive summarization but it is not giving good results,Could you please give me solution for this"
"TF2 TPU slow?"
"Cannot unzip the XNLI-MT 1.0 zip file."
"TypeError: forward() got an unexpected keyword argument 'labels'"
"Docs: Separate documentation for mbart"
"how to fine tune t5 model for summarization task using tensorflow2?"
"All learning rates are 0 warning"
"Add pegasus model cards"
"Experiment: ROUGE impact of using pegasus length-penalty implementation"
"test_run_glue_with_pabee failing"
"actions CI self-scheduled: run_examples torch even if run_torch_tests fails"
"Error in run_tf_squad.py script"
"TF2 implementation of LineByLineTextDataset?"
"Epoch iterator for run_pl_ner.py"
"Training GPT2 and Reformer from scratch.  "
"TrainingArguments are ignored?!"
"Getting Error from Default Data Collator while training Bert on SQUAD 2.0"
"Simple train from the start for translation transformer"
"Can't download  'Helsinki-NLP/opus-mt-hye-eng' model"
"Error in PyTorch Trainer when used with TPU"
"ERROR: No matching distribution found for tokenizers==0.8.1.rc1 (from transformers)"
"getting error while training bert language model. \"ValueError: Expected input batch_size (8) to match target batch_size (1024).\""
"Memory Issue while following LM tutorial"
"MASS : A generalization of BERT and GPT"
"Open-Retrieval Question Answering (ORQA)"
"Unknown task zero-shot-classification"
"Autotokenizer not returning instance of LongformerTokenizerFast"
"Hashing a tokenizer using the \ud83e\udd17 nlp lib is not deterministic"
"Longformer convert error"
"Error: 'GPT2Model' object has no attribute '_step' when converting tf-based checkpoint into pytorch"
"convert_graph_to_onnx not working as expected."
"[testing] automatically clean up temp dirs during teardown"
"\"BertEncoder' object has no attribute 'output_hidden_states\""
"Training Data of  xlm-roberta-large-finetuned-conll03-* models"
"Question about loss computing in BartForConditionalGeneration"
"finetune.py: error: unrecognized arguments"
"Upladed model is not indexed"
"[TFTrainer] gradient accumulation error "
"what's the difference between TFBertOutput and TFBertSelfOutput?"
"Longformer Memory Consumption query"
"Regarding GPU use for LM"
"Assertion error when training a new RoBERTa from scratch"
"from_pretrained() never works"
"about encoder and decoder input when using seq2seq model"
"Whole Word Masking Implementation"
"Model Upload does not show up `german-nlp-group/electra-base-german-uncased`"
"BERT and SpanBERT for Coreference Resolution"
"Could not output hidden states using TFBertModel"
"Always got RuntimeError while converting ALBERT model to TorchScript (.pt file)"
"Longformer slow than Bert"
"Truncated last sentence after bart finetuning on custom dataset."
"tf BERT model produced by convert_graph_to_onnx has unclear or wrong input shapes"
"Loading 'fmikaelian/flaubert-base-uncased-squad' throws unexpected, difficult to comprehend warning"
"trainer.train() fails on 'fmikaelian/flaubert-base-uncased-squad' fine-tuning SQuAD"
"Longformer pretrained weights are not really pretrained?"
"Unexpected output(prediction) for TokenClassification, using pipeline"
"How to enable sampling when using unigram tokenizers?"
"Can't load t5-11b from pre-trained"
"Can't load pegasus models."
"How to fine-tune GPT2 on Arithmetic Problem"
"Passing inputs_embeds into GenerationMixin.generate()"
"tokenizers/tokenizers.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin "
"Widget can't load model"
"bart-base config.*attention_heads (should be 12 was 16)"
"WNUT17 TF example stuck at first epoch "
"QA pipeline fails when using fast tokenizer"
"Model Clipping reduce the size of the model."
"504 Gateway Time-out when trying to access Uploaded Model page "
"TFTrainer Example"
"Add model card for facebook/mbart-large-en-ro"
"The squad processor's multi-threading crashes the script / causes large models to reload every call"
"Can we resize embedding with embedding weighted initialized differently??"
"Finetuning GPT2 produces IndexError: index out of range in self error"
"Huggingface create_optimizer method not working"
"Create my own language model"
"[Urgent] Word embedding initialization documentation and code might mismatch"
"Pretokenized text handling mistake in  tokenization_utils.py file"
"T5 Gradient Checkpointing"
"New Feature: Best-First Beam Search"
"Can not convert the pytorch pretrained bert model to onnx model "
"KeyError with DPR reader in Question Answering pipeline"
"Can not use the convert_graph_to_onnx.py to convert the pytorch model to onnx model"
"Tokenizer further tokenizes pretokenized input"
"Latest source has bug in pipelines for short inputs (regardless of padding)"
"Error using pipeline with officiel docker image (transformers 3.0.2)"
"BatchEncoding interacts poorly with apex.amp"
"Failing ONNX Slow Test"
"Failing bart-base slow test"
"all_hidden_states indentation bug in modeling_bert.py"
"[seq2seq] finetune.sh  OOMs in fp16 w torch 1.6 on colab"
"Delete Unused TFModelTesterMixin attributes"
"Unable to save and load RoBERTa model using tensorflow"
" tf2 transformers cache dir"
"Pegasus: IndexError: index out of range in self"
"How to use Huggingface model for continuous values directly?"
"PegasusForConditionalGeneration - Error in loading state dictionary"
"TextGenerationPipeline giving FutureWarning about AutoModelWithLMHead"
"FillMaskPipeline return special tokens i.e. <mask> as prediction"
"I can't reproduce the results of tf-xlm-r-ner-40-lang model "
"Fine tune masked language model on custom dataset 'index out of range in self'"
"unk handling in v3.0 different than v2.0?"
"[DistilBert] Flaky tests"
"Pegasus: OSError: Unable to load weights from pytorch checkpoint file."
"Bart: make decoder_input_ids correctly if labels specified."
"**Specifically for Pegasus-arxiv** -  PegasusForConditionalGeneration - Error in loading state dictionary"
"BartTokenizerFast cannot decode PyTorch tensors"
"PreTrainedModel's tie_weights invocation needs to be configurable"
"Tokenize got an unexpected keyword argument 'pad_to_max_length', 'return_attention_mask'"
"fine tuning with Chinese data LCQMC val_acc not increase"
"Error on `PreTrainedTokenizerBase.batch_encode_plus` with `return_overflowing_tokens=True, truncation=True`"
"How to convert tokenizer output to train_dataset which is required by Trainer API "
"Pre-training a language model on a large dataset"
"Running squad_convert_examples_to_features causes warnings."
"Run_glue.py, how can I continue previous fine-tuning training?"
"bert finetuning for multilingual question answering"
"New config param for cross-attention dimensionality"
"Error when loading my trained model"
"mbart broken in summarization pipeline"
"[doc] bart doc examples aren't for bart"
"['encoder.version', 'decoder.version'] are unexpected when loading a pretrained BART model"
"old nlp causes error that pip install -e. can't fix."
"Error when wandb is installed"
"Error while loading pretrained model with \"return_dict=True\""
"Sequence packing"
"Integer division of tensors using div or / is no longer supported torch"
"Finetune.sh showing killed"
"Why does the median cross entropy loss change when I change the random seed?"
"Zero-Shot-Classification: multi_class or multi_label?"
"Inconsistent handling of empty string in tokenizers"
"Pretrained GPT2DoubleHeadsModel"
"Value Error & dev file parameter: run_squad.py BERT QA finetuning"
"TFTrainer with TPUs: Here's a suggestion on getting it to work"
"New training arg: warmup_ratio"
"ner example failed on examples/token-classification % bash run.sh"
"Can't load config for New Community Model"
"Tokenizers works different between NFD/NFKD and NFC/NFKC normalize functions in lowercase Turkish(and probably some other languages)"
"BUILD upgrade to isort v5"
"missing reference `from model_bertabs import BertAbsSummarizer`"
"Question Answering demonstrator for contribute model stopped working"
"Longformer finetuning on TPUs IndexError: tuple index out of range"
"words of overflowing_tokens in function truncate_sequences is not in right order"
"Some weights of AlbertModel were not initialized ['albert.embeddings.position_ids'] "
"NER GermEval preprocessor not working as documented"
"Questions on the date of Wikipedia dumps for pretrained checkpoints (BERT and RoBERTa)."
" PegasusXSUMIntegrationTest.test_pegasus_xsum_summary"
"consolidate tf activation functions"
"Pegasus finetuning: OOM"
"longformer padding logging"
"RuntimeError: forward() Expected a value of type \u2018Tensor\u2019 for argument \u2018input_ids\u2019 but instead found type \u2018list\u2019 while loading a torchscript model following the documentation "
"Resuming training from a checkpoint on Windows does not resume at the correct global_step"
"Dataset Lazyloader for transformers trainer"
"Documentation detail in (TF)RobertaForSequenceClassification "
"id2lang in tokenization_xlm.py should be int, and removing hardcoding"
"How to use the reformer for question answering?"
"How to generate sentences in batches, instead of generating sentences one by one"
"BART for Pre-Training"
"[s2s] run_eval saves samples/second"
"RuntimeError: grad can be implicitly created only for scalar outputs"
"RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated"
"Add Language Agnostic Bert Sentence Embedding"
"Unable to install Transformers Master version"
"Removing memory/deleting a model: how to properly do this"
"Question: Differentiable Search in generate"
"Checklist for Model Hub"
"Cuda out of memory when fp16 training"
"Distributed training doesn\u2019t work"
"Roberta Large not working with SNLI dataset always gives random baseline that is 33 percent accuracy."
"Exported TF Bert model is much slower than that exported from Google's Bert"
"Unable to install Transformers Master version"
"Error in run_ner.py"
"Task specific params for pegasus-large to allow finetuning with correct generation_parameters"
"Bertology-like Analysis for BART, T5?"
"Character-level tokenization?"
"How can I get rid of this message every time I run GPT2?"
"Typo in modling_tf_bert"
"Why does examples/seq2seq/finetune.py only use sortish sampler for train? "
"[help] Add multigpu evalulation script for seq2seq"
"T5 batch inference same input data gives different outputs?"
"Dear transformers team, how could I use bert to NER task?"
"How to fine-tune T5 with some additional special tokens ?"
"What is the size of the context window in the 'openai-gpt' pre-trained model?"
"F16 support for DistilBert"
"Unexpected behavior encoding token_type_ids in GPT models"
"Maybe global step should be initialized to 0"
"AttributeError: 'MarianTokenizer' object has no attribute 'prepare_translation_batch'"
"2 Slow Test Failures That Sam Can Fix"
"distilbart-cnn reproduction"
"bart-large-cnn ROUGE-L scores"
"Potential bug in PLM training"
"Create smaller number of heads in attn without pruning using shared parameters"
"Tensorflow 2 Finetuning TF T5 using keras fit "
"Unable to establish Lock on cached tokenizer output from RobertaTokenizer"
"Bert transformer issue"
"How to generate on multiple GPUs?"
"How to use encode_plus to force padding to specific length"
"How to convert '.bin' model to '.onnx'"
"Loading a converted pytorch model in huggingface transformers properly"
"regarding the max token length of longformer"
"No attribute '_mp_fn' when fine-tuning mbart for en-ro translation task using TPU"
"Related to abstractive text summarization"
"Model.fit on GPT2 and TPUs"
"DistributedSortishSampler"
"RAM MemoryError"
"tokenization_gpt2 save vocabulary is not saving special tokens"
"pegasus-large: Can we have input text descriptions more than the maximum input length of 512?"
"Update `convert_bart` script to allow loading nonstandard model architectures and custom pretrained Fairseq models."
"Pegasus: replication and distillation results"
"unexpected behavior on RoBERTa tokenizer when using additional special tokens "
"Printing probabilities"
"FAILED tests/test_modeling_marian.py::TestMarian_EN_DE_More::test_forward "
"Hugging face - RuntimeError: Caught RuntimeError in replica 0 on device 0 on Azure Databricks"
"Changes in Pytorch 1.6 multinomial could break backward compatibility"
"Remove hard-coded uses of float32 to fix mixed precision use in Distilbert"
"Support nested data structures for input data"
"special token inconsistency for [UNK] token"
"How to save the whole model as SavedModel format for inference?"
"Is it possible to finetune reformer model for summarization task? "
"MarianMTModel.generate error: Segmentation fault (core dumped)"
"How does relative distance is computed for cross-attention in T5 model?"
"Albert loads model on both CPU and GPU at the same time"
"transformer multitasking"
"Memory blowup with TPU Trainer in master"
"gradient_accumulation_steps in trainer_tf"
"[TF T5] Possible Error using TF T5 with Keras"
"'BertEmbeddings' object has no attribute 'bias' while converting tf checkpoint"
"Bert Checkpoint Breaks 3.02 -> 3.1.0 due to new buffer in BertEmbeddings"
"AttributeError: 'DistilBertConfig' object has no attribute 'return_dict'"
"[t5] Missing requirements in examples/seq2seq"
"Getting import error"
"the result of translation task on en-zh is not good,especially in short text"
"Can the GPT2 of Transformers receive output hidden_states from external Encoder?"
"Can DistilBert.forward() support token_type_ids ?"
"Relaxing `PreTrainedModel` requirement in _save"
"Example config code uses invalid 'output_attention' rather than 'output_attentions'"
"Greedy decoding for non-beam-search appears to ignore postprocessing"
"[style] automate reformatting with pre-commit hooks"
"adding additional additional_special_tokens to tokenizer has inconsistent behavior"
"batch_encode_plus does not lead to the same predictions as encode_plus"
"Small bug on website"
"[model weights caching] model upload doesn't check model weights hash"
"T5 Tokenizer fails to decode correctly and prints \u2047"
"RuntimeError: Internal: /sentencepiece/src/sentencepiece_processor.cc(818) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
"(ONNX) Error while converting the model: bad allocation"
"inference over onnx output"
"AttributeError: 'list' object has no attribute 'clone' with BartTokenizer"
"Reopen: Unable to use run_squad with xla_spawn.py on TPU"
"onnx-export example notebook is failing for TF"
"non-interactive transformers-cli upload?"
"Load BERT+GPT2 in EncoderDecoder"
"Finetune other models for sentence-classification"
"The downloading url of GermEval 2014 dataset is out dated."
"PyTorch (with GPU) Trainer leaks CPU memory on Google Colab"
"Transformer-XL: Remove unused/unnecessary Parameters"
"Finetuning XLM-Roberta-2XLM-Roberta on custom dataset gives the following error:"
"Restoring ELECTRA-Small checkpoint doesn't work properly"
"Training script for other language (except English)"
"head_mask in modeling_bert.py"
"How to enable grad_fn when calling the generate() method of a T5 model"
"How to insert a hidden output from GPT2 model directly into a BERT layer?"
"PRETRAINED_INIT_CONFIGURATION for local model path"
"Tokenizers became slow compared to 2.8.0"
"Longformer config - vocabulary size"
"transformers-cli upload individual files simplification"
"SPM Tokenizer confusion with fairseq Roberta"
"Potential incorrect loss calculation for TFTokenClassification in TFTrainer"
"Incorrect loss calculation for the last batch in TFTrainer if dataloader_drop_last is False"
"Error installing transformers 3.1.0"
"distilled bart-large/bart-base"
"The configuration of 3.0.2 and 3.1.0 is not compatible"
"LXMERT imports"
"[gen utils] missing else case"
"RunTime Error: CUDA out of memory when running trainer.train()"
"LongformerForQuestionAnswering sample code error"
"[generation] multiple eos/pad asserts/ifs in generate search functions"
"Enhance a MarianMT pretrained model from HuggingFace with more training data"
" DefaultCPUAllocator: can't allocate memory: you tried to allocate 100663296 bytes"
"t5 embed_tokens"
"TypeError: __init__() got an unexpected keyword argument 'cache_dir'"
"Mobile Bert Tiny model"
"PegasusForConditionalGeneration stops at unknown token"
"run_squad.py not working on 3.1.0 version"
"access to the embeddings for query and text used in a downstream NLP task"
"How to bypass \"Special tokens have been added in the vocabulary...\" warning?"
"On Gpu sharing mechanism, specify model.to(' CPU '), but still use the Gpu"
"__init__() got an unexpected keyword argument 'cache_dir'"
"MLM performance difference between bert-base-cased and Conversational BERT"
"Diverse Beam Search decoding"
"Index out of range in Bart-large-xsum"
"Huggingface \"sentiment-analysis\" pipeline always output \"POSITIVE\" label even for negative sentences"
"getting 'ValueError-TextInputSequence must be str' in 'train_dataset = train_dataset.map(convert_to_features)'"
"Error in run_language_modeling on TPU: Transferring data with element type U8 has not been implemented on TPUs"
"Longformer global attention mask, 2 or 1? "
"Proposal: Offset based Token Classification utilities "
"Use  `run_language_modeling.py` to finetune gpt2, but it core unexpectedly."
"where can I download the pre-trained pytorch_model.bin files ? "
"PreTrained (custom) model not correctly initializing when using AutoModel methods"
"Adding `class_weights` argument for the loss function of transformers model"
"Python"
"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
"Getting underling S3 URL"
"No way around \"Truncation was not explicitely activated...\" error when using SingleSentenceClassificationProcessor."
"Unable to recreate onnx speedups demonstrated in 04-onnx-export.ipynb on mac or linux"
"SQuAD: Implement eval in Trainer-backed run_squad_trainer"
"\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference\" when I am finetuning on distilert pretrained model, After printing this it is taking a lot of time and using only one CPU, how can we parallelized to all the cores in the system ( even I hve 8 GPU's but it is not using tht)"
"Question about the test results of my own testsets with a fine-tuned BERT"
"the time of loading different models to GPU is nearly the same?"
"Batch_encode_plus with is_pretokenized=True outputs incomplete input_ids"
"max_length does not seem to work"
"Usage of targets argument in fill-mask pipeline (Pipeline cannot handle mixed args and kwargs)"
"T5-11b model parallelism"
"Convert 12-1 and 6-1 en-de models from AllenNLP"
"How to pass tokenized hypotheses to TFRobertaForSequenceClassification model directly for faster inference?"
"TFBert activation layer will be casted into float32 under mixed precision policy"
"[testing] test_trainer.py is failing"
"mBART 50"
"circleci testing issue"
"EncoderDecoderModel  generate function"
"Further Pretraining of Longformer RAM Consumption"
"How to use hugging on onw embedding"
"MobileBERT inconsistent output (padded / not padded text)"
"Clean up `benchmark_args_utils.py` \"no_...\" arguments"
"some sshleifer/xsum hub models  have bart-large-cnn task_specific_params['summarization']"
"T5Tokenizer shouldn't add pad token as prefix to labels"
"Importing unittests using python unittest framework"
"Longformer output_hidden_states=True outputs sequence length=512 for all inputs of different lengths"
"How to implement LayoutLM for information extraction"
"Distilbart's summaries start with an empty space?"
"Longformer run error"
"train/eval step results log not shown in terminal for tf_trainer.py"
"TypeError: __init__() got an unexpected keyword argument 'gradient_checkpointing'"
"is config argument necessary for XXModel.from_pretrained method? And when is needed?"
"needing area to put download/convert/eval scripts"
"broken pypi scipy package that affects tests under `examples`"
"Longformer inference time"
"ValueError: Wrong shape for input_ids (shape torch.Size([18])) or attention_mask (shape torch.Size([18]))"
"One command to run+aggregate distributed evaluation results"
"MBART/Marian for low resource/backtranslation"
"prepare for the label for EncoderDecoderModel"
"Generate coherent text with T5"
"How to return the word embeddings and how to understand the hidden_states in return?"
"can i self-define the decoder when i user EncoderDecoderModel?"
"Feature request: State the goals and none goals of the library in the README"
"modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator."
"backtranslation script"
"Seq2SeqDataset experiment: try to use arrow datasets"
"only fine tune the encoder part of BART"
"AssertionError with multiple GPU"
"Add tokenizer file save in convert_graph_to_onnx.py "
"evaluate_during_training after each epoch"
"Loss mask for fine-tuning GPT2LMHeadModel model"
" BertForMaskedLM Loss function "
"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
"OSError: Can't load weights for 'nlptown/bert-base-multilingual-uncased-sentiment'. "
"generate text function support part of the  target inputs"
"Onnx + TensorRT uses CPU not GPU"
"unexpected keyword argument 'force_fusions' when running the onnx notebook"
"Load Pre-Trained Model Using Docker"
"Fine tune with local model raised `torch.nn.modules.module.ModuleAttributeError: 'DataParallel' object has no attribute 'config'`"
"Gradient checkpointing for GPT-2"
"Inconsistent parameter naming conventions in ModelConfigs"
"OOM Issue when evaluating with Trainer"
"I reduce the longformer's attention window from 512 to 256, but train speed not changed "
"distributed launch raise Error"
"Create larger summaries by using Summarization models like T5 or Pegasus"
"Pegasus- Arxiv predicts random text"
"tf.keras.models.load_model() does not load saved model that includes TFOpenAIGPTLMHeadModel layer"
"__init__() got an unexpected keyword argument 'cache_dir' "
"weights partially missing for CamembertForMaskedLM "
"DistilBERT for token classification (pytorch) predicts wrong classes for <PAD> tokens"
"BERT Trainer.train() CUDA out of memory error"
"[s2s] Try to get ray/optuna + examples/seq2seq working"
"Bug in finetuning ALBERT on text-classification in GLUE"
"distributed eval cleanup"
"RuntimeError: CUDA out of memory. "
" When trying to train LongformerModel got **forward() got an unexpected keyword argument 'labels'**"
"GPT2 Heatmap Error: 'Parameter' object has no attribute 'get_shape'"
"[s2s]add wandb summary metric that tracks best val bleu/rouge"
"how to continue training from a checkpoint with Trainer?"
"Why does RoBERTa not label custom tokens as special tokens?"
"[s2s] reload dataloaders every epoch?"
"[models website] search UI issues"
"[model cards] yaml specs and related questions"
"Does the default weight_decay of 0.0 in transformers.AdamW make sense?"
"Links seem to be expired for German NER example "
"FSMT Cuda CI Failures"
"tokenizer.add_tokens conflict with MBart's tokenizer"
"The task name sentiment-analysis"
"Suggestion: Better to change the task name of  \"sentiment-analysis\" to \"text-classification\""
"FSMT Training scripts"
"a possible hack for FSMT's SinusoidalPositionalEmbedding peculiarity"
"test fsmt finetuning"
"trainer.evaluate() aggregates predictions on GPU and causes CUDA out of memory issues for large datasets"
"TypeError: 'ByteLevelBPETokenizer' object is not callable"
"KeyError: 'squeezebert' which from model zoo"
"[s2s] distributed_eval edge case"
"How to get cross attention weights of decoder when using 'encoderdecodermodel'"
"[example/glue] run_glue compute metrics fail for bart like models"
"very poor performance of Longformer on SQuAD-like question-answering tasks "
"[testing] when to @slow and when not to? (huge models download)"
"[fsmt] Expanding Positional Embeddings"
"[save/load model] authorized keys, no save keys, etc."
"A confusion about mrc model"
"LXMERT visual feature extraction during training/fine-tuning phase"
"When I updated my transformers to the latest, the previously trained model loaded with an error"
"Changing learning rate for BertModelforTokenClassification"
"Feature Request: Support Longformer 3D attention mask ?"
"LXMERT pre-training tasks"
"Multitask pre-training setting"
"Distilbert classification "
"[Bug/Question] Write With Transformers Implementation vs. Custom Implementation"
"Question about model configuration"
"Unable to serialize/save TF2.0 Bert model"
"I want to use the Bert2GPT2 architecture,but my pretrained Bert and GPT2 have different vocabs,so what should I do for the vocabs? "
"Fine tune BERT based models using Trainer fails"
"scibert-nli out of dace"
"\"index out of range in self\" when calling BertForTokenClassification"
"Error importing MBart from transformers"
"Support serialized tokenizer in AutoTokenizer"
"Bert Fine-Tuning on SQuAD with native TF2"
"test_run_eval_search SLOW failure"
"Marian/MBart should not save static position embeddings"
"BART metrics.json and validation checkpoint metrics seem to disagree"
"Wrong arg order for `nested_xla_mesh_reduce` in trainer.py"
"BertModel for 2 category classification - How to evaluate the performance"
"Cuda OOM training gpt2-xl with Trainer in multi-GPUs"
"[s2s] metrics.json is wrong on multigpu"
"Text generation with xlnet"
"Memory leak"
"Example Format of Data for token classification"
"T5 Cross-attention Decoder - Possible bug with relative_bias"
"[s2s] Marian beam search slow for en-de "
"PegasusTokenizer: Newline symbol"
"Add PRADO model"
"Problem loading a dynamic quantized distilbert model."
"data_collator error: AttributeError: 'dict' object has no attribute 'size'"
"Cannot import transformers with TF version 2.1.0"
"is there a tokenizer only used whitespace for spliting chinese sentence?"
"Error when fine-tune RoBERTa on NSP using Trainer"
"Trainer.py module 'datasets' has no attribute 'Dataset'"
"BufferedWriter takes most of the time"
"Trainer Evaluates at each step (Not of epoch end) , indentation bug"
"data_collator.py - line 326, in mask tokens - xlnet finetuning error"
"CentOS Error installing Transformers"
"AttributeError: 'TFTrainingArguments' object has no attribute 'args'"
"Difference between bart-large and bart-large-cnn vocabulary"
"[s2s] can distributed eval  intiate model download on each rank"
"generic text classification with TensorFlow error (AttributeError: 'TFTrainingArguments' object has no attribute 'args')"
"Faster Pegasus tokenizer tests"
"how can i convert bert pytorch to tf2 ?"
"Example for T5 model from doc is not working."
"How to add some parameters in T5 (in T5Block layer) and initialize the original T5 parameters with pre-trained model and the new introduced parameters randomly? "
"ImportError: cannot import name 'AutoModelForTokenClassification'"
"Difference between tokenize chinese char"
"Getting \"TypeError: forward() got multiple values for argument 'attention_mask'\" when replacing pytorch_transformers with transformers"
"test_rag_sequence_generate_batch failing on CUDA"
"Finetuning Pegasus for summarization task"
"The absence of source/target language parameters when using MBart in Summarization example"
"Add new PET Model"
"FunnelTransformerForSequenceClassification crashes when fine tuning with mixed precision flag"
"CUDA out of memory error for Bert Model "
"how to customize the position encoding"
"Movement Pruning for GPT2"
"Incorrect output fields names in docs"
"Missing keys when loading weights in TF are not useful"
"Custom preprocessing of text"
"[trainer] Training from scratch"
"(GPT2) Running out of GPU memory(24G) on WSL2 but not on native linux."
"Add DistilBERTGeneration comparable to BertGeneration"
"Uploading/Sharing large models to HuggingFace"
"Tokenizers as an optional dependency"
"Add support for exporting summarization models to ONNX"
"Bert base chinese model gives error :- EagerTensor object has no attribute 'size'"
"How to train a model based on CTRL"
"Allow creation of asymmetrical T5"
"Error: isTensor() INTERNAL ASSERT FAILED from traced RoBERTa model on iOS using LibTorch"
"Unable to load pipeline for question answering"
"GPT2LMHeadModel forward input"
"Colab pro -fine RoBERTa error tcmalloc: large alloc 6325288960"
"Possible error in MBart Tokenization script -- target lang code is only present in seq once"
"Add adapter support"
"Cannot reproduce example token classification GermEval 2014 (German NER) dataset"
"\"Sequence Classification with IMDb Reviews \" error, when using \"bert-base-multilingual-cased\" model."
"Getting Import error ImportError: cannot import name 'quantize' from 'transformers.convert_graph_to_onnx' (/opt/conda/lib/python3.7/site-packages/transformers/convert_graph_to_onnx.py)"
"[T5] Automatic setting of decoder_input_ids is misleading and does not correspond to the expected behavior of T5"
"Problem while using tokenizer.encode_plus for sentence pairs"
"import error in version 3.3.0, conflict with local directory \"datasets\""
"Fine-tune BERTForMaskedLM"
"RAG Retriever  (NameError: name 'load_dataset' is not defined   in retrieval_rag.py) "
"CUDA out of memory (ALBERT) - run_squad.py ignores --per_gpu_train_batch_size"
"Faced the TypeError:forward() got an unexpected keyword argument 'output_all_encoded_layers'"
"Setting up transformers/examples/seq2seq"
"Error training GPT-2 from scratch on Hindi"
"Getting Bert Embeddings in Batch"
"v3.3.0 - Issue with name conflict in transformers & datasets - AttributeError: module 'datasets' has no attribute '__version__'"
"What's the most straightforward way to initialise BertForSequenceClassification for different token rather than [CLS]?"
"T5 unsupervised training"
"Seq2seq example for T5 keeps on generating warning"
"Seq2SeqTrainer Distributed: AttributeError and the RuntimeError"
"RAG - how to precompute custom document index?"
"RAG - reproducing RAG-Sequence QA score "
"Seq2SeqTrainer: add a fast test that doesn't learn anything but can run on CPU"
"RAG: Can we have a document that explains the fine-tuning mechanism?"
"Loading saved model not working"
"Upload models using transformers-cli fails "
"Issue with Summary of the tasks - Named Entity Recognition in Docs"
"Tenosrflow Loading the saved Model For GPT2"
"Using BERT for spelling correction"
"Use of global attention of Longformer when generating"
"`run_squad_trainer` doesn't actually use a Rust tokenizer + errors in `squad_convert_example_to_features` when using a Rust tokenizer"
"Sharing Microsoft's DialogRPT (new dialog ranking model)"
"Is the multiple-choice head for the pre-trained `LongformerForMultipleChoice` model pre-trained?"
"quick questions about the `BertModelLMHeadModel`."
"BertforSequenceClassification MSELoss() without normalizing using sigmoid/softmax"
"How to generate data using beam search from a custom gpt2 model?"
"german distilbert not available?"
"Trucated Outputs while finetuning 'bart-base' on XSUM [Summarization Task]"
"Functionality to pass first few tokens as input to the decoder in T5 model"
"Turning the SQuAD dataset class into an iterator to save ram and redistribute time"
"[Reformer, Longformer, Roberta, GPT2, CTRL] attention_mask should be at second argument"
"[Transfo-XL] Impossible to pass `attention_mask` to model"
"[XLNet] attention_mask / input_mask - Why two `attention_mask` inputs?"
"[Longformer] Output both local attentions and global attentions when `output_attentions=True` -> Good Second Issue"
"huggingface transformer running on CPU behind celery/redis doens't work (but works by itself)"
"Overflow error: Can't convert negative value to unsigned it [RAG Model]"
"XLNet finetuning"
"MultiGPU Trainer: each processes uses more memory than 1 GPU job"
"Training loss suddenly increases and stays the same"
"Almost Have Model Parallelism Working on GPT2 Fine-Tuning"
"QA pipeline fails with long context. "
"[GPT-2] How many columns in LM model wte layer are positional embeddings?"
"ELECTRA - some weights are not loaded"
"Cammembert fine tuning from checkpoint"
"The links to examples on the website don't work"
"TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit' when running run_tf_text_classification.py"
"RAG model card code not working in Colab"
"T5 supervised denoising task"
"Trainer fails to correctly tackle XLNetForSequenceClassification outputs"
"Difference between CLS hidden state and pooled_output"
"T5: forward and generate produce different results even for greedy decoding of a single token"
"Seq2SeqTrainer: missing features"
"[s2s] label smoothing loss should be normalized"
"Converting Tensorflow checkpoint to Pytorch not work for TF models downloaded using TFAutoModel.from_pretrained()"
"Longformer2Roberta: global_attention_mask is never used"
"Incorrect tokenization with tokens added using tokenizer.add_tokens()"
"Problem with Finetuned GPT-2"
"RAG: NameError: name 'load_dataset' is not defined"
"RAG: error in outputs = model(input_ids=input_ids, labels=input_dict[\"labels\"])"
"Problem with automatic best model loading."
"Is this realy a list or a Dict[str, int]? I think the docstring might be wrong because in the model json file it is stored as a dict."
"Error Loading Gpt-2 model after training from scratch."
"Two slow deberta test failures"
"Trainer incorrectly checks pytorch version"
"Is training distilbert with TPU supported yet?"
"Import error for MarianMTModel"
"Sequence Classification One-Hot Encoded Data"
"Finetuning T5: Keyword arguments not recognized."
"Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized"
"Trainer evaluate returns empty dictionary"
"RobertaTokenizer.get_special_tokens_mask doesn't check for all special tokens, only for the sep and cls tokens"
"make modified_only_fixup complains about non .py files"
"RagRetriever.from_pretrained doesn't get another cache_dir. "
"XLNet evaluation fails if the size of evaluation set can't be divided by a given evaluation batch size"
"run_language_modeling.py TPU issue during evaluation"
"BartConfig saving and loading inconsistency"
"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead. "
"RagTokenForGeneration.from_pretrained fails while running demo script"
"TFBertMode.pre_trained('bert-base-uncased') --> OSError  "
"Does tokenizer.from_pretrained tokenize text on CPU even a GPU is available?"
"RAG : Can we fine-tune RAG with update frequency method similar to Fairseq framework?"
"way to make inference Zero Shot pipeline faster?"
"Ability to pre-train BART model"
"Tokenizer: UnboundLocalError with PaddingStrategy MAX_LENGTH"
"SquadProcessor: Wrong reference name/filename in docstring"
"Feature Request: Support training and evaluating on Squad-format (json) files in SquadDataset for easy Squad fine-tuning"
"Feature Request: Support training/evaluation on Squad-format (json) files in SquadDataset for quick Squad fine-tuning"
"OSError: Can't load config for saved_model when deploying on EC2."
"position_ids parameter cannot work with past parameter for GPT2Model during batch inference"
"Downloading DPR model ('facebook/dpr-ctx_encoder-single-nq-base')"
"Implement a TF2 version of `GPT2ForSequenceClassification`"
"Implement PyTorch and/or TensorFlow sequence classification architectures for causal language models"
"Free Inference API Not Accessible"
"Unable to pass encoder_outputs to generate calls"
"The newly added config decoder_start_token_id for bart-base model is wrong?"
"Is there a fine-tuning script for DPR?"
"Unique names for dataset cache for each tokenizer "
"How to get cross attention for bert when config.add_cross_attention is True"
"ImportError: cannot import name 'RobertaLMHeadModel'"
"ImportError: cannot import name 'RobertaLMHeadModel'"
"ValueError(\"The training dataset must have an asserted cardinality\") when running run_tf_text_classification.py"
"error AttributeError: 'tuple' object has no attribute 'logits'"
"quick question about `BertForMaskedLM`"
"NER pipeline documentation example failing"
"Project: Gather summarization datasets and try to replicate pegasus results on them"
"does tokenizer support emoji?"
"setup of Trainer class for distributed trainning"
"Fix Failing Slow tests"
"output probabilities of generated sequences in generate function"
"Eval_loss in prediction is very high : transformers/examples/token-classification/run_ner.py "
"T5 Beam search num_beans always equals 1"
"SqueezBert link gives a 404 error"
"loss.backward() being called twice in Trainer._training_step()"
"2 slow TF T5 common tests failing on master"
"TF Slow test CI"
"tokenizer_bert.py not call _clean_text?"
"Clear up confusing translation pipeline task naming"
"Default Model Licenses"
"squad data preprocessor error (list index out of range) while finetuning bert on squad 1.1"
"Correctly tokenize sentence pairs"
"Add FAVOR+ / Performer attention"
"TFTrainer doesn't work"
"Batch and smart batch support for pipelines."
"TFEncoderDecoder"
"Fine-tuning"
"Error with running run_language_modeling.py on GCP TPU"
"Using PaddingStrategy and TruncationStrategy throws an UnboundLocalError in tokenizers"
"When downloading RAG dpr indexes,  there is a pickle file loading error"
"RAG Tokenizer erroring out"
"Seq2Seq Example with Bart not Saving Best Model"
"Fail to run text classification example with run_tf_text_classification"
"How to get the word embedding  after pre-training ? for example, a embedding matrix"
"save_pretrained() does not check if xla is available"
"tokenizers dependency warning: `transformers 3.3.1 has requirement tokenizers==0.8.1.rc2, but you'll have tokenizers 0.9.0`"
"MLflow Trainer Callback"
"GPT2DoubleHeadsModel documentation example question (error in documentation)?"
"Strange error while using the `LongformerForMultipleChoice`"
"Trainer callback breaks old code"
"SQuAD example docs inaccurately suggest settings for bert-large-uncased on a single V100"
"Recording training loss and perplexity during training"
"run_tf_text_classification.py for custom dataset"
"[NEW MODEL] Multilingual document embeddings: cT-LASER"
"Fine Tuning SciBERT NLI model"
"2 RAG test failures"
"2 Deberta test failures"
"rag examples tests fail"
"examples/rag: test coverage, tiny model"
"Hosted Inference API for Token Classification doesn't Highlight Tokens correctly"
"wrong decoder_input_ids[:,0] for MarianMT models ?"
"can u help me out with how to input custom data files in RAG retriever and the data format"
"T5: Finetuning the language modelling objective on a  new dataset"
"how we can replace/swap the Wikipedia data with our custom data for knowledge retrieval in the RAG  model and  the format of the retrieval data.  "
"what is the perplexity of distilbert-base-uncased ? "
"Pytorch 1.6 DataParallel"
"GLUE STS-B on longer sequence lengths doesn't work?"
"Tokenizer Fast bug: ValueError: TextInputSequence must be str"
"blenderbot-3B has wrong model card"
"Cannot load pretrained microsoft's layoutlm"
"examples/seq2seq/finetune_trainer.py: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`."
"should PegasusTokenizer replace `/n` with `<n>`?"
"cannot load \"layoutlm-base-uncased\""
"Attention masks are ignored when using model.generate() in batch setting for GPT-2"
"Keep getting the same `Target 1 is out of bounds` error with `LongformerForMultipleChoice`"
"BertTokenizer meet multilingual corpus, it fails to work.@mfuntowicz"
"Does bart need to cache prev_key_padding_mask?"
"Unicode issue with tokenizer.decode()"
"HfArgumentParser not support optional bools"
"Help with finetuning mBART on an unseen language"
"AttributeError: 'tuple' object has no attribute 'detach'"
"Deutsch to English Translation Model by Google doesn't work anymore..."
"Update of DialoGPT `max_length`"
"Seq2seq finetune example: \"Please save or load state of the optimizer\""
"Use Marian-MT to evaluate translated outputs by printing out per-word log-probility"
"Is there any way to control the input of a layer of `Longformer`?"
"from transformers import RagSequenceForGeneration gives ImportError"
"How to create a QA model where the answer can be from the question text as well?"
"Cannot trace_module on models using model's generate function"
"Error in run_ner.py - ModuleNotFoundError: No module named 'tasks'"
"XLM-RoBERTa model for QA seems not properly work"
"Adding RAG to text-generation pipeline"
"I'm getting \"nan\" value for loss, while following a tutorial from the documentation"
"How can I tweak the `Longformer` code to control the input of a `Longformer`'s layer?"
"`decoder_config` variable not defined in EncoderDecoderModel.from_encoder_decoder_pretrained"
"RAG finetuning - unexpected keyword argument 'early_stop_callback'"
"Unable to serialize/save TF2.3.1 RobertaSequenceClassification model to saved model format "
"[RAG] RagTokenizer failing in decoding RAG Generator output"
"error when using the forward() function of the LongformerLayer class from the LongformerForMultipleChoice model"
"Recommended Adafactor settings for T5 cause error"
"T5 Conversion from Original Tensorflow Produce rubbish Text"
"[stas/sam] Newsroom dataset wierdness"
"T5 finetune outputting gibberish"
"BertForSequenceClassification  -> TFBertForSequenceClassification causes 'bert.embeddings.position_ids' not used error"
"Empty Conversation Responses"
"Can not convert the the custom trained BERT model to pytorch model for further use which should give me .bin file"
"TPU pod training with BERT "
"a"
"pip3 install issue"
"Import error when fine-tuning mbart from master branch"
"ValueError: too many values to unpack (expected 4)"
"Pipeline(summarization): CUDA error: an illegal memory access was encountered"
"Metrics calculate error: can only calculate the mean of floating types. Got Bool instead"
"the results of run_squad.py is terrible"
"BART/TFBart: allow decoder_input_ids.shape[-1] > 1 + use_cache = True"
"\"Cannot re-initialize CUDA in forked subprocess.\" error when running \"transformers/notebooks/05-benchmark.ipynb\" notebook"
"RAG - MissingIndex: Index with index_name 'embeddings' not initialized yet"
"[testing] trainer tests fail - 2 issues"
"[testing] test_modeling_deberta.py is failing"
"Support for custom data_collator in Trainer.train() with datasets.Dataset"
"Bart Caching: do we need encoder outputs after step 1?"
"RFC: Move `_NoLayerEmbedTokens` to modeling_tf_utils.py"
"Do I need to apply the softmax function to my logit before calculating the CrossEntropyLoss?"
"[RAG] RagSequenceForGeneration: Running \"retriever separately example\" giving error"
"[RAG] RagSequenceForGeneration should not load \"facebook/rag-token-nq\" and RagTokenForGeneration also should not load \"facebook/rag-sequence-nq\""
"[testing] test_trainer_callback.py cannot collect test class 'TestTrainerCallback' "
"[s2s trainer] tests fail on multi-gpu machine"
"State of ONNX"
"Token Type IDs returned from the tokenizer for T5 don't work with special tokens"
"[seq2seq distributed] child process stuck on error"
"BartTokenizer prepare_seq2seq_batch() does not return decoder_input_ids,  decoder_attention_mask as per document after passing tgt_texts"
"Where do the models go in colab?"
"RuntimeError with DistributedDataParallel "
"how to save and load fine-tuned model?"
"OperatorNotAllowedInGraphError in dbmdz/bert-base-italian-cased for Token Classification"
"SequenceSummary class in modeling_utils.py"
"unshared Albert"
"labels and decoder_input_ids"
"AttributeError: module 'tensorflow_core.keras.activations' has no attribute 'swish'"
"RAG generate function uses input_ids even when context_input_ids are given."
"can't set evaluation_strategy to \"epoch\""
"[Rag] extend_enc_output fails when number of retrieved documents not equal to RagConfig.n_docs"
"How to do categorical sequence classification?"
"xlm-mlm-17-1280 & xlm-mlm-100-1280 include languages?"
"question about `add_special_tokens` and embedding layer"
"Error(s) in loading state_dict for BertForTokenClassification"
"[testing] the test suite is many times slower than 2 weeks ago"
"Question answering example errors with BrokenPipeError: [Errno 32] Broken pipe"
"Github actions: more readable/informative output"
"from_pretrained incompatible with the models being downloaded"
"Issue with XLM-R for multiple-choice questions "
"pip install transformers by default install 2.5.1"
"AdamW "
"Bert2bert EncoderDecoderModel from Huggingface is generating a zero tensor for any input"
"GPT2Tokenizer.add_tokens() didnt change tokenizer.vocab_size"
"[EncoderDecoder] google/roberta2roberta_L-24_wikisplit"
"example for passage re-ranking using bert-multilingual-passage-reranking-msmarco"
"GPT2Tokenizer strips spaces surrounding special tokens"
"T5 Docs training example has shifted labels"
"[RAG] How to extract generated strings from `RetrievAugLMMarginOutput`"
"Reproducing Bart Xsum from Bart Large"
"[Model] M2M-100 Multilingual machine translation"
"pegasus/cnn_dm  12-2 distillation performing poorly"
"[T5] Ignore sentinel indices for unsupervised denoising / masking objective?"
"run_tf_text_classification.py giving \"ValueError: too many values to unpack\""
" `add_prefix_space=True` option in the BPE tokenizer "
"TypeError: __init__() got an unexpected keyword argument 'vocab_file' in transformers/tokenization_gpt2.py\", line 380"
"what's the values of start_positon and end_position while the answer is impossible in run_squad.py"
"Is it possible to recommend the deployment method for implementing trained mode"
"Loading a pytorch quantized model"
"EncoderDecoderModel not working with DDP"
"Validation loop gives OOM when finetuning T5"
"Reformer model does not work with padded sequences"
"Addition of MMI-antiLM decoding"
"cannot load customized tokenizer with modified vocabulary"
"Your example code for WNUT NER produces array indexing ValueError"
"Access bert output with output_hidden_states=True  of TFBertForSequenceClassification fails"
"EncoderDecoderModel loss function"
"Error: should have a 'get_encoder' function defined when running model.generate()"
"Code bug in tokenization_utils.py?"
"Unexpected/wrong handling of added special tokens in special_tokens_mask (GPT1, BERT, possibly others)"
" 'EncoderDecoderModel' object has no attribute '_init_weights' after `model.resize_token_embeddings(len(tokenizer))`"
"TF: Faster to way to set one column/all but one column of a tensor to -inf"
"T5 on multiple datasets"
"dropping \",\" in date because of Tokenization"
"T5-large on multiple gpus."
"A question about shift_tokens_right in BART model"
"xla_spawn and run_language_modeling slow on TPUs"
"Load tuned model without downloading from huggingface"
"T5 with allowing model changes "
"Should update version requirement for scipy in 'examples\\\\distillation\\\\requirements.txt'?"
"Unable to load UnifiedQA models, tf throws DataLossError"
"TrainingArguments error : TypeError: __init__() got an unexpected keyword argument 'evaluation_strategy'"
"[XLA] Cannot restore from checkpoint on TPU"
"How to make some structural changes to the EncoderDecoderModel ?"
"'DistributedDataParallel' object has no attribute 'save_pretrained'"
"[s2s test] examples/seq2seq/test_finetune_trainer.py::TestFinetuneTrainer::test_finetune_trainer_slow fails on GPU"
"T5 Decoder Inputs"
"[Good first issue] Documentation links in older docs versions"
"BertTokenizer's add_token won't add token"
"How to load tokenizer for models without vocab.txt?"
"do_lower_case not saved/loaded correctly for Tokenizers"
"Differences between facebook/bart-base and facebook/bart-large?"
"TextDataset bug with big files"
"src->transformers->generation_tf_util.py ->_generate_beam_search->outputs = self(**model_inputs)    why self ?There is not a function?"
"AttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'"
"weird output shape when fine-tuning TFDistilBertForSequenceClassification"
"tutorial document"
"Colab can't import trim_batch for T5, anything changed in transformers.tokenization_utils?"
"sentencepiece 0.1.94 causing segmentation fault"
"[bart] SinusoidalPositionalEmbedding breaks under pytorch-nightly"
"[test] tests/test_modeling_deberta.py breaks on pytorch-nightly"
"T5 on multiple tasks"
"(Load dataset failure) ConnectionError: Couldn\u2019t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py"
"[BUG] Unexpected overflowing_tokens in tokenizer.encode_plus"
"BlenderbotSmallTokenizer throws tuple index out of range error for stopword"
"Commit 121dd43 changes DialoGPT generation behavior"
"DataCollatorIntegrationTest::test_nsp failing on GPU"
"ModelUtilsTest.test_model_from_pretrained failiing on CUDA"
"RAG: Do we need to pretrained the doc-encoder when using a custom dataset?"
"Why the functions \"add_special_tokens()\" and \"resize_token_embeddings()\" hurt the performance of 'gpt2' and 'gpt2-medium' but not 'gpt2-large' and 'gpt2-xl' ?"
"Converting Transformers model to Tensorflow model"
"[T5] Unused `n_positions` and `max_position_embeddings`."
"Add m2m 100 multilingual translation model from FAIR"
"BertEncoder has no attribute 'bias' when convert tf checkpoint"
"[testing] port test_trainer_distributed to run with pytest"
"a multitude of deprecations for pytorch-1.7+"
"load 'microsoft/unilm-base-cased' failed "
"Missing Import"
"seq2seq/finetune.py: remove useless check"
"Pretraining for encoder of TF T5 model"
"`BartForConditionalGeneration.from_pretrained` suddenly fails"
"Longformer crashes for position embeddings indexing?"
"Hope more GPT Chinese pretrained model."
"Best practice to use this great repo for industry application."
"FastFormers into transformers"
"Hello world example fail with transformers-3.4"
"#7858 breaks IterableDataset with __len__ in Trainer"
"Documentation error in question-answering pipeline"
"RuntimeError: Trying to create tensor with negative dimension"
"Reformer implementation in Tensorflow"
"Rename swish to silu to give appropriate credit"
"should be BartConfig.prefix  None?"
"run_language_modeling crashes with import cannot import name 'DataCollatorForWholeWordMask' from 'transformers'"
"RagSequenceForGeneration how to get document texts retrieved  in response to a query"
"T5Tokenizer: decode does not show special tokens"
"[Model] mT5 Cross-Lingual Model"
"Documentation code snippet has extra ) after model code "
"Pegasus: Error when training with increased input length"
"fast tokenizer issue on most user uploaded models"
"behaviour of ZeroShotClassification using facebook/bart-large-mnli is different on online demo vs local machine"
"[s2s] distributed eval gets stuck on error w/ multigpu"
"Cannot load saved tokenizer using AutoTokenizer"
"Use pipeline on fine tuned model"
"Error with multi-gpu training "
"Bort (Amazon's reduced BERT)"
"How to perform model.predict loop with TFRobertaForSequenceClassification?"
"In built code not able to download for \"bert-base-uncased\" when running on cluster.  "
"How to get translation of one batch of sentences after batch_encode_plus? "
"Customize tokenizer in model card's widget"
"Vocab files missing in community pre-trained t5 model"
"Is there any Jupyter notebook or detailed example using BertGeneration or EncoderDecoderModel classes?"
"Trainer makes RAM go out of memory after a while"
"ETA on TFEncoderDecoderModel and is BERTShare from https://arxiv.org/pdf/1907.12461.pdf planned?"
"TransformerXL: StopIteration: Caught StopIteration in replica 0 on device 0"
"Make tokenizer.pad() also pad `labels`"
"Masking in Pooling Layer from BERT Output?"
"[s2s] Trainer vs PTL timings"
"ONNX T5 with Beam Search"
"BertTokenizer loses unicode character"
"EncoderDecoderModel: tie weights between different classes of models"
"ConnectionError: ('Connection aborted.', OSError(\"(32, 'EPIPE')\"))"
"generate() always starts with bos_token_id"
"Need suggestion on contributing TFDPR"
"raining loss is not decreasing when using the Roberta pre-trained model from the transformers library"
"Possible bug in \"trainer\" when training \"BertForPretraining.from_pretrained()\""
"Onnx converted model output shape not matching with the finetuned model (BUG)"
"AutoTokenizer.from_pretrained function cannot be customized "
"`do_predict` option of `TrainingArguments` - but no way to pass test set."
"Documentation on how to get results out of trainer is missing."
"cannot load pytorch_model.bin / pytorch version ?"
"Summarization outputs on T5-small gets truncated "
"trainer.evaluate returns 'epoch' from training"
"TensorFlow Longformer model as a saved model with attention outputs"
"T5 (probably BART) issues with the `tf.saved_model.save` API and the `output_xxx` configuration attributes."
"Configuration initialized from checkpoint does not keep the checkpoint identifier in its attributes"
"TextDataset support for tensorflow?"
"pytest Errors"
"Sentencepiece dependency causing docker build to fail"
"Mmmmianam"
"New model addition"
"'SummaryWriter' object has no attribute 'add_hparams'"
"[Benchmark]"
"Sentence transformer Segmentation Fault - Pytorch 1.4.0, 2.80"
"Poor f1 score when validating existing models"
"XLMRobertaTokenizer potential bug "
"Simple import issue for run_clm.py"
"Appropriate dataset format for language modeling example"
"Pickle error "
"[Benchmark]"
"tokenizer \"is_split_into_words\" seems not work"
"Setting os.environ['CUDA_VISIBLE_DEVICES'] = \u20181\u2019, but always training on GPU0, how to set it(GPT2)?"
"tokenizer's is_split_into_words seems not work"
"ValueError: decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation"
"Roberta weights are not initialized loading the bare Roberta"
"Example for running T5 for translation "
"[GPT2] Loss NaN after some time with FP16"
"Why is the accuracy rate of pre-trained GPT-2 model only ~26%?"
"When would pegasus be able to be exported in ONNX format?"
"convert_graph_to_onnx.py and associated example notebook are broken for TensorFlow"
" segmentation fault (core dumped)  proxychains4 python xxx.py"
"is it possible to extract the attention weights on test inputs when the pretrained model is fine-tuned on custom data?"
"ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler'"
"Contributing trained Greek<->English NMT models implemented with fairseq"
"filelock hangs for example script \"run_language_modeling.py\""
"Weird Behavior in Finetuning Pegasus on a Custom Dataset/Longer Summaries Generated"
"Error converting tensorflow checkpoints"
"_shift_right when to use"
"tokenizer.vocab key and values is change begin line 261?"
"Train BERT with CLI commands"
"when the txt file has 5GB, a Killed prompt appears."
"Disable default sigmoid function for single label classification Inference API"
"Encoder Decoder Model"
"GPT2 is not jit-traceable"
"Is there a pre-trained BERT model with the sequence length of 2048?"
"Low accuracy after load custom pretrained model in a text binary classification problem"
"Saving and reloading DistilBertForTokenClassification fine-tuned model"
"SqueezeBert does not appear to properly generate text"
"[commit #29b536a]AttributeError: module 'numpy.random' has no attribute 'Generator'"
"Finetuning T5 on translation wmt19(de-en)"
"Translation finetuning error : TypeError: '>' not supported between instances of 'function' and 'int'"
"[rag] missing a working End-to-end evaluation example"
"RAG performance on Open-NQ dataset much lower than expected"
"Training T5-large model for Question Answering"
"Why do I use XLMRobertaTokenizer and return an error on token_type_ids?"
"finetuning T5 on translation on TPU, questions about clarifying the setup"
"could you please give me a torch example of xlm-roberta-(base/large) for multilingual-text question?"
"Fine Tune Bert Ner using TFBertForTokenClassification.from_pretrained"
"Validation data in `run_mlm.py` is the same as train data"
"[s2s] 1 GPU test for run_distributed_eval"
"How can we freeze the last few layers of a  BERT model using tf 2.0(or higher)"
"Exception: process 0 terminated with exit code 17 when using xla_spawn "
"Resource exhausted when training in loop"
"Tokenizers save_pretrained broken when defining vocab and merges file arguments (v3.1) "
"run_mlm.py: error: argument"
"Running seq2seq_trainer with iterable datasets "
"error  'ascii' codec can't decode byte 0xc3 in position 6550: ordinal not in range(128)\\n\", when running finetune_trainer.py on multiple tpus "
"OOMKilled with exit code 137 with finetune_trainer.py"
"No loss in model output for TFElectraForPreTraining"
"FutureWarning: This config doesn't use attention memories, a core feature of XLNet even though I'm using mem_len"
" Does Tokenizer provide parameters to split the number\uff1f"
"tensorboard.compat.tensorflow_stub.errors.AlreadyExistsError: Directory already exists"
"Keyword arguments {'add_prefix_space': False} not recognized."
"AlbertTransformer head_mask not subscriptable error when not passed"
"Adding call back to measure time of each step "
"Muti GPU training with torch==1.7.0 not working"
"Measuring time when using xla_spawn on multiple cores "
"FRCNN in the LXMERT demo outputs different features when using a local image vs. an image from a URL "
"Language Model to get only the score of predefined tokens"
"pip cannot install transformers with python version 3.X version on Ubuntu 18.04"
"finetune_trainer being really slow on TPU"
"got an unexpected keyword argument 'early_stop_callback'"
"model parallelism for BART"
"Error in RAG finetuning script"
"Adding kNN language modeling and Machine Translation"
"TFTrainer stuck in evaluation"
"PEGASUS generation/decoding VERY Slow"
"apply_chunking_to_forward should only be the same in the chunking dimension"
"torch 1.4.0 transformers segment fault"
"I encountered an error when I was running code that the object could not be called.But the BertTokenizer doesn't exist in my code."
"[s2s trainer examples] a tight quality regression test"
"update from v3.0.0 to v3.4.0 got an error"
"finetune_trainer segfault "
"  assert tgt_line, f\"empty tgt line for index {index}\" with t5 "
"Cannot Load roberta tokenizer"
"TF generate() function is incompatible with output_attention and output_hidden_states "
"TFTrainerArguments: ImportError: Method `device` requires PyTorch."
"Which model to choose for seq2seq(generating headers for articles)?"
"BertTokenizerFast object has no attribute 'ids_to_tokens'"
"Fine-tuning for QA: how to prepare custom dataset?"
"removing runs folders "
"RobertaTokenizerFast is around 10 times slower than BertTokenizerFast #510"
"DataCollatorForWholeWordMask is missing _tensorize_batch method"
"training T5 on  multiple datasets "
"finetune_trainer crashes in the beginning "
"comment correction in modeling_bart.py"
"TPU padding in seq2seq codes "
"ImportError: cannot import name 'is_flax_available' from 'transformers.file_utils'"
"DataCollatorForWholeWordMask error persists after fix"
"Trainer QA Model Label Names"
"Training script \"run_mlm.py\" doesn't work for certain datasets"
"Inreproducible loss when train same setting bert model >=2 times"
"[s2s finetune] huge increase in memory demands with --fp16 native amp"
"Tokenizer problem for model 'patrickvonplaten/longformer-random-tiny'"
"All the weights of the model checkpoint at roberta-base were not used when initializing"
"Tokenizer return nothing instead of unk for certain token? "
"continuing fine-tuning from the last checkpoint "
"[seq2seq] translation tpu example doesnt work"
"Does MBartTokenizer remove the parameter decoder_input_ids?"
"Electra multi-gpu pretraining."
"Wrong files names in model list for \"xlm-roberta-large-finetuned-conll03-german\""
"Set num_beams=4 for all Helsinki-NLP models"
"RAG: Explanation on Retriever Variables."
"Get Scores for each NE Label"
"login to huggingface forum"
"CUDA out of memory (ALBERT)!!"
"Models fine-tuned with gradient checkpointing (=True) fails to export to ONXX"
"Dropout p is changing after loading"
"Model name 'facebook/rag-sequence-base/*' not found when running examples/rag/finetune.sh"
"Make sure the slot variables are created under the same strategy scope."
"Can't find model to down"
"A layman wants to train DistilBERT"
"config.attention_head_size for structured pruning out-of-box"
"Fine-tuning GPT: problems with padding"
"_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union"
"Add POINTER model"
"Can't down models from huggingface.cn!"
"ValueError: No gradients provided for any variable: ['tf_bert_for_masked_lm_6/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_masked_lm_6/bert/embeddings/position_embeddings/embeddings:0'......"
"Question Answering Documentation Example Bug"
"multiple hard-coded paths in transformers/file_utils.py"
"Pytorch Vs Onnx: Pytorch is faster and provides different output"
"Fine tuning a classification model with engineered features"
"Pegasus models load very slowly or do not load at all on initial execution of from_pretrained() when Python is spawned from within a Node.js process"
"TFBertForTokenClassification scoring only O labels on a NER task"
"GPT2 (pre-trained not fine-tuned) only generates additional special tokens "
"Support fp16 for inference"
"Trainer runs out of memory when computing eval score"
"How to print out the probability for each bean search result in gpt2 text generator?"
"Error when upload models: \"LFS: Client error\""
"transformers.TFTrainer: Does not support batches with sequences of variable lengths?"
"automodel"
"Gradient accumulation averages over gradients"
"`log_history` does not contain metrics anymore"
"I meet the zero gradient descent"
"error occurs when trainning transformer-xl by ddp"
"Allow tensorflow tensors as input to Tokenizer"
"Error when loading a model cloned without git-lfs is quite cryptic"
"Unable to install Transformers"
"Why is the XLM-RoBERTa sometimes producing a standalone start of the word character (the special underscore with ord = 9601)"
"TF T5-small with output hidden state and attention not owrking"
"Training the TFGPT2LMHeadModel with model.fit produces error"
"Failed to push model repo"
"Unexpected behavior when using PubMedBERT with AutoModelForMaskedLM"
"DPR model: FileNotFoundError: Couldn't find file"
"Fill-mask pipeline removes space after token prediction when loading pre-training model based on roberta-base"
"TPU issue: possible memory leak in eval loop"
"Finetune TFBertForMaskedLM model.fit() ValueError"
"Adding Confusion matrix support in Trainer"
"Issue while model sharing and uploading on huggingface"
"Using Pretrained BERT model to add additional words that are not recognized by the model"
"How to pretrain the model (like Roberta) again?"
"XLM-RoBERTa tokenizer changes characters during tokenization"
"MLflowCallback to log run_name argument"
"Tagged versions of model in new model hub don't work"
"Reformer model crashes during casual LM evaluation"
"LayoutLM Token Classification not learning"
"`TypeError: unhashable type: 'list'` when using DataCollatorForWholeWordMask"
"Problem while pretraining MLM from scratch using Transformers"
"[models website: files section] various issues/suggestions for a better UI"
"converting tensorflow checkpoint to pytorch"
"Bertabs example: index_select(): Expected dtype int64 for index"
"mBart prefix and suffix for language id"
"Pretrain PEGASUS from scratch"
"Add a new model ConvBert"
"TFBertModel not working at all with any type of model"
"TFLongformer Error: TypeError: __init__() missing 1 required positional argument: 'last_hidden_state'"
"Specify label name"
"Failed in predict function after converting xlnet model to onnx format"
"Upload models using Git fails"
"Pretrain BERT with user defined vocabulary"
"\"special token {} has to be either str or AddedToken but got:"
"`disable_ngram_loss` doesn't work correctly in ProphetNetForConditionalGeneration"
"Allow the user to input positional embeddings"
"TFGPT2LMHeadModel fp16 support"
"Prophetnet - predicted n-future tokens"
"Wrong model_max_length for BERTOverflow tokenizer"
"\"setup.py\" does not seem to have been updated for v3.5.1"
"After 2nd iteration: always same result when training in a loop"
"[T5] Add open / closed book answering models"
"Bert that receives text triplet as an input"
"[Improvements] Enable `git push` without requiring login when uploading model"
"REALM checkpoints to pytorch checkpoints"
"run_pl_glue.py token_type_id error on fresh install"
"Error: Asking to return token_type_ids while setting add_special_tokens to False"
"The Albert tokenizer file cannot download automatically and the official Albert tokenizer file is wrong, I cannot use it."
"Hosting and online deployment of a transformer chatbot (built with huggingface library)"
"Cannot train model from scratch using `run_mlm.py`."
"Improving performance results for BERT "
"PEGASUS do not have mask token"
"Speed up repetition penalty logits processor"
"Accessing gradients of Bart hidden states"
"TFTrainer & Eager mode"
"converting REALM tensorflow checkpoints to pytorch"
"Extracting word representations from BPE-tokenization-based models (GPT-2, RoBERTa, etc.)"
"Missing `tokenizers` file?"
"How to train EncoderDecoderModel using bert for seq-to-seq model"
"ValueError while running run_glue.py with xlnet model."
"Batch Size error"
"seq2seq_trainer optimization issue on TPU "
"`DataCollatorForLanguageModeling` modifies `input_ids` via `labels` variable"
"run_pl_glue.py (almost equivalent performance with non-english bert models)"
"CUDA error when training roBERTa from scratch with data parallel."
"[s2s] distillation.py fails with apex"
"AttributeError: module 'typing' has no attribute '_ClassVar'"
"Bi-Directional Reformer text multi class classification"
"Setting Evaluation Strategy in the TrainingArgs does not print validation metrics"
"Model embedding size and tokenizer size mismatch; resizing embedding will cause CUDA assert error"
"CPM LM"
"How can get the input embeddings_output for BERT?"
"from_pretrained()'s load() blocks forever in subprocess"
"Why use 'BertLayerNorm'  instead of torch.nn.LayerNorm ?"
"RAG: OSError: Can't load tokenizer for 'facebook/rag-sequence-nq/question_encoder_tokenizer'"
"WNLI benchmark results clarification"
"Error in NER examples, run.sh"
"Return output probabilities with Generate function "
"cannot load t5-base config "
"Can't upload the larger model file(9GB)"
"Make signature of `compute_metrics` parameter in Trainer class more flexible"
"Is Reformer supported under Encoder-Decoder framework?"
"Running Roberta on Race Multi choice dataset giving error "
"Issues Fine-tuning XLNET "
"Result changes if we don't pass attension mask in TFDistilbert model on SQUADv1 dataset"
"use the torchscript in a gpt model is slower  than origin one."
"Bert variants pretrained on Wikipedia are easily downloaded. Are the optimizers from the pretraining also available?"
"Pegasus Xsum Returning Tokens Not In Source Text"
"[Question] Pegasus tokenizer"
"connection issue"
"Pegasus example not working"
"issues with seq length with inference code for classification"
"CSV/JSON file format for examples/token-classification/run_ner.py"
"Cannot load tokenizer in community T5 pretrained model"
"training text_classification with tpu using xla_spawn gives wrong result"
"TypeError: an integer is required (got type NoneType)"
"Question about beam_sample: using two softmax?"
"providing the user with possibility to set the cache path "
"Generating from mT5"
"DPRReaderTokenizers returns, for multiple passages given, only the tokens & masks of one passage "
"T5v1.1 Addition of special tokens"
"Accuracy changes dramatically"
"Can't load weights for"
"[BUG] Wrong Scores for many SQUAD models"
"Model predictions wrong"
"distributed_eval does not run"
"eval of seq2seq/finetune_trainer does not work on multiple gpus "
"placing the run dir only in the output_dir"
"Issues with finetune_trainer.py on multiple gpus"
"Unable to Tie Encoder Decoder Parameters When Using EncoderDecoderModel Constructor"
"Broken links in example for torch.load() after. converting tensorflow checkpoint to pytorch save model"
"run_clm.py training script failing with CUDA out of memory error, using gpt2 and arguments from docs."
"a bug in generation_beam_search.py"
"Model conversion from PyTorch to TF2 doesn't work properly for XLM-Roberta"
"@ehsan-soe I fixed the problem by truncating incomplete batches. So if there are 2001 examples and my batch size = 2, then I truncate the last example and train on the first 2000. This has fixed it for me both with and without distributed. My load_and_cache function now looks like this"
"Longformer inference speed is slower than bert of the same length"
"It seem do not support convert multilabel classification model  to onnx  ?"
"[Benchmark] V100/A100 benchmarks, dashboard concept"
"[proposal] do not load all 3rd party packaged unless needed"
"Model can't be downloaded"
"[trainer] `model` argument is not the same depending on n_gpus"
"AttributeError: 'BertTokenizerFast' object has no attribute 'max_len'"
" Blank line indicates the end of a document for NER training ?"
"Add instructions for syncing forked masters to avoid references"
"\"AutoTokenizer.from_pretrained\" does not work when loading a pretrained Albert model"
"[core] transformers version number normalization"
"Allow to provide specific params in WandbCallback"
"Why there are no such 'cls/' layers in roberta pytorch checkpoints"
"Continued training of the original BERT models (not to PyTorch)"
"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 49: invalid start byte"
"[Help] GPU with query answering"
"Version 3.5 broke the multi context/questions feature for the QuestionAnsweringPipeline"
"AttributeError: type object 'T5ForConditionalGeneration' has no attribute 'from_config'"
"[Error: PyTorch to tf]convert_pytorch_checkpoint_to_tf2: AttributeError: bert.pooler.dense.weight not found in PyTorch model"
"Allow to set truncation strategy for pipeline"
"LXMERT - Visual features don't match original implementation"
"Model Parallelism and Big Models"
"Possible to add additional features as input to TFBertForSequenceClassification?"
"saving checkpoints on gs bucket "
"Converting all model Config classes to dataclasses"
"Documentation and source for `RobertaClassificationHead`"
"Using the XLNet or Tranformer-XL as an EncoderDecoder "
"Can't load tokenizer for 'facebook/rag-token-base/question_encoder_tokenizer'. "
"Unexpected output from bart-large"
"Different ouputs from code and Hosted Inference API"
"What would be the license of the model files available in Hugging face repository?"
"QA pipeline fails during convert_squad_examples_to_features"
"KeyError: 'eval_loss' when fine-tuning gpt-2 with run_clm.py"
"[FlaxBert] Non-broadcastable attention mask in batched forward-pass"
"[finetune_trainer] --evaluate_during_training is no more"
"Loss pooling layer parameters after Fine-tune."
"Can I get logits for each sequence I acqired from model.generate()?"
"Problem with using custom tokenizers with run_mlm.py"
"Multiprocessing behavior change 3.1.0 -> 3.2.0"
"Use GPT to assign sentence probability/perplexity given previous sentence? "
"Get locally cached models programatically "
"HuggingFace pipeline sentiment analysis giving wrong results."
"I can not find a Linear layer in the end of Multi-Head Attention layer like Figure 2 right, could someone help me solve it"
"cache reuse"
"Slower training time per batch for increasing dataset size"
"Shared vocabulary with EncoderDecoderModel"
"[s2s finetune_trainer] a mess around distributed "
"error: sentencepiece 0.1.94 is installed but sentencepiece==0.1.91 is required by {'transformers'}"
"Longform QA demo breaks after clearing cache"
"logging.set_verbosity_error() displays dict instead of NotebookTrainingTracker"
"AutoTokenizer can't find model/tokenizer config.json "
"cannot run \"examples/language-modeling/run_mlm.py\""
"Inconsistent PreTrainedTokenizerBase.pad  argument default value & docstring"
"RuntimeError: found torch.cuda.HalfTensor expected torch.cuda.FloatTensor while fine-tuning RAGSequence-base with custom data"
"T5 generations for pretraining objective degenerate"
"\"BertForMaskedLM - pretrained model\" cannot resize vocab output size"
"mT5 fine-tuned model generate wrong answer"
"How to globally change the PYTORCH_PRETRAINED_BERT_CACHE path"
"KeyError: 'mt5'"
"Some unintended things happen in Seq2SeqTrainer example"
"KeyError: 'labels' in   training_step in transformers/trainer.py"
"[trainer] add distributed_env to TrainingArguments"
"transformers/trainer.py stops after some iterations for iterative dataloaders."
"TypeError: forward() got an unexpected keyword argument 'past'"
"Unwanted left shift of target tokens in `get_nll`"
"AttributeError: 'NoneType' object has no attribute 'from_pretrained'"
"can the BertModel convert to onnx? whether any one had done sucessfully ?"
"different embedding weights for base-uncased with different transformers versions"
"length_penalty not influencing results (Bart, Pegasus)"
"Exporting ALBERT model to onnx increases model size by 7x"
"Token classification example only returns labels as -100 for longformer"
"Decrease Longformer window size / computational cost"
"Deberta Tokenizatiion"
"How to pass the attention mask as a param to model forward when using torchscript?"
"Results are different when fine-tuning continues after loading model from checkpoint "
"Resume training from checkpoint: not progressing"
"dropout(): argument 'input' (position 1) must be Tensor, not str With Bert"
"Extracting important information"
"UnicodeEncodeError: surrogates not allowed with GPT2Tokenizer"
"'Some weights of BertModel were not initialized from the model checkpoint at ./model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']'"
"clip_grad_norm on Multiple GPUs: (CUDA error: device-side assert triggered)"
"trainer.py does not handle distributed training for iterative datasets and is very slow"
"providing an example with a dummy iterative dataloaders"
"[\ud83d\ude80 Feature request]  Performer support, tensorflow code, not jax."
"custom prepare_inputs_for_generation for generation "
"finetune_trainer with python -m torch.distributed.launch"
"Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-Q_fyRn/sacrebleu/"
"Wrong Length of Dataset in examples/seq2seq/finetune_trainer.py"
"Removing Head Layer/Model Conversion"
"Using doc chunks without answer token during training ( BertForQuestionAnswering )"
"Unexpected situation when freezing BertForMaskedLM"
"Question: What's the difference between tokenizer_utils, tokenizer_utils_base & tokenizer_utils_fast"
"FlaxBertModel examples (and fast attention)"
"\"No log\" when training RobertaForSequenceClassification using Trainer"
"Help to run an Example Code (it's a bug maybe ?)"
"Fine-tune with custom data"
"Impossible to use  sentencepiece"
"BertModel outputs string instead of tensor"
"TransfoXL Slow Test Fails"
" ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds"
"run_glue.py fails with RoBERTa but succeeds with other models"
"Documentation License Query"
"Relative Attention Bias not initialized for T5ForConditionalGeneration in version 4.0.0"
"phase level tokenizer"
"Unexpected behavior when using TFRoberta model inside tf.keras model"
"Gradients of BERT layer outputs to inputs"
"MobileBertForSequenceClassification outputs super-high logits"
"sorry I mistakenly submitted a issue twice. Plz ignore (help delete) this one. "
"failure to use conda-forge apex  with torch1.6 and --amp_backend='apex' + --fp16_opt_level O1 "
"Error running source code -- import"
"NER Pipeline Issue"
"Why BertSelfAttention reshape Q,K,V from 3-D tensor to 4-D tensor"
"how to use EncoderDecoderModel to do en-de translation?"
"Sparse Transormer"
"Error during validation Trainer step"
"vocab_file and merges_file still required params for loading serialized tokenizers"
"batch_sampler with trainer.py would not set the epoch "
"Wrong shape output for loss of TFGPT2LMHeadModel"
"Fine-tuning on Language Model using two tasks"
"shutil.Error: Destination path '/home/ubuntu/.cache/huggingface/transformers/transformers' already exists"
"run_ner.py with xlm-roberta-base raises an IndexError in tokenize_and_align_labels"
"FileNotFoundError: [Errno 2] No such file or directory: 'cached_train_BertTokenizer_180.lock'"
"TFBertModel NOT learning at all! "
"PegasusTokenizer requires the SentencePiece library but it was not found in your environment"
"Make loss function an init parameter"
"EncoderDecoderModel works poorly with Mlflow"
"02-transformery.ipynb - output from model only strings 'last_hidden_state', 'pooler_output'"
"MobileBERT decoder capabilities"
"BertForMaskedLM train"
"Deepcopy and pickling fails for modeling_outputs"
"BertConfig.id2label use list instead of \"int: string\" dict"
"[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): "
"DistilBert PyTorch to TensorFlow conversion - input sequence length is max 5 tokens for tensorflow"
"AttributeError: 'Trainer' object has no attribute 'is_world_master'"
"Marge - Pre-training via Paraphrasing"
"AlbertTokenizer handles special tokens incorrectly"
"ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds"
"\ud83c\udf1f CTRLsum"
"\"run_mlm_wwm.py\", line 284 AttributeError: 'DataTrainingArguments' object has no attribute 'valid_ref_file'"
"LayoutLM wrong shape for bbox in docs"
"getattr introduces bug when setting booleans with config file "
" Error tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
"About the input of BERT"
"run_clm.py Issue | MODEL_FOR_CAUSAL_LM_MAPPING is None"
"Use Softmax classifiering for run_glue.py example"
"Untranslation of some words from an external dictionary"
"Compatibility scripts"
"Uber AI plug and play language model (PPLM)"
"GPT2 attention mask"
"ImportError: cannot import name 'DPRReader' from 'transformers'"
"Improve coverage of the documentation"
"[docs] missing info on call back registry"
"BERT outputs are different with the same input in training mode"
"Zero Shot Classification Pipeline fails when running in CPU-only Docker container"
"google/bert2bert_L-24_wmt_de_en doesn't match official implementation"
"The example code does not work"
"XLNet ONNX model giving error: \"Attempting to broadcast an axis by a dimension other than 1\""
"\ud83d\udc1b [TF_BART] \"<internal expr>\" has dtype float32 in the TRUE branch, but dtype=int32 in the FALSE branch"
"BlenderBot RuntimeError: CUDA error: device-side assert triggered"
"\ud83d\udc1b [TFBART] LayerDrop not working on TPU"
"Add caching mechanism to BERT/RoBERTa/GPT2 for Seq2Seq accelerated generation"
"TFTrainingArguments"
"Can't load mt5 model after resizing token embedding"
"Token classification example (run_ner.py) should work without fast tokenizers"
"Having to specify too many `ignore_keys` in `Trainer.prediction_step`"
"\"resize_token_embeddings\" in BertForeMaskedLM won't change last linear layer \"output dimension\""
"overflow_to_sample_mapping missing in in documentation"
"ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' - SAVE_STATE_WARNING has been removed from pytorch"
"CharacterBERT"
"Embedding documents on multi-GPU single-ode Docker using pretrained models of huggingface transformers and pytorch DistributedDataParallel"
"Add BartForCausalLM analogs to `ProphetNetForCausalLM`"
"attention_mask size"
"get type error when I run the example code of token classification"
"Zero Shot Classification Pipeline gives poor results locally than online demo"
"Add Definition of a transformer to the glossary"
"T5 fails on many datasets with [libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0):  terminate called after throwing an instance of 'google::protobuf::FatalException'   what():  CHECK failed: (index) >= (0):  Aborted "
"Fine tune GPT-2 pytorch"
"Segmentation fault (core dumped) running run_qa.py"
"Image rendering not working in example notebook"
"Problem with Token Classification models "
"Adding to docs how to train CTRL Model with control codes. "
"Getting a 404 error when loading TFXLMRobertaModel from 'xlm-roberta-large'"
"BertForSequenceClassification finetune training loss and accuracy have some problem"
"run_clm.py Early stopping with ^C"
"run_clm example gives `CUDA out of memory. Tried to allocate` error"
"Chinese"
"Not able to load T5 tokenizer"
"head mask issue  transformers==3.5.1"
"[TorchScript] Received several warning during Summarization model conversion"
"Is the LayoutLM working now?"
"bug with _load_optimizer_and_scheduler in trainer.py"
"Link to BERTology example is broken"
"Unexpected logits shape on prediction with TFRobertaForSequenceClassification"
"Seq2Seq training calculate_rouge with precision and recall"
"Cannot load custom tokenizer for Trainer"
"Cannot load community model on local machine"
"Time for second encoding is much higher than first time"
"Cannot disable logging from trainer module"
"Not able to train RoBERTa language model from scratch"
"Longformer `token_type_ids` Vocabulary Size is 1 But Documentation States Otherwise"
"Some Models do not support gradient checkpointing"
"Roberta training crashing due to position_id embedding"
"Different inference results of a keras including transformer model on TPU vs CPU?"
"Which dataset is used for training GPT, GPT2 from scratch?"
"[Generation] Add generation outputs"
"RobertaTokenizer fails to do_lower_case, different behavior between version 2 and 3"
"BART cannot accept -100 as ignored label"
"Predict single sentence for Glue Tasks"
"seq2seq finetuning scripts break before training (cannot import name ParallelMode)"
"Trainer: support iterable datasets for evaluation "
"adapting trainer.py for multiple optimizers "
"RAGRetriever loads dataset in the default cache dir even if a different one is specified"
"Saving model errors"
"Ray tune hyperparameters search error"
"BertTokenizer.from_pretrained fails for local_files_only=True when added_tokens.json is missing"
"Log metrics along with hparams in TensorBoardCallback"
"BertForSequenceClassification and DistilBertForSequenceClassification use pooler output in different ways"
"evaluate_during_training is not acceptable in newer version of the Transformer"
"Sharded DDP training fails with seq2seq models"
"T5 checkpoint contains weights missing on current model."
"Getting a 404 error when loading 'model=facebook/bart-large-mnli' from pipeline('zero-shot-classification')"
"Unified transformer interface"
"Trainer bug? Loss and logits are \u201cnan\u201d when fine-tuning NLI model (both RoBERTa/BART)"
"Metric calculation across batches in seq2seq examples"
"IndexError: index out of range in self while using longformers when i try to pass token_type_ids"
"Roberta python Tokenizer encodes differently across transformers==2.11 and transformers==4.0.1"
"Language modeling logging"
"Trainer returns logits of only one sequence instead of entire evaluation dataset"
"GPT2 eval with attention_mask not returning expected result"
"[ci] install fairscale on self-runner CIs"
"[trainer]  speed issues: --fp16 doesn't improve speed, DP runs really slow "
"[RagSequenceForGeneration] generate \"without\" input_ids"
"can we use ckpt model file generated after finetuning the pre-trained models on custom dataset"
"Problem with pretraining GPT-2 on TPU with Pytorch/XLA"
"MRPC Reproducibility with transformers-4.1.0"
"Segfault on python 3.9 exit"
"example code for fine-tuning CLM does not work for GPT"
"Loading MPNet from disc: ValueError: An instance of tokenizer class MPNetTokenizer cannot be converted in a Fast tokenizer instance."
"Error \"if input.dim() == 2 and bias is not None\""
"Beam search fails when using model parallelism"
"when to use sortish sampler "
"[wanted] explicit docs for inherited methods "
"Load saved Pytorch model into Tensorflow or convert from Pytorch model to TF"
"[model_utils] very slow model instantiation"
"DataTrainingArguments: __init__() got an unexpected keyword argument 'evaluate_during_training'"
"Saving Pretrained Tokenizer"
"Error while loading model file - .ckpt file :: Missing key(s) in state_dict"
"MLFlow logger breaks training"
"LXMERT cross_modality_matching logits order in code vs. documentation"
"bert-mlm-converting from tensorflow to pytorch"
"Save underlying BertModel only"
"File \"/opt/conda/envs/updated/lib/python3.7/site-packages/fairscale/nn/data_parallel/sharded_ddp.py\", line 280, in _setup_backward_hooks     assert p_tmp.grad_fn is not None"
"checkpoint callbacks "
"run_clm.py AttributeError: 'NoneType' object has no attribute 'keys'"
"TAPAS: IndexError: index out of range in self"
"Does provide any scripts about how to convert mpnet pretrain model to transformers pretrain one?"
"passing config file to train with on multiple gpus"
"allowing the user to set booleans with HfArgumentParser"
"n_gpu=1 in case of using distributed pytorch launcher "
"n_gpus is set to 1 in case of distributed training on multiple gpus, how to access to the correct n_gpus"
"Can't lazy initialize BART model on GPU"
"Differences between original implementation and HuggingFace implementation"
"Generate function does not work with GPU"
"command line_by_line missing in https://github.com/huggingface/transformers/tree/master/examples/language-modeling"
"run_mlm.py crashes when saving model checkpoint"
"mBART finetuned on XSUM"
"Bug SqueezeBERT stops with no error "
"Help: How to deploy a fine tuned t5 model in production"
"Load from a TF 1.0 checkpoint in modeling_tf_utils.py"
"AssertionError with model_parallel in run_clm.py"
"BatchEncoding.to accepted types too restrictive"
"[s2s] test_finetune_trainer_slow fails when run in group"
"AssertionError: Non-consecutive added token '<pad>' found. Should have index 40002 but has index 40000 in saved vocabulary"
"T5 tokenizer.vocab_size and config.vocab_size mismatch?"
"numpy ndarray type is not allowed on process pytorch model"
"GPT2 distributed TPU pre-training using run_clm.py "
"ValueError: Tokenizer class T5Tokenizer does not exist or is not currently imported."
"Prediction problem of glue task"
"Pegasus Documentation May Conflict With Seq2Seq ReadMe"
"torch.hub colab doesn't work"
"[seq2seq] memory regression"
" compute_metrics in the trainer does not seem to be extensible "
"[finetune_trainer]  max length cl args redesign"
"[hf args] shouldn't match partial arg names"
"Unable to load LayoutLM from pretrained"
"Output probability from model.generate"
"how can I change the AlbertModel's vocab"
"Loss printed by tensorflow fit() differs from loss using custom loop for RoBERTa"
"Disable progress bar for Trainer"
"Vision Transformer"
"issue with evaluation of seq2seq_trainer.py on multiple gpus"
"TFRobertaModel warning  - `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated"
"Why Bert-chinese use do_lower_case=False?"
"SummarizationModule, Trainer and BertPreTrainedModel"
"Problem converting slow tokenizer to fast: token out of vocabulary"
"Good Second Issue: T5 FP16 in Pytorch"
"`transformers.models.bart.modeling_bart._prepare_bart_decoder_inputs` seems to be renamed but remains in the document"
"\u3010 run_mlm.py \u3011attention_mask will be set to [1,1,...1] with DataCollatorForLanguageModeling "
"comment correction in test_retrieval_rag.py?"
"from_pretrained does not load the modified part of model "
"Entry-level demo of visual question answering"
"ModuleNotFoundError: No module named 'tokenizations.tokenizations'"
"T5-base goes out of memory on 4 GPUs with as small batch size as 4 "
"RAG model implementation seems different from the paper"
"[TFBart-like models] Problem with tf saving"
"[model site] missing language tags for t5 models"
"[model site] search UI: language: tags, directionality and filtering"
"Bug: metrics inside on_evalute callback is passed wrongly "
"Fail when running the multimodal example"
"Some weights of AlbertForPreTraining were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']"
"Splitting texts longer that `tokenizer.max_length` into blocks of same size "
"Conda dependencies conflict with pip dependencies"
"Music Transformers"
"Issue with 'char_to_token()' function of DistilBertTokenizerFast "
"No module named 'transformers.modeling_albert'"
"expected str, bytes or os.PathLike object, not NoneType"
"how to checkpoint all the validation scores in huggingface trainer "
"Fail to reload tokenizer from save_pretrained method"
"block sparse bert"
"TF Longformer has some graph compilation/execution issue"
"Data Loading as a Service"
"\"RuntimeError: Input, output and indices must be on the current device\" when trying to finetune MBart"
"Multiprocessing CUDA issues when importing transformers"
"Arrow file is too large when saving vector data"
"Possible bug in `train_batch_size`"
"MBart prepare_seq2seq_batch"
"Training of BART slow on TPU - aten ops investigation"
"XLNet evaluation on SQuAD"
"Blenderbot-3B config seems to be a little wrong"
"error while finetuning for Regression task."
"Training loss not getting logged"
"Loading a set of tokenized files for training"
"DeBERTa in TF (TFAutoModel): unrecognized configuration class"
"Jupyter Notebook Kernel crashes when tokenizing large dataset"
"Finetune mbart rouge score difference between training and evaluation part"
"Multi turn conversation with Blender Bot "
"How to implement seq2seq attention mask conviniently?"
"Custom train/validation file not supported in  run_qa.py"
"Excessive GPU-GPU communication with GPT2 making multi-GPU training slow?"
"Why does datasets get imported when running \"from transformers.models.roberta.tokenization_roberta_fast import RobertaTokenizerFast\""
"how to evaluate models on SUPER_GLUE benchmark?"
"How do I handle class imbalance for text data when using pretrained models like BERT?"
"[docs] TFRobertaModel example: last_hidden_states -> last_hidden_state"
"replacing apex.normalization.FusedLayerNorm with torch.nn.LayerNorm"
"BartModel's `past_key_values` seems to have different explanations in input_doc and output_doc"
"[Marian] Doc says `config.add_bias_logits=True`, but config is has `config.add_bias_logits=False`"
"Where is the impact when output_attentions=True?"
"Conditional Generation using input_embeds instead of input_ids"
"Similar usage of `past_key_values` in CausalLM and Seq2SeqLM"
"Model inputs and outputs are ``None`` when converting fine-tuned gpt2 to Tensorflow?"
"`run_glue.py` fails when using my own dataset of regression task"
"wrong output for Bert-larged-uncased"
"run_glue.py with XLNet model on CoLA dataset reaches 0 accuracy"
"CUDA runtime error during benchmarking"
"trainer.predict() returns different values from model.logits"
"How to use Longformer for summarization"
"Generate Function - Manual decoder_input_ids Error (Bart, Pegasus)"
"Retrieval Collapse when fine-tuning RAG"
"Unable to train xlnet with tensorflow"
"[autoformatters] wrapping destroying items/lists"
"`pip install -e .[dev]` in Python 3.9.1+ fails because `jaxlib==0.1.55` cannot be found"
"About Multi GPU"
"Why was DataCollatorForNextSentencePrediction removed ?"
"shift_tokens_right in BART, FSMT incompatible with DataCollatorForLanguageModelling"
"Transformer models for semantic parsing"
"[Announcement] Changing model type of Barthez"
"[utils/get_modified_files.py] fails with a few PR checkout tools"
"Is it possible to export a pytorch .pt file after finetuning a model?"
"Apache Hadoop (HDFS) File Loading from_pretrained"
"T5 base use a lot of memory to train on "
"RuntimeError: The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 1"
"Can't find pretrained model for TFPegasusForConditionalGeneration "
"Doc styling utils adds parasites new lines"
"Adding Stochastic Weight Averaging to transformer optimizers"
"[examples/text-classification] `do_predict` for the test set of local datasets"
"Loading fine-tuned models "
"urgent please help on memory issue during save "
"Cannot use TransfoXLLMHeadModel with Trainer class because it returns a non scalar loss"
"Some layers of pretrained Albert model albert-base-v2 didn't match the architecture of AlbertForMaskedLM in latest transfomers 4.1.1."
"Error when running run_clm.py on Python3.9/MacOS"
"Rename `nlp` variables into more appropriate names"
"[EncoderDecoder] Make sure `use_cache` is set to `True` for all Bert2Bert, Roberta2Roberta by default"
"[Blenderbot] Model yields weird results"
"Closed"
"Error while loading finetuned distilbert model: embedding dimension mismatch"
"FileNotFoundError when instantiating RagRetriever"
"UnboundLocalError when generating sequences"
"RuntimeError when running Reformer model"
"Unable to train sequence classification task using TFTrainer"
"Have RAG return generator cross-attentions when output_attentions=True"
"Cannot Evaluate While Training Using the Trainer"
"max_target length for question answering system"
"model.generate() has the same speed on CPU and GPU"
"[trainer] fractional epoch "
"request for run_text_classification.py"
"dataset not being sent to device when using Trainer (distributed)"
"ProphetNetNgramAttention: Number of attention heads"
"Using Huggingface library with DeepSpeed"
"Problems with using LongFormer"
"tf trainer dataset cardinality issue - potentially a bug"
"[make docs] please help make the validation process easier"
"Can not load a saved tokenizer using AutoTokenizer"
"Question on the example script run_glue.py for text classification"
"Question About Attention Score Computation & Intuition"
"RoBERTa tokenizer does not add start and end token at the beginning and end of the sentence"
"torch.nn.modules.module.ModuleAttributeError: 'RecursiveScriptModule' object has no attribute 'resize_token_embeddings'"
"Model previews not working for models that require MecabTokenizer"
"bug in distributed codes AssertionError: Default process group is not initialized"
"[Benchmark]onnx-export"
"config.json not found when loading fasttext-language-id model"
"Can't run T5 models because of missing protoc"
"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte "
"Model Hub hanging in model's loading"
"T2TDataCollator 'target_ids' key error"
"Converting T5 (text to text transfer transformer model) checkpoints to pytorch "
"Documentation's example script linked does no exist anymore"
"mBART is not saving (learned) position embeddings"
"Siamese Multi-depth Transformer-based Hierarchical Encoder"
"[BlenderbotSmallTokenizer] Cannot download tokenizer"
"Print All Tokens Over a Certain Probability Threshold: T5 "
"Data format for TFTrainer for TFGpt2"
"Seq2Seq include custom glossary/dictionary"
"xla_spawn.py crashes when training on TPU V3-32"
"Need clarification in /examples/research_projects/rag/use_own_knowledge_dataset.py"
"strange output of fast/slow tokenizers"
"BertForTokenClassificiation save"
"bounding by compute, retraining from the time the model is killed "
"Is the GPT-2 forward too different from Bert or RoBerta?"
"Generating sequence from two input sequences"
"RFC: ternary assignment style in transformers code revisited "
"Entity level F-1 scores in run_ner.py"
"Fine-tuning LMwithNSP"
"Quick tour runs into OOM on Colab "
"Dynamic padding + truncation in classification script"
"DPRReaderTokenizer does not generate the attention_mask properly"
"Where is convert_bert_original_tf_checkpoint_to_pytorch.py?"
"SMITH Google"
"tokenizer decode method"
"Adding Megatron models."
"finetune_trainer.py script is not using given config file"
"pegasus fine-tune: TypeError: shift_tokens_right() missing 1 required positional argument: 'decoder_start_token_id'"
"Tensorflow pretrained FlauBERT mixed precision error"
"How to train the models in smaller spochs"
"Multilingual MiniLM"
"Converting original BERT tf checkpoints to BertForMaskedLM"
"Pipeline - Truncation Keyword not Recognized  "
"Trainer is using DataParallel on parallelized models "
"Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized"
"BatchEncoding.to() throwing torch NameError in 4.2.0; identical code works in 4.1.1"
"A question about the weight decay"
"Custom mask when performing forward pass"
"How to fine-tune T5/Bart for other languages on summarization?"
"Longformer version of RoBERTa error"
"WARNING:tensorflow:AutoGraph"
"disable message \"Some layers from the model checkpoint at bert-base-cased were not used when initializing\""
"disable message \"Some layers from the model checkpoint ...\""
"Difference in decoded strings between a tokenizer and the corresponding fast tokenizer"
"why set masked_bias as -10000 in GPT2"
"Order of inputs (difference between doc and output)"
"[Model Exporting] How to export a fine tuned model to a single pytorch or tensorflow model file?"
"saving the model during run_mlm.py"
"saving the model during run_mlm"
"TypeError: on_init_end() got an unexpected keyword argument 'model'"
"TypeError: on_init_end() got an unexpected keyword argument 'model'"
"Mistake in the \"Summary of the tasks\" article"
"[DeepSpeed] Features to integrate / Optimizations to add / Experiments to do"
"[run_ner.py]You need to instantiate RobertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs"
"Convert ckpt from TFTrainer to huggingface format."
"Why do not use 'torch.nn.MultiheadAttention' to substitude 'Class BertSelfAttention+BertSelfOutput' for pytorch"
"Conditional branching logic in modeling_tf_xlnet.py causing error with TF Graph"
"Error in GPT2 while using gradient checkpointing."
"Text generation pipeline - output_scores parameter"
"Train robertatokenizer failed due to pad token not found"
"SQuAD 2.0 metric not supported"
"wandb breaks tests - importlib.util.find_spec-related under forked process"
"Weighted Loss in BertForTokenClassification"
"Passing in custom BartForConditionalGeneration model as generator to RagSequenceForGeneration"
"Issue with TrainingArguments docs."
"[Question] How to use threads for huggingface transformers"
"key error when use trainer to fine_tuning a dataset"
"ImportError: cannot import name 'Dataset'"
"Missing argument: decoder_head_mask for T5"
"Wrong offsets_mapping in T5TokenizerFast "
"Weights used for Masked LM predictions"
"key error when use trainer to fine_tuning a dataset "
"XLMRobertaTokenizerFast producing wrong tokenized output"
"ValueError: Expected floating point type, got <dtype: 'int32'> for TFGPT2LMHeadModel"
"Conditional branching logic in modeling_tf_flaubert.py causing errors with TF Graph"
"Multi-GPU inference with Tensorflow backend"
"[Feature Request] Add 3D attention mask for T5Model"
"Fail to convert the Funnel Transformer tensorflow version to transformer one when use the official script"
"Odd predictions of T5 models in recent versions "
"RAG : Adding end to end training for the retriever (both question encoder and doc encoder) "
"Training Bert2Bert with EncoderDecoderModel and Seq2SeqTrainer results with Cuda OOM"
"Easier perplexity computation"
"Does the latest huggingface-transformers version work with tokenizers==0.10.0?"
"Error w/Transformers 4.2.0 and TF Nightly"
"RAG Fine Tuning"
"AutoModelForMaskedLM not working when using MBartForConditionalGeneration architecture."
"BertTokenizer and encode_plus()"
"\"Converting Tensorflow Checkpoints\" document has wrong link in v4.2.0+"
"ModuleAttributeError occurs during Converting TensorFlow Checkpoints (BERT)"
"Tokenizstion"
"run_ner.py crashes when dev or test contain previously unseen labels"
"IndexError: index out of bounds when running run_mlm.py"
"Fine-tuning LM with NSP"
"Cannot compile tokenizers on PowerPC 9 while installing transformers"
"bert_tokenizer.decode(bert_tokenizer.encode(sentence))!=sentence"
"How to enable tokenizer padding option in feature extraction pipeline?"
"AttributeError: 'Seq2SeqTrainer' object has no attribute '_actual_model'"
"bert-base-cased predicts tokens instead of whole words after fine-tuning on fill-mask task"
"Visualize self-attention for GLUE task"
"Generating sentence embeddings from pretrained transformers model"
"BertGenerationDecoder .generate() issue during inference with PyTorch Lightning"
"Can't load previously built tokenizers"
"[Open in Colab] links not working in examples/README.md"
"MLM training for DeBERTa not supported: configuration class is missing"
" Is there a C++ interface?"
"input one model's output to another one"
"ModuleAttributeError: 'GPT2LMHeadModel' object has no attribute 'backward'"
"ModuleAttributeError: 'GPT2LMHeadModel' object has no attribute 'backward'"
"The model learns nothing after 3 epochs of training"
"Model Parallelism for DeBERTa"
"WANDB_DISABLED env variable not working as expected"
"NAN return from F.softmax function in pytorch implementation of BART self-attention"
"how to run pegasus finetune on multiple gpus"
"ProphetNetForCausalLM text generation fails"
"ValueError(\"The training dataset must have an asserted cardinality\") when running run_tf_ner.py"
"DeepSpeed: Exits with CUDA runtime error on A100 (requires recompiling DeepSpeed for NVIDIA 8.0 Arch)"
"Let Trainer provide the device to perform training"
"Add support for RemBERT"
"Slow BERT Tokenizer adds UNK when calling tokenize()"
"Error when passing --line_by_line to run_mlm.py"
"CUDA out of memory error on Trainer hyperparameter_search"
"T5 Model Parallelism in 4.3.0"
"convert_graph_to_onnx.convert broken for translation model facebook/wmt19-en-de"
"Run_ner.py falsely aligns prediction list"
"AutoModel doesn't work with DPRContextEncoder"
"ERROR about using layer_past and use_cache in Attention Layer of GPT2"
"Docs suggest to use discriminator weights for ElectraForMaskedLM instead of generator"
"Mismatch of the mask token id of BART between fairseq and huggingface"
"OSError: [Errno 116] Stale file handle"
"[fsmt] Exporting the operator triu to ONNX opset version 12 is not supported"
"Error using TFAutoModelForSequenceClassification with Tensorflow 2.2.0"
"RAG Model without DPR"
"--fp16 fine-tuning appears to be taking more memory (4.3.0). "
"DistilGPT2 extremely strange model behaviour"
"Wrong offsets mapping in XLMRobertaTokenizerFast"
"fine tune patrickvonplaten/longformer2roberta-cnn_dailymail-fp16 using LED updates"
"mT5 additional_special_tokens seems not work"
"Trainer object empties dataset"
"ValueError: Couldn't instantiate the backend tokenizer while loading model tokenizer"
"Improve PyTorch examples for FP16"
"named_parameters not showing embedding matrix of RobertaLMHead (more a question than a bug)"
"Improve the `run_xlni` example to use the Datasets library"
"Extra indicators for BPE for Unicode Characters "
"save tokenizer and model from fine tuned LED model"
"squad_v2 crashes during evaluation"
"index mismatch in \"offset_mapping\" with TokenizerFast and pre-tokenized input"
"[wip] [doc] Parallelism notes"
"saving best model only using modelcheckpoint keras"
"TFBartForConditionalGeneration with labels padded with -100 gives Nan loss."
"TF loss function output inconsistent with Pytorch one for multiple tasks"
"Add classes for Multi-label classification models? "
"RagRetriever question_hidden_states shape"
"New API for TensorFlow saved models not compatible with T5 and MarianMT"
"padding='max_length' allowing more than max length"
"Link Not Working"
"I want to train a BART model for conditional text generation. I want to train the encoder and the decoder separately for a specific task. Can anyone help with the code? I am new to this.  @patrickvonplaten"
"Calculating Confidence score for Question Answering Models"
"Translation Model in ONNX: Choosable Output Formats"
"GPT2 MNLI training using run_glue.py"
"Truncated Translations with mT5 model"
"Strange start token in MT5 generation"
"Add the ability to skip runtime version check"
"does LED use distributed training by default?"
"Conversion of Electra checkpoint from official repo TF (pretrained on custom dataset)"
"[trainer] a consistent way to limit the number of items"
"[trainer] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters"
"convert_graph_to_onnx.convert broken for model bart-large / wmt19-en-de"
"Finetuning ProphetNet with Seq2SeqTrainer fails."
"Can I use a smaller base model than allenai/led-base-16384 for LED?"
"`label_to_id` in `run_glue.py` seems to have a wrong `if` statement"
"Missing head_mask and decoder_head_mask arguments in encoder-decoder models"
"[trainer] renaming cl args/ trainer attributes to be clear per-gpu vs total"
"[wip] [doc] Performance and Scalability notes"
"I am trying to Fine tune on BartForConditionalGeneration but I end up getting all <pad_tokens>. Can you please help resolve it? "
"ImportError: cannot import name 'get_last_checkpoint'"
"Mixed Precision support and avoid \u2018\u5206\u54af\u554a\u592a\u2019"
"support Mixed Precision and avoid 'dtype=float32' in implementation"
"[docs] use `versionadded`, `versionchanged` and `deprecated` directive"
"logging_epochs argument for TrainingArguments"
"Multi-TPU training uses just 1 out of 8 cores."
"SQUAD Question Answering example:: RuntimeError: Could not infer dtype of NoneType"
"Some model use serve previous version can not do inference in web api."
"About max_length in generation_utils.py"
"Head masking and test_head_masking not working properly for TFT5 models."
"Padding tokens affect MobileBert output"
"AttributeError with T5Tokenizer"
"Add support for tf2 encoder_decoder"
"Longformer: raise TypeError(\"pred must not be a Python bool\", pred)"
"[trainer] seq2seq doesn't handle mt5 correctly"
"Whole word mask in run_mlm_wwm.py"
"where is position_embedding_type used"
"IndexError when finetuning barthez on summarization"
"Exception: You're trying to run a `Unigram` model but you're file was trained with a different algorithm"
"Strange hyperparameter warning"
"[DOCS] curl links go to 404 not found in NER tutorial"
"DeBERTa pretraining using MLM: model gradients become NAN "
"Some weights of {} were not initialized from the model checkpoint"
"examples/seq2seq , where can I find the  definition for the sortish_sampler argument?"
"Exporting model to onnx increases the model size"
"Finetune_Trainer Question"
"Conversion of BPE tokenizer for Marian models"
"[Quick poll] Give your opinion on the future of \ud83e\udd17 transformers: 40k edition!"
"Remove Token from Vocab?"
"Seeking clarification on T5  prefix for summarization"
"rfc: new benchmark tool"
"ImportError: cannot import name 'PreTrainedEncoderDecoder' from 'transformers' (unknown location)"
"TFGPT2LMHeadModel unknown location"
"Does Sortish Sampler work with multiple GPUs in seq2seq?"
"run_seq2seq.py doesn't work after enabling sortish sampler"
"Missing model license information"
"PPLM example - AttributeError issue"
"Tokenizer return offsets"
"Error \"Expected input batch_size (16) to match target batch_size (1440)\" in the WNUT NER example"
"[seq2seq] some logging for all processes in distributed mode"
"run_seq2seq.py : Why we pad labels with -100?"
"How to add more fields in TrainingArguments"
"Gradient accumulation and distributed parallelism will reduce the effect?"
"AttributeError: 'torch.Size' object has no attribute 'as_list'"
"prediction_step() is not using compute_loss()"
"RAG + DPR model performance issues"
"[doc] transformers.PreTrainedTokenizer.encode() doesn't get resolved to its doc"
"AttributeError: module 'torch.utils' has no attribute 'checkpoint' for fine tune LED"
"Would you like to add convert the generator script by ConvBert"
"Implementing ELECTRIC training for ELECTRA"
"Deploying a transformers pipeline into Google Cloud AI-Platform prediction"
"Missing None verification in the CLM language modeling example"
"Hyperparameter search w/ Optuna CUDA out of memory"
"Hyperparameter search w/ RayTune BrokenPipeError: [Errno 32] Broken pipe"
"[2D Parallelism] Tracking feasibility"
"Possible bug in `prepare_for_model` when using fast tokenizers"
"trainer_seq2seq.py Question"
"Can't import pipeline"
"Converting pretrained tf2 bert model to pytorch model for using FillMaskPipeline"
"[Good first issue] ALBERT PyTorch Integration tests"
"[Good first issue] ALBERT TensorFlow Integration tests"
"[Good first issue] BERT Generation PyTorch Integration tests"
"[Good first issue] DistilBERT PyTorch Integration tests"
"[Good first issue] ELECTRA PyTorch Integration tests"
"[Good first issue] FlauBERT PyTorch Integration tests"
"[Good first issue] LXMERT PyTorch Integration tests"
"[Good first issue] MPNet PyTorch Integration tests"
"[Good first issue] DistilBERT TensorFlow Integration tests"
"[Good first issue] LXMERT TensorFlow Integration tests"
"[Good first issue] MobileBERT TensorFlow Integration tests"
"[Good first issue] MPNet TensorFlow Integration tests"
"[mBART] one slow integration test is failing on master"
"tokenizer is slow when adding new tokens"
"Problem while initializing custom model with added tokens "
"How to resize RobertaLMHead with pretrained weights? "
"What is the correct way to use Adafactor?"
"Deepseep configs keys probelm"
"Model Save/Load Fails for Hadoop File Server"
"[trainer] new in pytorch: `torch.optim._multi_tensor` faster optimizers "
"Disk memory management"
"DebertaForSequenceClassification documents  examples  report RuntimeError: Index tensor must have the same number of dimensions as input tensor"
"Make use of attention_mask in Trainer's compute_metrics"
"Can't make sense of encoding for a downloadable AutoTokenizer"
"Loss function inputs for DistilBertForTokenClassification-like model using DistilBertModel"
"How to train on shards of bookcorpus + wikipedia + openwebtext on 1 TB disk."
"[documentation] non-PR doc editing"
"Adversarial/amnesic heads"
"\ud83d\ude80 Faster batch translation with FSMT model"
"[DeepSpeed] [success] trained t5-11b on 1x 40GB gpu"
"German DistilBertModel raises an issue"
"Converting wav2vec2-base-960h to ONNX report an error while converting"
"[License info] Longformer SQuAD finetuned model"
"run_ner.py raised error"
"[models] why aren't .bin files compressed for faster download?"
"Why two separators?"
"Problem fine-tuning BERTweet"
"OOM when trying to fine tune patrickvonplaten/led-large-16384-pubmed"
"return_dict scores are inconsistent between sampling and beam search"
"[Question] Pipeline QA start_index"
"Do not allow fine tuning with sequence size larger than during training"
"Feature-extraction pipeline to return Tensor"
"python utils/check_repo.py fails"
"Tokenizer Batch decoding  of predictions obtained from model.generate in t5"
"Protobuf"
"Accessing language modeling script checkpoint model and tokenizer for finetuning"
"Datasets library not suitable for huge text datasets"
"T5 doubling training time per iteration from save_steps to save_steps (1st 100 steps 33s/it - then, 75s/it)"
"custom JSON data breaks run_seq2seq.py"
"Override Default Params on QnA Pipeline"
"AttributeError: module 'transformers' has no attribute 'PegasusForCausalLM'"
"generation length always equal to 20 when using run_seq2seq.py script"
"Cannot import DataCollatorForSeq2Seq from Transformers library"
"[s2s examples] convert existing scripts to run_seq2seq.py from finetune_trainer.py"
"[examples s2s] run_seq2seq.py tweaks"
"seq2seq: fail gracefully when predicting using --device cpu and --fp16"
"Pegasus ONNX format?"
"[s2s examples] dataset porting"
"BertGenerationTokenizer provides an unexpected value for BertGenerationModel"
"Can you give some suggestion about add features with input_ids to token-classification model ?"
"ImportError: cannot import name 'list_datasets'"
"Installing tf2.0 in my env but still get ImportError in my code"
"run_ner.py fails when loading a model/checkpoint from a directory"
"[example] run_ner.py raised error: IndexError: Target 3 is out of bounds."
"Error: \"Transformers CLI tool: error: unrecognized arguments: kvantorium-small\"  while deploying machine learning model to hugging face profile"
"Cannnot train Roberta: 2 different errors"
"When encoding text to feature vectors - Would be awesome to be able to use the simplest tokenizer with a split on spaces"
"Dimension error while finetuning longformer with roberta-large EncoderDecoderModel "
"Model templates tests are run twice"
"\"Connection error, and we cannot find the requested files in the cached path.\" ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
"Integrating GPT-2 model with Web page"
"AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'new_ones'"
"assertion failed: [predictions must be >= 0]"
"[tests] where to put deepspeed + fairscale tests"
"Unclear error \"NotImplementedError:  \"while saving tokenizer. How fix it?"
"pipeline(\"sentiment-analysis') - index out of range in self"
"Supporting truncation from both ends of the sequence in BertTokenizerFast"
"model.generate needs BART config update"
"Tapas not working with tables exceeding token limit"
"Language modelling head has zero weights in pretrained TFMobileBertForMaskedLM"
"[question] Are the tensorflow bert weights same as the original repo ?"
"How to run distributed training on multiple machines?"
"Pre-Training for Question Generation"
"DeBERTa v2 throws \"TypeError: stat: path should be string...\", v1 not"
"Issue training Longformer "
"PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration"
"Non-JSON-serializable tokenizer config with `save_pretrained` "
"Git does not find the model folder and does not commit model files in the hugging face"
"Bug in RAG Sequence generate "
"Possible bug in RAG Tokenizer"
"CUDA Out of Memory After Several Epochs"
"Exporting transformers models in ONNX format"
"Allow `do_lower_case=True` for any tokenizer"
"Help on training TFBERT to IntegerEncoded sequences"
"Converted pytorch model to onnx does not work correctly"
"XLM-R tokenizer is none"
"Bug in numpy_pad_and_concatenate"
"Trainer Evaluates at every step"
"Where the helsinki models downloaded to? when using the pretrained models"
"cant install from source"
"Adding end-to-end retriever training to RAG with RAY implementation. "
"Text to Speech Generalized End-To-End Loss for Speaker Verification, Real Time Voice Cloning"
"Back Translation"
"ValueError: `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`), got GPT2LMHeadModel"
"Direct way to apply different learning rate for different group of parameters in Trainer."
"T5 GPU Runtime Degradation"
"context manager for seeding, or generating fixed random tensor."
"T5 Base length of Tokenizer not equal config vocab_size "
"Model not training beyond 1st epoch"
"BERT with regression head cannot fit one datapoint"
"Issue using num_beams parameter for T5 / DeepSpeed"
"Problem with evaluation_strategy"
"Model Parallelism for Bert Models"
"rfc: integration tests need non-example application for testing"
"Multiple Mask support in Pipeline"
"past_key_values tuple index out of range error when using text2text-generation pipeline with encoder-decoder model"
"Seq2seq now has larger memory requirements, OOM w/Deepspeed on previously runnable models "
"Increasing gradient accummulation steps significantly slows down training"
"[example scripts] disambiguate language specification API"
"[example scripts] inconsistency around eval vs val"
"[tests] failing test only when run in a group"
"NER pipeline doesn't work for a list of sequences"
"run_langauge_modeling for T5"
"T5 training with Keras: InvalidArgumentError:  logits and labels must have the same first dimension"
"Saving PruneBERT notebook fails to run on torch > 1.5"
"What does the \"<s> token\" mean in Longformer's global_attention_mask?"
"How to train an MBart model from scratch for a new language pair? "
"Conditional generation with T5"
"Loading a model from local files achieves way too lower accuracy in comparison to model downloading"
"Why is the attention_mask added to the attn_weights instead of multiplying/masking?"
"ONNX Export for Fine-Tuned DistilBertForTokenClassification"
"Inconsistent loss computation?"
"`super()` does not have `prepare_seq2seq_batch()`  in `transformers/models/rag/tokenization_rag.py` "
"Saving HF wrapped in Keras"
"Support for DeBERTa V2 models"
"Failing Multi-GPU trainer test"
"0% GPU usage when using `hyperparameter_search`"
"run_mlm.py not utilizing TPU"
"Make use of our copy-consistency script for task-specific models"
"Uploaded a new model but is not found on the hub."
"Fine-tuning Seq2Seq models for Machine translation"
"ONNX Export - cannot resolve operator 'Shape' with opsets: ai.onnx v11"
"StopIteration error happened"
"Better Fine-Tuning by Reducing Representational Collapse"
"Fast Tokenizers instantiated via vocab/merge files do not respect skip_special_tokens=True"
"1.3GB dataset creates over 107GB of cache file!"
"Tokenizer is working different from expected functionality. "
"different behavior for get_input_embeddings()  between 4.2.x  and 4.3.x in Tensorflow"
"QA Documentation: I got error just copy and pasting documentation"
"RuntimeError: Overflow when unpacking long"
"StopIteration Error when running beam search for squad 2.0"
"discrepancy between the Huggingface T5Tokenizer and the original T5tokenizer"
"T5 relative attention bias: Discrepancy to original implementation"
"Slow Multi-GPU DDP training with run_clm.py and GPT2"
"No module named 'tasks'"
"Trainer.train() is stuck "
"Showing individual token and corresponding score during beam search"
"Converting original T5 to be used in Transformers"
"Wav2Vec2 finetune"
"Multilabel Sequence Classification in trainer"
"Request to add Switch Transformer"
"ConvBert not compatible with torch v1.6"
"Question about (no_decay = ['bias', 'LayerNorm.weight']) in BERT(Transformer-based) "
"CUDA memory error on increasing the number of generations"
"Upgrade transformers from 3.5.0 to 4.3.2 instance error"
"`compute_metrics` show better results than `generate` because target data leaks"
"TensorFlow Question-Answering example fails to run (cardinality error)"
"[BUG] [Ray-Tune] ValueError: checkpoint not in list"
"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract not available for tensorflow"
"Load custom models"
"ImportError: cannot import name 'MBart50TokenizerFast' from 'transformers' (unknown location)"
"[Question]: Register new Tokenizer"
"Deberta Tokenizer convert_ids_to_tokens() is not giving expected results"
"NER label re-alignment always expects B labelled first sub-words"
"Tapas Tokenizer  makes DataFrame iterrows() iterator crazy ..."
"Language Modeling Task (GPT2 / CLM) Does Not Generate Line Breaks?"
"Summarization of long text with T5 seems to output random memory content"
"ElectraForQuestionAnswering with SQuADHead"
"ImportError: cannot import name 'pipeline' from 'transformers' (unknown location)"
"Improving training time for Marian MT model with the Trainer "
"Performance of mbart-large-50-many-to-many-mmt on de/fr/it"
"Trainer.train argument resume_from_last_checkpoint"
"[tests] tests/test_trainer_distributed.py intermittent failure"
"Random Word Replacement Probability"
"Minor documentation issue"
"Masking issues with GPT2LMHeadModel.generate()"
"Trainer train continues after resume_from_checkpoint on a checkpoint with early stop"
"[examples s2s] AttributeError: 'MBartTokenizerFast' object has no attribute 'tgt_lang'"
"[pretrained] model classes aren't checking the arch of the pretrained model it loads"
"Marian input decoding bug"
"[predict] AttributeError: 'Seq2SeqTrainer' object has no attribute 'metrics_format'"
"AutoTokenizer from pretrained BERT throws TypeError when encoding certain input"
"Converting fairseq NMT to transformers misses model weight"
"Object of type 'int64' is not JSON serializable in Trainer.save_checkpoint"
"unexpected keyword argument 'forced_bos_token_id' when using mbart-large-50-many-to-many-mmt"
"Tensorflow not found but i can import it"
"convert_tokens_to_string documentation bug"
"Documentation of the decode method is missing"
"Issue Loading bert-based-german-cased"
"pretraining objective of T5 model "
"[Example] Using label_smoothing_factor raise error when evaluating model"
"Matrix multiplication error for ReformerModelWithLMHead when tie_word_embeddings is True"
"LayoutLM Tensorflow model"
"ValueError: too many values to unpack (expected 2)"
"Huggingface mt5 does not reach the performance of original mt5 on paws-x "
"ForTokenClassification head on BART"
"Guidance for continued pre-training of BART with de-noising."
"[Question] Add a new token to tokenizer and bart model"
"BERT for speech"
"[Tensor Parallelism] Megatron-LM to transformers"
"Input mismatch with TFDistilBert training from scratch inspite of cross checking input dimensions"
"[DeepSpeed] unable to increase batch size from 4 for T5-3b with 2x 32GB V100 GPUs"
"mBART 50 models not found in model shortcut name list"
"Raise an error instead of a warning when model files are not loaded correctly"
"[DeepSpeed] strange learning rate schedule in linear_schedule_with_warmup"
"bug in bert pretraining "
"Return cross-attention weights in generation function"
"[Benchmark] Converting a QA distilbert model to onnx - the f1 score plummet"
"[trainer] port metrics logging and saving methods to all example scripts"
"Problem with GPT2/DistilGPT2 prediction - dimension mismatch"
"tokenizer.Tokenizer compatibility with Inference API or Auto* classes"
"Translate English into Japanese using mbart"
"DialoGPT tokenizer config issue"
"Where can we find the `RAG` implementation?"
"MarianMT - ONNX only accepts fixed input despite setting dynamic axes"
"Custom tokenizer with run_mlm script"
"[Benchmark]"
"BertForMaskedLM cannot be initialized from BERT checkpoints"
"Padding of bbox input in LayoutLM"
"Got \"RuntimeError: CUDA error: device-side assert triggered\" with Seq2SeqTrainer"
"Can every line in the input CSV file contain more than one sentence when pertraining BERT for MLM Loss?"
"ProphetNet Positional Embeddings Index Issue"
"Fine-tuning bart-base on XSum and got 34.0 as ROUGE1 (40.61 with higher lr)"
"BART Summarization : Torchscript Export / Inference Triton Server"
"Security Bug found - looking for contact for responsible disclosure"
"Rag Use Your Knowledge dataset"
"denoising objective for pretraining"
"Loading mBART Large 50 MMT (many-to-many) is slow"
"Knowledge Retrieval missing from BlenderBot Implementation"
"can't allocate memory error with wav2vec2"
"device-side assert triggered Error while doing inference on Distilbert and Bert"
"TFMarianModel from_pretrained can't load weights"
"Why should `attn_weights` be reshaped twice in BartAttention ?"
"ReformerForQuestionAnswering : int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
"Load pretrained model except the head layer for a specific downstream task"
"deprecated reference `tokenizer.max_len` in glue.py (PR #10220)"
"[Documentation issue] Sequence to sequence models"
"DPR decode_best_spans include spans from title"
"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte"
"Training LongformerForQuestionAnswering on TriviaQA"
"AttributeError: 'QAModel' object has no attribute 'automatic_optimization'"
"[firewalled env] OFFLINE mode"
"Trainer.train() gets stuck when executed on K8 pods"
"Option to output \"test predictions\" text file with each checkpoint in run_seq2seq.py"
"Fine-tune pretrained Wav2Vec2 on a small custom dataset "
"DDP performing slightly worse in terms of loss and metrics than DP"
"MNLI evaluation on pretrained models"
"loss.backward() TypeError seed issue for pretrained reformer model"
"Tokenizer not working"
"some bugs about mbart50 for spanish"
"NER Pipeline not working"
"DeepSpeedEngine object has no attribute 'no_sync'"
"RobertaTokenizerFast does not add special tokens"
" how to freeze specific layers of TFbert model and just train a classifier? "
"Does the synonym replacement tasks need Transformer?"
"[Deepspeed] getting multiple prints of: Avoid using `tokenizers` before the fork if possible"
"SageMaker Model Parallel: cluttered tensorboard plots"
"Model Hub: Search by model size"
"Problem running T5 (configuration) with text classification"
"Question about the `decoder_input_ids`  in `LEDForConditionalGeneration` forward method"
"Problem using add_special_tokens"
"Trainer: Make `best_model_checkpoint` path in `trainer_state.json` relative to `args.output_dir`"
"Bug when combining grouped beam search and constrained prefix decoding"
"Slow evaluation using Trainer with TPUs in Colab"
"Unable to convert Facebook/mbart-many-to-many model to onxx"
"[examples] add --max_train_samples  --max_val_samples  --max_test_samples cl args to all scripts"
"RAG and retrieved documents"
"Trainer's load_best_model_at_end argument results in error with DistributedDataParallel"
"Inference with Finetuned BERT Model outputting odd results"
"Adding Longformer Encoder Decoder support for T5"
"About the speed when return_dict is set to True"
"TF Dataset Pipeline throws `RuntimeError: Already borrowed` when tokenizing"
"Confused about the time of forword"
"[Trainer] add --max_train_samples  --max_val_samples  --max_test_samples "
"Setting max_length for model training produces error"
"Option to output \"test_preds_seq2seq.txt\" text file with each checkpoint generated in \"run_seq2seq.py\""
"TypeError: __init__() got an unexpected keyword argument 'model' in `run_seq2seq.py` example when using on our own files"
"Bug in Electra Example"
"TypeError: can only concatenate str (not \"int\") to str"
"AttributeError: 'Trainer' object has no attribute 'log_metrics'"
"changing the way checkpoint is done in the new release "
"When I try to import my model I run into an error \"TypeError: PyMetaspace.__new__() got an unexpected keyword argument: str_rep\""
"pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed."
"OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5'] When I try to use my model"
"BART for generating sequence of length more than 1024 tokens"
"BartForConditionalGeneration breaks with label smoothing loss"
"pytorch Albert quantization error"
"How can I make the logging utils log to a file as well? "
"How to Improve inference time of facebook/mbart many to many model?"
"[Wav2Vec2] Improve SpecAugment function by converting numpy based function to pytorch based function"
"How to Reduce the inference time of Facebook/many to many model?"
"Script for squad_v2 for custom data not working"
"Tflite conversion error for TFMT5ForConditionalGeneration model "
"modeling files loaded when they aren't being asked to be loaded"
"run_ner.py training data file format"
"The described function in docs was not implemented in source code"
"(Sorry I can not visit the forum) BORT question: pre-training-using-knowledge-distillation is better than pre-training-only for downstream tasks?"
"Question: change location of cache datasets"
"Needed a feature to convert facebook mbart many - many model to ONNX runtime inorder to reduce the inference time"
"Issue with converting my own BERT TF2 checkpoint to PyTorch and loading the converted PyTorch checkpoint for training"
"Continue pre-training using the example code \"run_mlm.py\""
"The size of CoNLL-2003 is not consistant with the official release."
"Facing NCCL error on Multi-GPU training(on single machine) using run_glue.py script"
"generate() decoder_input_ids padding"
"Question regarding training of BartForConditionalGeneration"
"Different result in AutoModelForCausalLM"
"[examples] should all examples support the predict stage?"
"Corrupted Relative Attention in T5 Decoder"
"Constrained decoding?"
"Trainer not logging to WandB in SageMaker"
"Pipeline's QnA and run_qa predictions do not match"
"ONNX Training for Transformers"
"Model Weights Fail to Load from Pre-Trained Model when Using `tf.name_scope` "
"Albert quantized"
"Wav2Vec fine code"
"DeBERTa Fast Tokenizer"
"f\"The model '{self.model.__class__.__name__}' is not supported for {self.task}. Supported models are {supported_models}\","
"Fine tune of speaker embeddings model"
"GLUE benchmark crashes with MNLI and STSB"
"Fine tuning a pipeline"
"'Trainer' object has no attribute 'log_metrics'"
"Loading tapas model into pipeline from directory gives different result"
"Error in run_squad.py with BartForQuestionAnswering model"
"Why the positional embeddings in bert are not inplemented by sin/cos as the original paper said? Are these embeddings trainable?"
"Dynamic batch size for Seq2SeqTrainer"
"Bug in Hosted inference API"
"Converting models for tensoflowjs (node)"
"Adding option to truncation from beginning instead of end, for both longest_first and longest_second"
"Unable to translate Arabic to many other languages in MBart-50"
"Inconsistent API output for Q&A models between eager mode and torchscripted "
"BERT as encoder - position ids"
"fine-tune Pegasus with xsum using Colab but generation results have no difference"
"Different vocab_size between model and tokenizer of mT5"
"Typo in deberta_v2/__init__.py"
"Test/Predict on summarization task"
"Calling Inference API returns input text"
"RAG with RAY workers keep repetitive copies  of knowledge base as  .nfs files until the process is done. "
"tensorflow model convert onnx"
"Transfomer-xl  padding token"
"Wave2vec custom training tokenizer bug"
"\ud83d\udc1b Bug in attention head mask for cross-attention module in encoder-decoder models"
"Facing Issue while running `run_tf_multiple_choice.py` from examples"
"OSError: Can't load weights for 'facebook/mbart-large-cc25' when using TFMBartModel"
"Similar issue like #1091 in Blenderbot"
"Dead link to optuna.create_study under hyperparamter_search in Trainer"
"How to get best model from hyperparameter search easily"
"[RAG] Expected RAG output after fine tuning"
"Dear developer, does transformers have the support to translate Chinese text into English?"
"[website] installation doc blues"
"[examples] run_glue_deebert.py distrbuted fails"
"I have trained Bert on my own data which has been converted to IDs by using BertForMaskedLM, but when I use the model for the further fine-tuned, I found this error "
"[Causal Language Modeling] seems not as expected"
"Mismatch between input and target batch_sizes while training FSMT model"
"from_pretrained() - some model weights not initialized message"
"XLSR-53"
"Advice on creating/wrapping `PreTrainedModel` to be compatible with the codebase?"
"Import error for class Speech2TextProcessor, Speech2TextTransformerForConditionalGeneration"
"The dimension of Feature extraction"
"bug in run_finetune"
"Movement pruning for DistilGPT2 - pre_trained model, issue while using dynamic_quantization"
"seq2seq example with T5 does not run due to issue with loading tokernizer"
"Why HFArgumentParser.parse_dict(TrainerArguments) return tuple instead of dict?"
"request about deepspeed tutorial "
"Issue when customizing loss in Trainer"
"Can't reproduce xlm-roberta-large finetuned result on XNLI"
"Small question about BertForMaskedLM usage on TF model"
"CUBLAS_STATUS_INTERNAL_ERROR at examples/question-answering/run_qa.py"
"No model card for  roberta-large-finetuned-wsc"
"AlbertForSequenceClassification random output"
"Can't load config for hosted model, works when downloaded"
"SortedDL for contiguous LM"
"Implementing efficient self attention in T5"
"OOM issues with save_pretrained models"
"Not able to convert T5 tf checkpoints"
"changing \".view()\" to \".reshape()\" for pytorch"
"Request: Ignore Dataset transforms when iterating to the most recent checkpoint when resuming training"
"Run_qa  crashes because of parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))  "
"wav2vec2: `convert_tokens_to_string` contracts legitimately repeated characters"
"MNLI eval/test dataset is not being preprocessed in `run_glue.py`"
"wav2vec2: adding single-char tokens to tokenizer causes tokenization mistakes"
"Invalid pytorch_model.bin for TAPAS-large"
"Model \"deberta-v2--xxlarge-mnli\" doesn't work!!!"
"Average checkpoints"
"considering `pad_to_multiple_of` for run_mlm.py"
"expanduser path in Trainer"
"Using `label` in Trainer leads to TypeError"
"I get different results everytime I run run_squad.py"
"Help using Speech2Text"
"Issues with Multi-GPU"
"Support Quantization Aware Fine-tuning in all models (pytorch)"
"Nonetype when using deepspeed "
"Unable to reduce time in summarization!"
"Unable To Load Pretrained Longformer Models' Tokenizers"
"Space token cannot be add when is_split_into_words = True"
"export T5 model to onnx with past_key_values"
"seq2seq BertGeneration model failed \"ValueError: You have to specify either input_ids or inputs_embeds\""
"[Question] How do I prevent a lack of VRAM halfway through training a (Pegasus) model?"
"DistilBertTokenizerFast ignores \"do_lower_case=False\" parameter"
"Infernal tokenizer loading trained "
"Allow private model hosting and resolution "
"MarianMT - tokenizer.supported_language_codes -> 'NoneType' object has no attribute 'supported_language_codes'"
"How to use deepspeed finetune RAG?"
"training LayouttLM 1 epoch in distributed more results in error"
"[trainer] loss = NaN with label_smoothing and full-fp16 eval"
"Reformer _pad_to_mult_of_chunk_length seems incorrect"
"Improve the speed of adding tokens from added_tokens.json"
"hf_argparser doesn't set the required flag on non-defaulted enums"
"T5-base out of memory on one 2080 GPU with batchsize 4, sequence length 100"
"[TFMarian] Slow integration tests are failing"
"Token Classification: How to tokenize and align labels with overflow and stride?"
"Question answering: a couple of things after fine-tuning a model"
"Naming convention for (pytorch) checkpoints broken?"
"mBART Large-50 MMT provides incorrect translation when the source and target language are the same"
"OSerror, when loading 'wav2vec2-large-xlsr-53' Model of Wav2vec2"
"Add `cross_attentions` to the output of TensorFlow encoder-decoder models"
"Trying to implement \"nielsr/luke-large\" gives \"KeyError: 'luke'\""
"Seq2Seq Model with PreTrained BERT Model is Throwing Error During Training: ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")"
"Performance Issue in doing inferencing hugging face models"
"How to generate texts in huggingface in a batch way?"
"Please provide format of the dataset to finetuning wav2vec using run_asr.py script"
"Trainer crashes when saving checkpoint"
"Inheriting from BartForConditionalGeneration into a new class - weight  not initializing"
"ValueError: Unsupported value type BatchEncoding returned by IteratorSpec._serialize"
"'Trainer' object has no attribute 'log_metrics'"
"Is any possible with pipeline for using local model?"
"Pegasus-Large Question"
"Language model for wav2vec2.0 decoding"
"How can I get the exact position von answers?"
"Cannot use custom roberta tokenizer with run_mlm_wwm.py"
"Run Time Error: RuntimeError: Expected hidden[0] size (2, 1, 512), got [2, 128, 512] - Seq2Seq Model with PreTrained BERT Model"
"iterative evaluation in Trainer to save memory"
"Train tokenizer for Deberta"
"broken models on the hub"
"Multi-node training with the latest transformers/examples code"
"Stacked Roberta run_mlm.py"
"Fix log message for training from checkpoint with global step"
"run_clm.py does not work with any other block_size other than 1024"
"[examples run_summarization.py] t5 worse score w/ --source_prefix \"summarize: \" than w/o"
"Position ids in RoBERTa"
"`group_texts` duplicates special tokens"
"load wav2vec model from local path"
"Tokenizer becomes very slow after adding new tokens"
"BigBird"
"DialoGPT- cannot increase number of conversation turns"
"Issues with MODEL_FOR_MASKED_LM_MAPPING.keys(), and transformer.utils.check_min_version()"
"bug in new version 4.4.0 sentencepiece is not available"
"Tensorflow Keras model.loads_weights() breaks on TFElectraModel trained with v4.3.0"
"run_clm.py gpt-2 training example in documentation runs out of memory on a 32gb v100, should be verified and/or modified "
"Online decoding for ASR"
"Google Colab TypeError: expected str, bytes or os.PathLike object, not NoneType"
"BERT for Regression predicts constant"
"Even slower when using multiple gpus with sharded_ddp"
"AlbertForMaskedLM always has bad results"
"TokenClassificationPipeline: ignoring subwords"
"TokenClassificationPipeline: top-k predictions"
"Cannot import name swish from transformers.activations"
"auto model encodings for a text snippet returns different floating values across different batch sizes"
"Bug in multi-gpu training setting max_iters "
"TAPAS for Question Generation"
"Differences between S2T and Wav2Vec2"
"torch.nn.modules.module.ModuleAttributeError: 'AlbertEmbeddings' object has no attribute 'bias'"
"EncoderDecoderModel with different model dimensions"
"How to interpret fine-tuned model results and use model"
"Typo in M2M100 model page"
"Can DeepSpeed ZeRO-3 be applied for training? "
"TypeError: __init__() got an unexpected keyword argument 'filepath' when using RAG model"
"[Deepspeed ZeRO-3] Broken model save on fresh Transformers branch"
"HerbertTokenizer doesn't work on version 3.5.1"
"run_summarization script breaks with label_smoothing_factor and pad_to_max_length true"
"Pretrained XLNetTokenizer not returning tokenizer"
"Truncated words on GPT-2 output"
"How to get a probability for the result of t5_tokenizer.decode(output,...)?"
"How much vRAM should I have for fine tuning DeBERTa v2 xxlarge?"
"Initializing ddp is extremely slow when finetuning RAG"
"ONNX export outputs many warnings"
"I am finetuning mBART for summarization using finetune_trainer.py on custom dataset, but I keep getting this error. "
"handle_impossible_answer not working in the question answering pipeline for ROBERTa model"
"Domain adaptation "
"Example code for ReformerForMaskedLM"
"[trainer] figuring out why eval with `--fp16_full_eval` is 25% slower"
"mt5 getting nans with fp16"
"JSONLINES support on examples/seq2seq/run_translation.py "
"checkpoint breaks with deepspeed "
"Running \"convert_graph_to_onnx.py\" doesn't work."
"ReformerEmbedding unclear behavior"
"Log continuously models with wandb"
"getting nans with t5-large + fix"
"Encoder Decoder Model didn't return a reasonable result"
"run_mlm.py: CUDA error: device-side assert triggered, THCTensorIndex"
"weird large memory usage of mbert model "
"Local Attention for GPT2"
"Issues finetuning MBART 50 many to many"
"Generating text with MBart Large 50 on GPU with Tensorflow is significantly slower than with Pytorch"
"pegasus-xsum summarized a story of Eiffel Tower into one on the World Trade Center"
"Can\u2019t download the pre-trained pegasus-large model"
"why My Albert pretrain loss can't decrease?"
"issue of run_mlm.py "
"How to fine-tune RAG on MS-MARCO dataset?"
"Is there a `DataCollator` cat mask n-gram words for LM?"
"Add GPT-Neo"
"Option to change loss function for fine tuning"
"How to train encoder decoder for explicit negation generation"
"Small inconsistency in tokenization_utils for special tokens retrieval"
"Longformer training : CUDA error: device-side assert triggered"
"Error building extension 'fused_adam'"
"m2m_100 finetuning not working (KeyError: none)"
"If run trainer._maybe_log_save_evaluate() twice continuously, it will appear \u201cZeroDivisionError: float division by zero\u201d"
"The exact English pretraining data and Chinese pretraining data that are exact same to the BERT paper's pretraining data."
"transformers import error"
"Camembert-base MaskedLM has different config settings that actual camambert-base"
"not created config.json in Wav2Vec2ForCTC for ASR"
"Training GPT2 does not use GPU"
"Wav2Vec2/XLRS-Wav2Vec2 Pre-Training"
"transformers.models.auto.tokenization_auto"
"`XLMRobertaTokenizer` `encode_plus` api producing `<unk>` for a valid token"
"RuntimeError: while running run_common_voice.py (XLSR wav2vec finetuning week)"
"Scheduler Not Pickleable"
"MlFlow log artefacts"
"AttributeError: 'RobertaConfig' object has no attribute 'attn_type'"
"Wav2vec2 Training Loss not decreasing"
"Memory accumulates when training in a loop"
"Error Loading a Hub Model (Multilingual-MiniLM)"
"ImportError: cannot import name 'BertLayerNorm' when upgrading to latest transformers"
"[trainer] large scale models support"
" Invalid argument:  Incompatible shapes: [24,1536,12,514] vs. [24,1536,12,513]"
"save only the best performing checkpoint"
"[doc] Custom datasets page reference dataset library as NLP library"
"Getting a model to work on a system with no internet access"
"Error with detecting cached files when running without Internet connection (related to #10067)"
"Exception: cannot import name 'Regex' from 'tokenizers' "
"Improve the documentation for TrainingArguments.label_names, and if possible raise an error if users misinterpret this attribute like I did"
"LengthGroupedSampler slowly iterates over dataset"
"Summarization length not controlled by max_length, min_length"
"pegasus xsum won't train on xsum dataset"
"AttributeError: 'Trainer' object has no attribute 'log_metrics'"
" longformer speed compared to bert model"
"OSError: file bert-base-uncased/config.json not found"
"GPT2 on TPU, training is so slow."
"Tokenizer is adding ## to every word from the second."
"Use reformer in down stream task meet problem"
"/pytorch/xla/torch_xla/csrc/helpers.h:100 : Check failed: scalar_value.isIntegral()"
"Models not able to run when packed with PyInstaller"
"Typo in examples/text-classification README"
"Add Pooler to DistilBERT"
"Training with DeepSpeed takes more GPU memory than without DeepSpeed"
"Error while predicting on single sentence for token classification task"
"Another way to express masked_index = torch.nonzero(input_ids == self.tokenizer.mask_token_id, as_tuple=False)"
"Can't download the facebook/bart-large-mnli tensorflow model"
"Add DALL-E: Zero-Shot Text-to-Image Generation"
"saving pretrained models that were obtained from another model"
"Addition of  SequenceClassification config specific documentation to XModelForSequenceClassification."
"Wav2Vec2CTCTokenizer does not take the vocabulary into account when identifying tokens in a sentence"
"Converting marian tatoeba models"
"Please implement DUMA: Reading Comprehension with Transposition Thinking"
"Are there memory leaks when using DeepSpeed on training T5?"
"Save model error: list index out of range after pass input_processing call"
"[MarianMTModel] 'list' object has no attribute 'size'"
"How to freeze Camembert model for Classification tasks? "
"[Trainer] possible DDP memory regression"
"Input gets lost when converting mBART decoder to onnx"
"check_version not valid"
"Returning Confidence Score For Extractive QA Task When Using Non-Pipeline Approach "
"What is the score of trainer.predict()?"
"Supporting `config_path` for `AutoModel`"
"compute perplexity using a custom metric function"
"pkg_resources' working_set caching breaks transformers import on google colab"
"Gradient checkpointing in Wav2Vec2"
"Reproducing DistilRoBERTa"
"Transformers QA Online Demo is not working"
"Add GPT Neo models to Write With Transformer"
"Tagged Model Version Not Working"
"FineTune XLSR-Wav2Vec2 on New Langauge WER still 1 "
"AttributeError due to multi-processing using PyTorchBenchmark"
"BART : Cannot run trainer.evaluate() after trainer.train()"
"unable to use multiple GPUs with HF integration of DeepSpeed on Jupyter notebooks"
"Can't find ibert-roberta-base model"
"GPT Neo, Print Most Probable Next Word: String Indices Must Be Integers"
"Get following error with EncoderDecoder model: TypeError: forward() got an unexpected keyword argument 'use_cache'"
"ROUGE Multiple References"
"KeyError: 'gpt_neo' with EleutherAI/gpt-neo-1.3B"
"conda install transformers (not working) behaving differently from pip install transformers (working) for CentOS 7.9"
"Getting `raise NotImplementedError` for base_model.get_input_embeddings() when upgrading from pytorch-transformers"
"ReduceLROnPlateau-like functionality?"
"\"Converting Tensorflow Checkpoints\" meets  ('Pointer shape torch.Size([312]) and array shape (128,) mismatched', torch.Size([312]), (128,))"
"about .py file"
"error: fine-tunes language model with added_tokens"
"How to load weights from a private server?"
"run_seq2seq.py meet bug in using huggingface datasets billsum"
"a memory leak in evaluation"
"use `BaseModelOutput` as common interface for all different `BaseModelOutputWith*`?"
"OSError: Can't load config for '/content/wav2vec2-large-xlsr-asr-demo'. Make sure that:"
"Add new CANINE model"
"Cannot run the gpt neo 2.7B example"
"T5 documentation for computing pretraining loss seems to have a mistake "
"Enable multiple `eval_dataset` in `Trainer` API"
"Trainer API crashes GPUs"
"Module Not found: datasets_modules.datasets.output"
"cannot import name 'AutoModelForSequenceClassification' from 'transformers'"
"Strange ValueError with GPT-2"
"Fine Tune GPT-NEO 2.7B"
"pipeline.from_pretrained"
"Roberta and XLNet sentence pair training example"
"How to get masked word prediction for other languages"
"RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3"
"GPT-2 example is broken?"
"404 Client Error: Not Found for url: https://huggingface.co/%5CHuggingface-Sentiment-Pipeline/resolve/main/config.json"
"BertForTokenClassification class ignores long tokens when making predictions"
"Was bert-large-uncased-whole-word-masking-finetuned-squad fine tuned or not."
"DeBERTa xlarge v2 throwing runtime error"
"Trainer not logging into Tensorboard"
"max_length in beam_search() and  group_beam_search() does not consider beam_scorer.max_length"
"[LXMERT] Unclear what img_tensorize does with color spaces"
"Can't load model estimater after training"
"[DeepSpeed] ZeRO stage 3 integration: getting started and issues"
"Multi-GPU seq2seq example evaluation significantly slower than legacy example evaluation"
"Potential incorrect application of layer norm in BlenderbotSmallDecoder"
"Use Bert model without pretrained weights"
"Training mask language model using multiple files"
"Implement fast tokenizer for Big Bird models"
"Add parallelize method to GPT-neo models"
"Got ValueError when `output_hidden_states=True` with `eval_accumulation_steps`"
"Difference in tokenizer output depending on where `add_prefix_space` is set. "
"MemoryError when computing metrics on Wav2Vec2"
"run_summarization: fine tuning Pegasus large, CUDA out of memory error "
"save_strategy=\"no\" but checkpoints are created after each evaulation"
"Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512) with Hugging face sentiment classifier"
"Inconsistent ProphetNet Tokenization"
"[docs] [sphinx] need to resolve cross-references for inherited/mixin methods"
"FP16 overflow with GPT-Neo when using sequence lengths of 2048."
"bug in quantization on albert "
"GPTNeo: importing model with padded vocab size should truncate wte"
"'BertTokenizer' object has no attribute 'decode'"
"performance drop after using bert"
"How to use tensorboard with Trainer?"
"using RAG with local documents"
"using RAG with local documents"
"CTRL model can not work"
"[Possible Bug] Getting IndexError: list index out of range when fine-tuning custom LM model"
"[question/help] T5 cross-attention shows inconsistent results"
"Cannot get test logits after training for TFSequenceClassifier on TF 2"
"Using BERTModel for learning a siamese encoder"
"XLMRobertaTokenizerFast gives incorrect offset mappings when loaded from disk"
"GPTNeo: RuntimeError: shape mismatch when using past_key_values to go forward more than one token"
"Confusion"
"GPT2 IndexError: index out of range in functional.py by running run_clm.py when adding any special tokens (even eos and bos only)"
"Model config is logged twice on startup"
"lr scheduler before optimizer step warning"
"Dependency version check fails for tokenizers"
"[run_clm] handling large inputs"
"Where to add truncation=True for warning Truncation was not explicitely activated but max_length is provided a specific value, please use truncation=True to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior."
"XLNET tokenization changes after saving and loading"
"How to resume_from_checkpoint for Seq2SeqTrainer of EncoderDecoderLM"
"Add CodeTrans, a model for source code generation, documentation generation and similar subtasks."
"Nested MLflow logging with cross-validation"
"Wrong num_label configuration in Fine Tuning NER when model_name_or_path is specified"
"Errors in inference API"
"ALBERT pretrained tokenizer loading failed on Google Colab"
"Not very good answers"
"Create embeddings vectors for the context parameter of QuestionAnsweringPipeline for reusability."
"denoising with sentence permutation, and language sampling"
"Clear add labels to token classification example"
"Problem with data download"
"Trainer callbacks such as on_epoch_end do not pass in the documented eval dataloader"
"Inference time got very high, very low CUDA activity"
"OOM issue with prediction"
"Training loss is not logged correctly when doing evaluation with Trainer"
"cannot import name 'BigBirdModel' from 'transformers'"
"Using run_language_modeling.py to train an English adapter"
"[BUG] padding tokens are also masked in DataCollatorForLanguageModeling"
"Multi-`train_dataset` in Huggingface Trainer"
"Why padding tokens can be masked in albert model? Is it bug or right?"
"LM finetuning on domain specific unlabelled data"
"Why optimizer need split parameter group?"
"ZeroDivisionError: float division by zero after some epochs while training using run_mmimdb.py"
"error while training wave2vec on arabic text"
"tokenizer.encode_plus returns torch.tensors loaded on the desired device"
"[run_clm] tokenize_function clarification makes it non-hashable => no-reusing cache"
"Unable to resume checkpoints with TFBertModel using tf.distribute.Strategy and a custom LM head that shares the underlying TFBertEmbeddings layer"
"Error in running run_tf_text_classification.py"
"Encoder-Decoder Models Can't Generate using Apex"
"Using BART for Mask Infilling makes all the first tokens missing  "
"MemoryError: when we run_language_model.py to train an English Adapter"
"TypeError: expected str, bytes or os.PathLike object, not NoneType"
"Why couldn't I use encoder_hidden_states when position_ids is not None? GPT2Model.foward()"
"Sequential constraints?"
"How to kill bad starts when pre-training from scratch"
"Can not  instantiate BertGenerationEncoder or  BertGenerationDecoder from bert model"
"Loading pretrained mBART model always generate the same output"
"strange memory usage for t5 models"
"ELECTRA-large-discriminator results are not stable"
"wav2vec 2.0 doesn't appear to do vector quantization"
"Decoding throws Segmentation Fault "
"Loading a model saved with `TFGPT2LMHeadModel.save_pretrained` with `GPT2LMHeadModel.from_pretrained(..., from_tf=True)`"
"ProphetNet with AttributeError: module 'torch.utils' has no attribute 'checkpoint'"
"Transfer learning on bert"
"Getting no attribute 'output_attentions' error when upgrading to latest huggingface transformers "
"trainer.evaluate() expects batch_size to match target batch_size"
"Issue: Adding new tokens to bert tokenizer in QA"
"Issue: List index out of range when using Seq2SeqTrainer"
"How to extract the specific output using the method \"encoder_output[0]\""
"ModuleNotFoundError: No module named 'transformers.modeling_camembert'"
"Rework examples/ to overwrite cache_dir for datasets too. "
"Issue: Trainer error on `evaluate()` in multithreaded/distributed context (shape mismatch)"
"[RFC] introduce `config.trained_precision`"
"Documentation enhancement - model_type"
"Beam search on BART seq2seq"
"It don't find simple logic sequences "
"Load BART-base error"
"Question about validation_set"
"Weird issue with OOM on exported save_pretrained models"
"run_qa.py fails evaluating on Squad2"
"\"Connection error, and we cannot find the requested files in the cached path.\" ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
"BigBird Causal Attention"
"Getting `NameError: name 'BertOnlyMLMHead' is not defined` error when upgrading to latest transformers "
"add new token to Bert"
"position_ids generated from Roberta"
"Cant load tokenizer locally after downloading it"
"Batching in NER pipeline"
"RuntimeError: leaf variable has been moved into the graph interior"
"Enable Wav2Vec2 Pretraining"
"TypeError: can't pickle _thread.RLock objects hyperparameter_search raytune"
"[Benchmark]"
"Big Bird generate() \"local variable 'next_tokens' referenced before assignment\""
"Getting KeyError: 'loss' when fine-tuning model on a pre-trained MLM"
"[Benchmark]"
"[Benchmark]"
"About pre-trained model : facebook/wav2vec2-large-xlsr-53 & facebook/wav2vec2-base"
"--sharded_ddp \"zero_dp_3 offload\" fails with AssertionError "
"Failed to import transformers"
"TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect"
"Multi-Workers distributed training"
"TensorFlow \"predict\" returns empty output with MirroredStrategy"
"chunk of words for input token"
"inf/nan in generate (beam_sample) with small temperature values"
"DataCollatorForSOP marked as deprecated but DataCollatorForLanguageModeling does not offer the same functionality"
"gpt-neo 2.7 crashes, 1.3 runs fine"
"squad_convert_example_to_features is broken"
"Running gpt-neo 2.7B with less than 13GB of system memory like Colab"
"We should make an eco freindly phone and it should be affordable for everyone"
"[Benchmark]"
"fp16 compatibility"
"failed to import BertModel"
"Adding and consequently removing tokens leads to incorrect number of input embeddings"
"tf.function and half precision fails with Roberta models"
"Beam search decoding and language model integration for Wav2Vec2ForCTC models"
"Loading from checkpoint seems to hang indefinitely for Roberta"
"`resize_token_embeddings` not taken into account in `save_pretrained` for `EncoderDecoderModel`"
"Zero-shot pipeline feature extraction"
"Question about T5-11b model weights"
"google/pegasus-cnn_dailymail generates blank file"
"Python crashes when loading Bert model from pretrained"
"OSError: Unable to load weights from pytorch checkpoint file"
"serious bug with trainer.py when restarting the training from a checkpoint"
"Cannot save GPT2 model with signature"
"Longformer model with weight(model.encoder.embed_positions.weight) error"
"Problems with webbased editing of model cards "
"small bug in RAG model"
"env about run longformer model downloaded from https://github.com/allenai/longformer "
"invalid multinomial distribution (with replacement=False, not enough non-negative category to sample)"
"Getting time offsets of beginning and end of each word in Wav2Vec2"
"RAG with RAY implementation: Ray workers memory slowly increase over time."
"[Benchmark] GPT2LMHeadModel (gpt2-medium) forward pass inference became 9% slower compared to 2.8.0 release"
"The output of IBERT is float32. Am I doing wrong?"
"T5Model crashes when trained with multiple GPUs"
"large memory usage when resuming training from a checkpoint"
"Error in loading model tokenizer ('Helsinki-NLP/opus-mt-en-fr' actually loads 'Helsinki-NLP/opus-mt-en-de')"
"Irregular VRAM usage with gpt-neo inference with sequences longer than 250 tokens"
"EncoderDecoderModel's decoder gets unexpected use_cache argument"
"Bug in trainer: substantially different results from restarting from a checkpoint and without"
"Parameter missing from state_dict of optimizer when loading from checkpoint"
"run_ner.py example MobileBERT FP16 returns nan loss"
"batch_encode_plus set a sort parameter"
"Potential bug: Tokens with punctuation are re-tokenized although I've set `is_split_into_words=True`"
"Bug in GPT2ForSequenceClassification"
"M2M-100 SentencePiece model produces tokens that are missing on the fixed dictionary"
" tf generate compatible with tf.function"
"Perform max_input_tokens truncation with Summarization Pipeline"
"Warpping model.generate before exporting into tensorflow savedmodel format"
"mlflow parameter overflow when training a language adapter "
"[run_summarization.py] wrong dataset leads to CUDA error:s"
"absolute embeddings in Deberta"
"'Tensor' object has no attribute 'size'"
"fine tuning encoder decoder for custom language translation"
"Question-answering pipeline failing with Nonetype exception when selecting spans with tokens outside of the context"
"Whyhttps://github.com/huggingface/transformers/tree/master/examples/pplm"
"possible mistake in documentation"
"Different results between `AlbertTokenizer` and `AlbertTokenizerFast` modules with a new `spiece.model` file"
"Training a TimeSFormer for video classification "
"torch_xla/csrc/tensor_methods.cpp:880 : Check failed: xla::ShapeUtil::Compatible(shapes.back(), tensor_shape) "
"Index out of range in self with fine-tuned DPR Context Encoder"
"RuntimeError: CUDA error: device-side assert triggered"
"Megatron fused CUDA kernels to improve Hugging Face model classes' scalability"
"ERRORS: run_mlm_performer.py"
"[examples] UserWarning: `max_length` is deprecated"
"Output probability from `model.generate` for TF models"
"Wav2vec2: comparison to original implementation"
"Trainer._remove_unused_columns() returns None"
"some issue in loading local txt file as Dataset for run_mlm.py"
"[docs]Incorrect way of input encoding for \"multiple choice\" models in documentation?"
"CUDA OOM in the middle of training when the training data is large"
"Distributed DataSampler has fixed data order despite random seeds."
"S3 checkpoints not working with distributed training on sagemaker "
"MayBe There is a bug with class DebertaV2PredictionHeadTransform"
"PreTrainedTokenizerFast.save_pretrained() ERROR"
"RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 237414383616 bytes. Error code 12 (Cannot allocate memory)"
"unable to import transformers in Python <3.8"
"Download offile HuggingFace Models in other format than \".bin\" Format"
"Positional embeddings are not applied when input embeddings are passed in for Pytorch DistilBert model"
"metric is uninitialized when csv data is supplied to example/pytorch/text-classification/run_glue_no_trainer.py"
"Documentation 404 error"
"[CI] solving the pytest crashing and hanging CI job"
"How to use GPU when running run_summarization.py"
"What do these model parameters mean?"
"Small bug while converting wav2vec2 model trained using fairseq to huggingface"
"Allow adding custom logits processors in the `generate` method"
"checkpointing is not still covering all cases "
"Roberta Tokenizer cannot handle inputs with `<mask>` token"
"Transformers Pegasus - how do I fine tune another language?"
"Parameter in `DebertaV2Tokenizer.__init__()` without documentation: `split_by_punct`"
"[Question] Implementing character based tokenizer"
"Race condition when using --save_total_limit, --load_best_model_at_end and deepspeed zero2+cpu_offload"
"Transformers 4.1.1 & Tensorflow 2.0, AttributeError: module'tensorflow_core.keras.activations' has no attribute'swish' "
"IBert: What would be the possible reason `IntLayerNorm` does not decrease the loss?"
"Simple questions about EncoderDecoderModel"
"ALBERT: The following keyword arguments are not supported by this model: ['cls_index', 'p_mask', 'is_impossible']."
"RoBERTa: ValueError: The two structures don't have the same sequence length. Input structure has length 5, while shallow structure has length 4."
"`sp_model_kwargs` param missing at unpickle in `XLMRobertaTokenizer`"
"tensorflow version is not able to pick the trained model from local directory in an air gapped system"
"convert gpt2 from tensorflow to pytorch"
"\u68af\u5ea6\u7206\u70b8\u95ee\u9898"
"Feedback whilst resuming"
"Minor error on example distillation script"
"BERT model gets fairly random results"
"[wav2vec] deepspeed eval bug in the case of >1 gpus"
"Google Colab TypeError: expected str, bytes or os.PathLike object, not NoneType"
"Activating gradient checkpointing"
"mBART and DataCollatorForLanguageModeling: index -1 is out of bounds for dimension 1 with size N"
"wav2vec2 doesn't work with torch.distributed.launch & multi GPU"
"cannot import name 'set_seed' from 'transformers'"
"Unable to use custom dataset: AttributeError: 'list' object has no attribute 'keys'"
"Perturb Hidden-State in Encoder-Decoder Models"
"Can this `@slow` annotation be removed at barthez tokenizer test="
"\"Is next sentence\" pre-training task availability for Language Modeling scripts"
"extending metric_for_best_model to a list of strings "
"support batch-sampler in trainer "
"T5-large FP16 produces nan in loss"
"[DeepSpeed] ZeRO-Infinity integration: getting started and issues"
"[resume optimization] skip loading pretrained weights on resume"
"binary classification does not work with a large amount of data"
"Train GPT2 with Trainer & TrainingArguments using/specifying attention_mask"
"Can not import modeling_mbart"
"can not import mbart and mT5 modeling file"
"[Flax] Add FlaxBart model"
"[Docs] Add Caching Example For CI? "
"Error In Running Predictions for run_text_classification.py"
"[Docs] Clarify Subphrase classification?"
"The performance of the huggingface QA model depend on the order in which it loads"
"MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\") Not working"
"run_mlm.py : Missing key(s) in state_dict & Unexpected key(s) in state_dict"
"Importing problem "
"TFLongformerForMaskedMLM example throws ValueError \"shapes are incompatible\""
"Tensorflow \u201cIndex out of bound\u201d error when trying to use the TF Longformer transformer in a custom TF network"
"mbart encoder decoder model"
"not able load model from pipeline NotFound error"
"Penalise n-gram repetition in generated sequences"
"Issue in checkpointing"
"encoder decoder in transformers"
"Fine-Tuning TFGPT2LMHeadModel / What to pass to fit"
"Help understanding how to build a dataset for language as with the old TextDataset"
"I-BERT: expected str, bytes or os.PathLike object, not NoneType"
"Piece A"
"Issues with TFGPT2ForSequenceClassification"
"Run_summarization not working for mbart50"
"BART summarization, tokenizer not working"
"RoBERTa adds two sep tokens"
"How to set up a custom tokenizer for distilbart"
"Compute probability of target sentences given an input"
"Distributed multi-node support for CPU cluster"
"Deberta v2 Fast Tokenizer"
"generate text with inputs_embeds (instead of input_ids) for T5."
"Adding custom tokens makes the T5Tokenizer always strip spaces"
"Files not accessible via IPv6"
"How to run transformer model like t5-small, facebook/bart-large-cnn without loading pretrained weights?"
"Adafactor gives RuntimeError: tensors must be 2-D"
"Unable to load DistilBertModel during training "
"Pegasus tokenizer does not have bos token, cannot pretrain"
"Error when trying to use GPTNeo model"
"'return_dict_in_generate' and 'output_scores' argument in BartForConditionalGeneration.generate()"
"Strugle to change `num_labels` of roberta-large-mnli"
"T5 fp16 crashes on the CPU (but works on CUDA)"
"tokenizer not padding input_ids"
"why run_translation.py automatically is running on cpu?"
"Bugs when trying to train a T5model from scratch in the run_summarization.py script"
"NaN Person/Spearman corr. when fine-tuning BERT with example code and commands"
"DPR with ELECTRA models"
"Error In Fine-Tuning Transformer XL ValueError: The two structures don't have the same sequence length. Input structure has length 3, while shallow structure has length 2."
"AttributeError: 'TrainingArguments' object has no attribute 'resume_from_checkpoint' in training GPT2"
"hyperparameter_search raytune: ModuleNotFoundError: No module named 'datasets_modules'"
"Temporary files from an interrupted download litter the disk"
"`TypeError: TextInputSequence must be str` from Fast Tokenizer"
"[fixup/style] requires TF but doesn't say that cleanly"
"reformer-enwik8 Fine-tuning"
"Number of BART layers is confusing in the Pretrained models page"
"longform"
"no connection error"
"potential mismatch between `save_pretrained` and `from_pretrained` for `AutoTokenizer`"
"How to use `model.generate` with custom model with additional parameters?"
"Support saving (and loading) models to a remote bucket"
"ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length"
"unable to save model1.h5 .I am using huggingface distilbert"
"Wrong results for GLUE task STS-B"
"Punctuation in Wav2Vec2"
"Windows Test Errors "
"evaluation in TFTrainer does not run on GPU"
"Cannot use multiple GPUs to finetune RAG using sample code with customized knowledge"
"Distil BART for text simplification"
"Is the \"dummy inputs and standard lengths\" implication, when using tracing for exporting a model, still true?"
"[consistent use] `F` vs. `nn.functional`"
"Using `TFGPT2LMHeadModel.generate` with tf.distribute.TPUStrategy and tf.function"
"Seq2SeqTrainer not working for a list of inputs: TypeError: can't convert np.ndarray of type numpy.object_"
"TFWav2Vec2Model"
"Model type to AutoModelForQuestionAnswering incorrect"
"Is it correct to load weights from task A to train task B"
"[RAG] ModuleNotFoundError: No module named 'git' when finetuning the model"
"In \"Question Answering\" separate context from question"
"Error when using Adafactor without learn rate"
"wrong default value of argument \"ignore_index\" in CrossEntropyLoss for loss calculation in models forward method"
"[fairscale] rng states saving fails in an extended multi-gpu test"
"When I use run_ner.py to fine-tune the model based on Bert, I cannot predict any entities"
"CUDA error: an illegal memory access was encountered"
"NN_pruning module for Question Answering"
"LukeForEntitySpanClassification - ValueError: only one element tensors can be converted to Python scalars"
"Felix"
"Add visual + link to Premium Support webpage to README"
"I-BERT tokenizer not loading; example code not working."
"Multilingual MobileBERT"
"How to change training epochs when using run_summarization.py"
"IDE cannot correctly navigate to references, It will navigate all object to `transformers/utils/dummy_pt_objects.py`."
"How to train TFBertForTokenClassification without padding mechanism "
"Cannot load studio-ousia/luke-base for AutoModelForTokenClassification"
"Bad result in fine-tuning XLNet for SQuAD"
"Strange implementation of `convert_tokens_to_string` in albert tokenizer."
"Key Error: 'pre-processing' during conversion from tatoeba to Marian model"
"Very large difference between the results after resume"
"Reformer inference widget is broken"
"[DOC] Fine-Tuning NER Custom Dataset Clarification"
"DISTILBERT: run_squad.py not working"
"Memory Leak in Deberta (v1) Base "
"NCLL No space left on device Error while training with deepspeed"
"[Doc] Something wrong in description of 'DistilBertForSequenceClassification' in doc"
"IBERT: Testing the speedup"
"RuntimeError: Error(s) in loading state_dict for Wav2Vec2ForCTC"
"[Question] How to move and reuse preprocessed dataset?"
"GPTNeoForCausalLM: resuming Trainer from checkpoint causes Missing key(s) in state_dict: \"lm_head.weight\""
"KeyError: 'bigbird_pegasus'"
"[Question]  How to serialize and load a trained RoBERTa model?"
"license missing for xlm-roberta-large, and bert-base-spanish-wwm models"
"Why run_translation.py automatically runs on CPU?"
"Zeroshot pipeline performance worse on CPU when processing multiple texts as \"batch\""
"Cannot reproduce results from zeroshot demo app"
"Issue getting prediction_scores from TransfoXLHeadLM model when labels are provided"
"Routing Transformers / Add Google PG-19 Models"
"Remove \"`optional`, defaults to :obj:`None`\""
"Trainer skips training when continuing training with model.from_pretrained()"
"DeBERTa pretraining data preparation"
"Flag to disable shuffling for data loader"
"Mixed precision training : link broken"
"Offline installation of the transformers repo (error message)"
"channel_len specified but not used"
"[RAG] official facebook example code for RAG is not working anymore."
"Loading Basic GPT-2  model gives warning that attention layers weren't loaded from pre-trained weights"
"stop at load tokenizer_config.json when run barthez for mrpc "
"AssertionError: internal model should be a reference to self.model"
"How to accelerate the inference speed when using pipeline"
"Reformer for questions answering(squad)"
"Unable to import transformers: ImportError: numpy>=1.17 is required for a normal functioning of this module, but found numpy==1.16.3"
"Blender 9B model"
"Request for feature for setting batch size in pipeline when inference"
"parameter `ignore_keys` of `trainer.predict` not accessible in `Trainer` or `TrainingArguments`"
"RagRetriever fails to find faiss-gpu installed with pip not conda"
"ValueError: could not broadcast input array from shape (2816,384) into shape (2698,384)"
"Plug a custom tokenizer into PreTrainedTokenizer"
"Warnings about some weights that were not initialized in Greek BERT"
"A bug in modeling_tf_marian.py and modeling_tf_pegasus.py SinusoidalPositionalEmbedding _init_weight"
"ImportError: cannot import name 'load_dataset' from 'datasets'"
"[Benchmark]"
"Bert2bert on Swag with very low accuracy"
"`ci/circleci: run_tests_torch` reaches 10 min. time limit"
"Import `SPIECE_UNDERLINE` from `file_utils` instead of WET definition"
"CPU Memory Leak when using RoBERTa for just word vector representation"
"Cant load google/reformer-enwik8"
"Problem with mT5 and the official Summarization notebook"
"Convert blenderbot checkpoint to tensorflow (TF)"
"Wrong output used by RobertaForSequenceClassification classification head"
"mbart-large-cc25 tokenization_utils_fast.py TypeError "
"[deepspeed] supporting `--adafactor` "
"parallelize and deparallelize method for GPT-Neo series model"
"Trainer accumulates GPU usage at the beginning of each step"
"A problem of Ibert IntSoftmax"
"word_to_tokens method of XLNetTokenizerFast not behaving correctly"
"error in load of tokenizer with add_token"
"Add batching to pipelines"
"Unable to use fill-mask pipeline on gpt-neo model"
"Error when using IterableDataset as train_dataset for Trainer"
"AttributeError when using EncoderDecoderModel.forward() with encoder_outputs and return_dict=True"
"DataCollatorForWholeWordMask only works for BERT, and nothing is said in the docstring."
"Trainer removes newer checkpoints, not older."
"Different performance when training different transformers version"
"Finetune - Helsinki-NLP/opus-mt-fr-en "
"Unintentional(?) interface change on loss function in models didn't work well for single-column regression"
"`generate` with `num_beam` > 1 does not work in EncoderDecoder models when `past` is supplied."
"PyInstaller Transformers runtime import error"
"GPT Neo past_key_values unexpected behaviour"
"EncoderDecoder Cross Attention Generation Output Shape does not match Documentation"
"PegasusTokenizer returning None"
"facebook/mbart-large-50-one-to-many-mmt fails on Swahili"
"LongformerForSequenceClassification: global_attention_mask=None"
"T5EncoderModel slower in half-precision"
"[trainer] the noisy tensorflow loaded when asked explicitly not to load it"
"Bug in TokenClassificationPipeline"
"get_length_grouped_indices() uses slow list concat"
"[trainer] multi-node tweaks"
"[examples] add desc to `dataset.map` to improve tqdm bars"
"ImportError: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.8.1rc1."
"CamemBert Tokenizer AttributeError: 'NoneType' object has no attribute 'tokenize'"
"[examples] run_clm re-processes dataset on every run"
"Text Generation, adding random words, weird linebreaks & symbols at random."
"bert model (bert-base-chinese) consumed too much memory"
"Index out of range when doing manual testing for TFBertModel"
"version of T5 is not reported in HuggingFace models "
"How to save and load model from local path in pipeline api ?"
"Wrong LayerNorm weight names in \"bert-base-uncased\" checkpoint ?"
"GPT Neo for Sequence Classification"
"Permission error for cardiffnlp/twitter-roberta-base-emotion"
"How get sentenses embbedings from TFBertForMaskedLM"
"ValueError batch-size mismatch when redefining classifier layer on BertForSequenceClassification"
"same sentence different padding length result different embedding."
"Training Transformer XL from scratch"
"Hugging Face model Bio_ClinicalBERT producing 404 error"
"My modified `run_glue.py` works well with v4.1.1 but not good with v4.6.0"
"possible bug in `TokenizerFast` when setting `return_offset_mapping=True`"
"[AutomaticSpeechRecognitionPipeline] CUDA support"
"Delete key or set to `None` in __getstate__ impl."
"[docs] XLnet reference link bug in description of past_index Parameter of TrainingArguments "
"Seq2seq-based model running slowly on TPU"
"[BUG] Trainer predict bug under DDP model."
"convert_pytorch_checkpoint_to_tf2.py AttributeError: embeddings.word_embeddings.weight not found in PyTorch model"
"Not able to fine tune language model"
"Module torch has no attribute minimum for modeling_big_bird.py"
"Is 10% in annotation different from 0.5 in code\uff1f"
"Bug in MLM example scripts"
"Generate Function call throughs error when \"inputs_embeds\" argument passed"
"Issues loading finetuned BERT"
"Regression in training speed since 4.4.0"
"Request addition of 'GPT2ForwardBackward' models"
"Token Classification OOM"
"Gradient is None in after deepspeed backward "
"Multi-node training for casual language modeling example does not work"
"Permission denied for cardiffnlp/twitter-roberta-base-emotion"
"ONNX model conversion"
"'SequenceClassifierOutput' object has no attribute 'log_softmax'"
"Bart tokenizer and bart model for conditional generation have different vocab size "
"[Benchmark]"
"# \ud83d\udda5 Benchmarking `transformers`"
"Wrong BlenderbotConfig description (max_position_embeddings)"
"Custom train file not supported in run_qa.py"
"Issue: BART does not learn during fine-tuning for abstractive text summarization"
"Want to use bert-base-uncased model without internet connection"
"Errors in Quickstart  Documentation related to GPT-2"
"[lm examples] replicate --config_overrides addition to other LM examples"
"Cannot add tokenizer to model repo"
"basic_tokenizer don't preserve token encoding/format"
"Trainer : AttributeError: 'str' object has no attribute '_memory_tracker'"
"KeyError: 'labels' in distill_classifier.py"
"Adding new Jax Models."
"BertForMaskedLM training fails when using iterable eval_dataset and DataCollatorForLanguageModeling collator."
"Mask token mismatch with the model on hosted inference API of Model Hub"
"Find the requested files in the cached path without the internet"
"Wrong subword aggregation when using aggregation_strategy"
"Add a new pipeline for the Relation Extraction task."
"GPT2 saved pb file cannot handle dynamic sequence length"
"Deepspeed integration ignores Optuna trial parameters in hyperparameter_search"
"Small error in documentation / Typo"
"mutil gpu errors"
" Provides an option to select the parallel mode of the Trainer."
"[Flax] Add attention weights outputs to all models"
"Problem when freezing all GPT2 model except the LM head"
"'error': 'Model Matthieu/stsb-xlm-r-multilingual is currently loading'"
"Customize pretrained model for model hub"
"Fine tuning with transformer models for Regression tasks"
"xla_spawn.py:  xm.get_ordinal() got an unexpected keyword argument 'local'"
"Distillation of Pegasus using Pseudo labeling"
"Inference for pinned model keeps loading"
"How to get back the identified words from LayoutLMForTokenClassification?"
"RuntimeError: The size of tensor a (716) must match the size of tensor b (512) at non-singleton dimension 1"
"Wrong perplexity when evaluate the megatron-gpt2."
"Trainer reported loss is wrong when using DeepSpeed and gradient_accumulation_steps > 1"
"ProphetNetForConditionalGeneration model isn't returning all objects properly"
"Trainer.predict using customized model.predict function?"
"BERT pretraining: [SEP] vs. Segment Embeddings?"
"Modifying the distill bert architecture"
"Use `self.assertEqual` instead of `assert` in tests."
"Ray pickling issue when running hp search"
"How to load the best performance checkpoint after  training\uff1f"
"LayoutLMv2 Model"
"Predict masked word at the beginning of the sentence"
"Flax text-classification multi-optimizer incorrect"
"[docs] XLNETModel forward returns last_hidden_state 3rd dim should be d_model instead of hidden_size "
"XLM tokenizer lang2id attribute is None"
"position_ids version changed during training"
"Typo in Pegasus model usage example"
"RuntimeError: CUDA error: device-side assert triggered"
"wandb integration gags during hyperparameter search"
"Loading mbart-large-50-one-to-many-mmt is very slow"
"Encoding/decoding NLP model in tensorflow lite (fine-tuned GPT2)"
"Flax port vision Transformer to flax"
"report_to flag does not work with TFTrainer"
"ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds"
"TypeError: __init__() got an unexpected keyword argument 'force_bos_token_to_be_generated'"
"AutoModel abstraction fails for pre-training initialization"
"Uncoupling ZeRO-3 weak ref bridge b/w Trainer and modeling_utils"
"Killed Message"
"Byt5"
"Issue: IndexError: \"Index out of range in self\" when generating translations with MarianMTModel"
"Add new token to pretrained GPT2 tokenizer"
"Summarization also supports MT5ForConditionalGeneration"
"How to achive character lvl tokenization? (cant convert from huggingface/tokenizers)"
"Reproducibility Questions"
"run_qa.py for Question and answering doesn't work for SQUAD2 "
"Saving and loading a model does not work"
"RuntimeError: Overflow when unpacking long during training the model"
"It seems not able to add the args \"repetition_penalty\" when running the code run_summarization.py for prediction."
"T5-Training Arguments"
"No package metadata found for tqdm while generating exe"
" AttributeError: 'GPT2LMHeadModel' object has no attribute 'get_encoder'"
"Movement Pruning does not achieve expected results"
"Add loss reduction parameter in forward() method"
"    EOFError(\"No valid references for a sentence!\") for run_translation example"
"Trainer API"
"CLIPFeatureExtractor should resize images with kept aspect ratio"
"XLNET on SQuAD2 evaluation error"
"tensorflow has no attribute swish"
"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used."
"Add SENet Blocks in Encoding Layers"
"Unable to find examples on using DPR for transfer learning,request you to provide examples"
"Exporting the operator repeat_interleave to ONNX opset version (<=12) is not supported!"
"ImportError: cannot import name 'MarianMTModel'"
"Fast Tokenization fail for the pretrained model"
"Megatron GPT2 not compatible with transformers"
"where is the code for  DetrFeatureExtractor, DetrForObjectDetection"
"Fluctuating embedding given by different random seed during inference"
"Issue while using DPR with tensorflow and py torch"
"some issue in FlaxBertForMultipleChoice"
"Translation example generates the same input"
"RAG end to end with RAY throws pickling error"
"MIssmatch between problem_type and loss functions in DistillBert for sequence classification"
"FAILED tests/test_tokenization_deberta_v2.py::DebertaV2TokenizationTest::test_tokenizer_integration"
"Allow registerable Components"
"Remove \"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\""
"Getting IndexError: index out of range in self while finetuning GPTNeo on Text Classification"
"Seq2SeqTrainer: cannot set max length when we evaluate/(generate) during training"
"xla_spawn.py: Cannot load large (~1GB) optimizer.pt from checkpoint"
"Documents of `past_key_values` in input and output for `PegasusModel` are not aligned"
"A bug of modeling_wav2vec2.py:1033 line"
"After loading fine-tuned model from local and use it for prediction, it continue training from scratch again!"
"I cannot import deepsepped"
"Using the  latest DPR checkpoint available with HuggingFace DPR class"
"Support for pointer-generator architectures."
"pipelines should allow passing in tokenizer arguments"
"Why my simple Bert model for text classification could not learn anything?"
"Electra model vocabulary"
"ImportError: cannot import name 'AutoTokenizer' from 'transformers'"
"Question: Masked Loss for LukeForEntitySpanClassification"
"OpenAI GPT language modeling shape mismatch: 512 position embeddings, 1024 input emebddings"
"[end2end RAG]  AttributeError: module 'pickle' has no attribute 'PickleBuffer'"
"No max_length set on huawei-noah/TinyBERT_General_4L_312D/config.json"
"How to update the GPT2 with loss which are provided from another separate module?"
"Settings for perfect Story writing based on the input text?"
"[testing] set tests to not rebuild datasets"
"[skipped test] to fix"
"[testing] making network tests more reliable"
"fp16 models getting auto converted to fp32 in .from_pretrained() "
"ImportError: cannot ipmort name 'TFAutoModel'"
"How can we predict story future based on past events?"
"Selecting specific GPU CUDA devices"
"grads is None when using GPT2 transformers in tensorflow"
"XLM-R XL/XXL"
"Inconsistent behavior on CPU vs. GPU "
"src_lang/tgt_lang missing in mbart example"
" Using whitespace tokenizer for training models "
"OSError: Unable to open file (file signature not found)"
"Use Distilbert to run language model, encounter error  \"Unrecognized configuration class \"  "
"GPT2 Flax \"TypeError: JAX only supports number and bool dtypes, got dtype object in array\""
"Memory Efficient FP 16 Training"
"examples requirements isn't in sync with `require_version_examples`"
"Checkpoint detected info log in run_clm.py"
"Provide more useful error message in Detr from_pretrained when timm not installed"
"Replicating PEGASUS results on a benchmark dataset"
"Speedup batch matmul in pytorch"
"Create a torchscript version of Tokenizer in Bert"
"Continuous training on Fine-tuned Model"
"DetrFeatureExtractor post_process not rescaling bboxes as expected"
"\ud83c\udf1f New model addition - GPT-J-6B"
"FillMaskPipeline very slow when provided with a large `targets`"
" 'Speech2TextProcessor' has no attribute 'from_pretrained'`"
"GPT2 medium config n_ctx is wrong I guess?"
"ViT tensorflow Implementation"
"Issue with mBART50 es-en translation "
"What is the correct way to pass labels to DetrForSegmentation?"
"How can I add a CNN layer on top of bert model?"
"How to access training loss in TrainerCallback?"
"Why attention mask is -10000 but not * 0?"
"How to pass `past_key_values` to GPTNeo model?"
"Get the loss in LongformerForQuestionAnswering for fine-tuning"
"Hosted inference api keeps returning 400 error"
"GPT Neo Tokenizers can't change BOS or EOS token"
"Passing a custom stopping_criteria list to model.generate() yields a multiple value error for that keyword arg"
"ValueError in predict function for ClassificationModel"
"[Performance] Tracking open Issues and PRs (pytorch transformers)"
"Multi-GPU training has literally no GPU-Utilization (0%)"
"got multiple values for argument 'input_shape'"
"TypeError when trying to load pretrained ALBERT model in BertTokenizer "
"Adding fastseq support to more recent version of HF transformers"
"wav2vec2 not converging when finetuning"
"Using checkpoints in gpt neo xl"
"[FLAX] port GPTNeo to Flax"
"RuntimeError: Could not infer dtype of numpy.int64 on Squad T5"
"CLIP tokenizer inconsistent with OpenAI release"
"Ouput Includes Input"
"How to train the new wav2vec unsupervised model using hugging face ?"
"Feature request for encoding more than one pair of texts"
"do_normalize set to True by default for WAV2VEC tokenizer"
"\ud83e\udd17 The Hugging Face Course is out!"
"Pretraining for TFWav2Vec2"
"Can't run QA fine-tune for bert/albert in distributed way"
"Missing code for predicting custom labels in Bert"
"[testing] concurrent dist tests fail when using the same master_port"
"Documentation for tiny-gpt2 in transformers/examples/pytorch"
"ViT for resolution beyond 224x224 support"
"Special tokens not tokenized properly"
"Allow setting permissions of downloaded models (via envvar)"
"Vision Transformer (ViT) feature vector example (not classification)"
"How can we modify the MM-IMDB model for sequence to sequence generation tasks?"
"Pretrained XLM model with TLM objective generates nonsensical predictions"
"TPU training is stuck using T5 with PyTorch Lightning"
"Exception during hyperparameter search with Ray and transformers library starting from version 4.5.0"
"Can't run 124M using transformers"
"KeyError: 'labels' during Distilling Zero Shot Classification"
"Inconsistency between GPTNeo and GPT2 config classes"
"TextDatasetForNextSentencePrediction does not seem to contain truncate function unlike LineByLineWithSOPTextDataset"
"T5 Generate from Encoder Output"
"How to figure out which pretrained tokenizers support emojis?"
"Cannot import RobertaPreTrainedModel"
"LayoutXLM not loaded"
"Batched pipeline for NER"
"Where I can find official pretrained weights of SOP in Albert and NSP in Bert?"
"XLM-RoBERTa MLM Trainer not saving 'sentencepiece.bpe.model' file"
"[Docs] Broken Link in the Benchmarks.rst"
"ValueError: char_to_token() is not available when using Python based tokenizers ; XLNetTokenizer and encodings.char_to_token bug ;"
"Training in google colab with TPU using TFTrainer fails with "
"blenderbot checkpoint convert script has bug."
"The kernel appears to have died. It will restart automatically. from transformers import pipeline"
"Better documentation for generation parameter defaults"
"Clearer indication for overridden method in generation"
"[Question] When pretraining a language model, can I choose to mask specific words?"
"Getting 404 Client Error when loading BaptisteDoyen/camembert-base-xnli"
"Missing PredictionHeadTransform for BertGenerationDecoder"
"T5 model seq2seq text generation using word embeddings instead of token_ids does not work"
"[Trainer.py] tr_loss in trainer with distributed training"
"Tokenizer encoding skips \ufffd character"
"Argument `never_split` not working on `AutoTokenizer`"
"Pegasus pretraining in fp16 results in NaN loss"
"Batch inference runtime slows down for inputs with different length sentences"
"RobertaForMaskedLM.from_pretrained throwing some weights not initialized error when loading same model type"
"Reconstructing Tokens from Bert Embedding?"
"can predict_with_generate (do_eval) work with sharded_ddp fairscale in 4.6.1+?"
"BART fine-tuning doesn't work and produces a fixed output for each input"
"[doc] t5 incomplete example"
"Modify BERT encoder layers?"
"Can't load tokenizer for 'imxly/t5-pegasus'."
"RoFormerTokenizerFast has a wrong result when setting \"return_offsets_mapping=True\""
"TFBertForMaskedLM won't reload from saved checkpoint, shape mismatch issue"
"Different Weights between google-bert (uncased_L-12_H-768_A-12) and Huggingface-bert (bert-base-uncased)"
"finding a bug in training the code of /src/transformers/models/detr/modeling_detr.py"
"Got unexpected result when using BertTokenizer in Chinese"
"RAG with T5 in a multitask setting"
"First hidden state of the last layer of Bert (french version : FlauBert) only prints vectors of 0 or -0 after using it !!"
"[Documentation Example] Task Summary - Start/End of Span in QA Example"
"`ValueError: Expected input batch_size to match target batch_size` occurs when training GPT2 with `Seq2SeqTrainer`"
"353 duplicate tokens in GPT-2?"
"GPT2Model cannot handle 3D attention_mask"
"TFWav2Vec2ForCTC & Wav2Vec2ForCTC gives different loss values"
"Mbart continue training with same training task on a specific language"
"Causal Mask in BertGeneration"
"[Documentation] Example for LEDForConditionalGeneration does not work"
"Add TFSpeech2Text"
"Add error message to Wav2Vec2 & Hubert if labels > vocab_size"
"[Deepspeed zero3] lazy weights init "
"[Deepspeed] [performance] inefficient load with `from_pretrained` w/ zero3"
"[performance] module init w/ `from_pretrained` skip storage allocation"
"Transformers-CLI not saving pytorch model after conversion"
"ViTFeatureExtractor.save_pretrained() generate \"preprocessor_config.json\" but not \"config.json\""
"TFWav2Vec2ForCTC: Error when using padded batch and attention mask"
"Memory leak when using DistilBert for inference to extract [CLS] hidden state"
"[examples] replicate the new `--log_level` feature to all trainer-based pytorch examples"
"BART infilling example? "
"Error: while executing run_qa.py from examples/pytorch/question-answering/  directory"
"T ** 2 in distillation process"
"Are there any examples showing how to use the `metric_for_best_model` of class `TrainingArguments`?"
"Dimensional weight error"
"Tokenizing in the dataset and padding manually using tokenizer.pad in the collator"
"odd whitespace handling with imported sentencepiece models"
"New `--log_level` feature introduces failures using 'passive' mode"
"Model is saved every eval_steps steps if eval_steps < save_steps. Is this expected behavior?"
"Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task"
"Downloading the models is getting slower than before"
"`fill-mask` pipeline cannot load tokenizer's `config.json` (fixed in 4.8.0)"
"Generate text with `model.generate` on TPU does not work"
"How to assign gpu when using run_language_modeling.py"
"Default Parameters for training DistillBERT and DistillGPT2"
"Missing tokenizer_class for `mbart-large-50-many-to-one-mmt` model"
"ValueError: expected sequence of length 133 at dim 1 (got 80)"
"How to get offset mapping then decoding wav2vec?"
"[examples] [distributed] process datasets.map only on main process in "
"All evaluation processes overload one GPU, when other 7 are available. While Training process fine and is distributed across all 8 cards"
"TypeError: __init__() got an unexpected keyword argument 'report_to'"
"Generate text until condition"
"Prediction fails for certain batch sizes"
"ERROR: Failed building wheel for tokenizers"
"Input structure has type class tuple while shallow structure has type class transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
"[examples] add `main_process_first` context manager to datasets map calls"
"Tokens Jumbling"
"[Trainer.py] when --load_best_model_at_end is set in Distributed Training"
"Wav2vec2 Dataset"
"ImportError: cannot import name 'BertEncoder' from 'transformers'"
"model.generate occurs error:  generation_beam_search"
"Issue in layer-drop implementation in TensorFlow models in graph mode"
"conversion wav2vec2 model from fairseq to huggingface"
"TypeError: new(): invalid data type 'numpy.str_'"
"Tracking variables other than loss during training"
"Module version identification problem"
"A fast tokenizer for BertJapaneseTokenizer"
"About a error in retrain a xlm model"
"Size of tensors not matching even though using tweets (all same length)"
"Request: New LM Adapted checkpoints for T5"
"A model or a config like 'transformer_iwslt_de_en' for machine translation"
"Connot correctly fine-tune Bert for generation"
"GPT2-large for sequence classification default num_labels differs from the default for GPT2-small and GPT2-medium"
"`fill-mask` pipeline provides `<mask>` token among predictions"
"getting error with BertForMaskedLM"
"Reference postprocess_qa_predictions score method"
"[Deepspeed][initialization] pegasus: unable to load/init the weights"
"[FLAX] Core dump using example code"
"MLM training fails with no validation file"
"Wrong logical operation"
"New Model: Charformer: Fast Character Transformers via Gradient-based Subword Tokenization "
"\ud83c\udf1f New model addition: FNet"
"Benchmark Colab does not work"
"Streaming mode in training examples"
"BertTokenizer with BertJapaneseTokenizer pretrained model generates unintended tokenization."
"DeepSpeed gets stuck when training"
"HfArgumentParser defaults booleans to on"
"Loading custom model"
"[roberta] lm_head.decoder save/load needs fixing"
"[DeepSpeed] Convert from fp16 to fp32 issue zero_to_fp32.py"
"[DeBerta V2] The vocab size of DeBerta V2 is incorrect"
"Distilling zero-shot classification: Assertion `srcIndex < srcSelectDimSize` failed."
"how to continue pre-train custom data"
"TPU not initialized when running official `run_mlm_flax.py` example."
"Using huggingface Pipeline in industry"
"Add DEBERTA-base model for usage in EncoderDecoderModel."
"IndexError: index out of bound, MLM+XLA (pre-training)"
"Expand text-generation pipeline support for other causal models e.g., BigBirdForCausalLM"
"cookiecutter template for adding flax model"
"[Wav2Vec2] Better names for internal classes"
"Cannot load model saved with AutoModelForMaskedLM.from_pretrained if state_dict = True"
"Deit"
"Instantiating a model from `pipeline()` ignores `model_kwargs` parameter"
"Finetuned model generates incomprehensible text when used while in memory but works fine when loaded via saved checkpoints."
"Wiki content part : XLM-RoBERTa,   \"xlm-roberta-base\""
"Setting global tokens in BigBirdModel"
"convert_graph_to_onnx.py failing to run on Wav2Vec2 models"
"gpt2 causal mask to skip learning context input? (beginner question)"
"Extractive summarization pipeline"
"Release `utils/style_doc.py` as a python package"
"[Pegasus][tokenizer] pegasus tokenizer doesn't have any BOS token?"
"BartForConditionalGeneration: `decoder_input_ids` should not be computed if `decoder_inputs_embeds` is set"
"How to fine-tune a model with my custom tokenizer?"
"Mismatch between tokenizer and model in pipeline"
"AttributeError when using custom IterableDataset with set_epoch method"
"How to construct a pretrain by myself using Tensorflow2 +Keras?"
"Sources of randomness for Longformer"
"Text Classification on GLUE on TPU using Jax/Flax : BigBird"
"checkpoints are not saved after implementing a custom loss "
"Tensorboard error while running mlm_flax TPU example script on TPU"
"GPT2: discrepancy between `inputs_embeds` and `input_ids` when the input sentence has length = 1 "
"Fine-tuning t5-large: greedy predictions don't match teacher-forcing results"
"Get core dump after import FlaxRobertaModel"
"Bug in MLM script not parsing arguments properly + lack of warning on incorrect args"
"Flax MLM example script has PyTorch dependency"
"Feature Request: Flax Encoder-Decoder Model"
"`max_steps` would not override `num_train_epochs` when training with IterableDataset"
"text-classification example broken; version issue / inconsistency issues"
"jax hybrip_clip error ( numpy  is not a valid JAX type )"
"model.generate does not work when using a FlaxGPTNeoForCausalLM model in PT (flax-community-event)"
"Creating Flax VisualBert based on Flax Bert"
"Error while running rn_clm_flax.py training script"
"[Flax] Non working model when exporting to Huggingface"
"Our Model works well in the local ! But when we upload that it doesnt work showing irrelevent performance ! "
"`transformers-cli env` doesn't work out-of-the-box on v3-8 TPU"
"Loading FlaxHybridCLIP trained model "
"Can hidden states be passed instead of input_ids or inputs_embeds in Transformers OpenAI GPT2?"
"Keep getting an OOM when doing a evaluation"
"How much GPU memory is required for DistilBERT?"
"Really long training time on bert fine-tuning for classification"
"[Examples][Flax] AttributeError: 'DataTrainingArguments' object has no attribute 'test_file'"
"FlaxRobertaModel.from_pretrained does not load weights correctly"
"Load Trainer state"
"Is there a Bert version of the OpenAIGPTLMHeadModel?"
" run_clm_no_trainer.py  ModuleNotFoundError"
"Flax Save/Load from base model with different name"
"The \"additional_special_tokens\" argument in the \".from_pretrained\" method of the tokenizer is not necessarily taken into account."
"[Flax] from_pretrained does not consider the passed dtype"
"Extend the testing of tokenizers that just have a legacy version"
"Attempting to load non-existent vocab files from cache leads to a breaking behavior offline (while non-breaking online)"
"How to make BART infill unmasked deletions (not masked tokens)?"
"ModuleNotFoundError: No module named 'transformers'"
"[Flax] Error converting model to PyTorch from Flax"
"Necessary resources for training a (small/tiny) LM from scratch?"
"Get Start with CamembertForSequenceClassification"
"TypeError: cannot pickle '_LazyModule' object"
"`model_name_or_path` does not seem to load in previously trained checkpoints"
"Issue converting Flax model to Pytorch"
"Slow gpt2 training on TPU with run_clm_flax.py"
"Cached data not checked for integrity"
"Issue in terms of accuracy of onnx converted models of SQUAD based ROBERTA  on legal domain."
"Pegasus from Pytorch to tensorflow"
"Can't Select Specific GPU by TrainingArguments"
"AutoTokenizer not loading gpt2 model on instance without internet connection even after caching model"
"push_to_hub related issues (from Google Colab)"
"PEGASUS using ONNX"
"HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Max retries exceeded"
"Summarization failure \"All images are copyrighted\" for certain text inputs"
"tuple index out of range for FlaxMBartForConditionalGeneration"
"ImportError: cannot import name 'LineByLineTextDataset' from 'transformers' (unknown location)"
"Unable to quantize Google's LaBSE model using convert_graph_to_onnx.py"
"ViT doesnt use tokenizer, yet shown as example transformer website"
"AttributeError for DataCollatorForLanguageModelling with tokenizers.Tokenizer"
"[Flax]Not able to Run Hugging Face GPT2 model for jax on TPU's"
"Error when running wav2vec2 embeddings"
"OOM during saving step"
"'MT5Tokenizer' is not defined (on Google colab) "
"Git LFS bug when uploading to hub"
"XLM-RoBERTa NER extraction breaks/splitting the words !"
"GPT-2 asking for Padding Token"
"`tokenizer.special_tokens_map` has stringified list for \"additional_special_tokens\" value."
"Custom tokenizer from Tokenizers library"
"Cannot load .pt model using Transformers"
"How to transfer fine-tuned model from python to rust?"
"Facing Issue while loading pytorch model as flax model"
"\ud83d\udc1b `model_init` fails when its a partially evaluated funtion."
"Remote process received SIGTERM on 96 core  tpu-vm during group_text map on datasets"
"Unable to load mT5 with FlaxAutoModelForSeq2SeqLM"
"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.HalfTensor [12, 4096, 1]], which is output 0 of ViewBackward, is at version 1; expected version 0 instead."
"Weird outputs by `opus-mt-en-es`"
"validation metrics not being logged by Trainer"
"can't pickle <class 'types.AutoModelForCausalLM'>"
"unclear `prepare_seq2seq_batch` deprecation"
"Inconsistent shapes between value and initializer for parameter: FlaxGPT2LMHeadModel"
"can't load flax weights in PyTorch if flax model is saved with dtype `bfloat16`"
"Add Flax Models to Pipelines"
"GPTNeo Error Attempting to Generate Text"
"How much of an improvement is DistilGPT-2 over an equivalent model trained without distilation?"
"TypeError: forward() got an unexpected keyword argument 'label' in main tutorial"
"Vocab Size does not change when adding new tokens"
"Error pushing GPT2 flax training model to hub"
"Long-Short Transformer"
"Error on training XLNet. RuntimeError: CUDA error: device-side assert triggered"
"Slower training speed under DeepSpeed"
"USE_TORCH while import transformers forever true"
"\"token_type_ids\" is discarded when using GenerationMixin in \u201dgeneration_utils.py\u201c"
"Adding an argument to exclude some states (pretrained weights) from being loaded."
"`TFHubertModelTest.test_model_from_pretrained` is failing"
"`TestMarian_MT_EN::test_batch_generation_mt_en` Failing due to randomly generated tokens"
"Inconsistency between the tokenization of `CLIPTokenizer` and `CLIPTokenizerFast` with `openai/clip-vit-base-patch32`"
"TF TransfoXL doesn't work with the `generate` method"
"Autotokenizer error \"Already borrowed\" when used on thread pool"
"Can't load pretrained model when working in virtual environment"
"'TransfoXLLMHeadModelOutput' object has no attribute 'loss'"
"word_ids() returned by RoBERTa Tokenizer behaves inconsistently for alphanumeric tokens like '18th'"
"translation with identical source and target language, for text normalization"
"Vocab size difference between tokenizer and config for XLMR."
"Converting fairseq roberta to transformer throws ModuleAttributeError: 'RobertaHubInterface' object has no attribute 'args'"
"Too Many kernels and embeddings were randomly initialized when loading Hugging Face GPT-2 Model"
"Nothing"
"Processing custom wikipedia data with clm training script throws error when \"blockifying\" data"
"Mask prediction does not work with whitespace  before mask token"
"Running out of memory when resume training."
"Flax - Loading pretrained model overwrites weights of different shapes"
"confusing description in prepare_seq2seq_batch of MBart"
"[trainer] `--load_best_model_at_end` silently turns of `--save_steps` settings"
"No docs for v2.3.0"
"[doc] parallelism - when to use which mode"
"OSError: Not found: \"/root/.cache/huggingface/transformers/5ec31591d9130cc9be0872e6b3dc0b276e514ab96e68404ac4a876ff03cb413b.dbd4bc2544d5c9f8f0d109844726c1600fa95cf0ba770b54c146f702be6e55dc\": No such file or directory Error #2"
"Strange output from summarization models "
"SystemError: <built-in method run_backward of torch._C._EngineBase object at 0x7f06bfae6b30> returned NULL without setting an error> ```"
"[Examples]Flax Seq2Seq example fails when doing only eval or predict"
"Doc - expecting `push_to_hub` method for Tokenizers to be also in the Tokenizer class doc pages"
"Examples/flax/run_clm_flax.py showing error file extension error for train_file attribute even though file has the correct extension"
"Where is the casual mask when using BertLMHeadModel and set config.is_decoder = True?"
"Convert model from flax to TF"
"[Bug?] question answering - end position of each input is weird"
"Error while performing eval on clm using gpt2 in flax"
"layoutlm TokenClassificationPipeline"
"[testing] failing tests/deepspeed/test_deepspeed.py::TrainerIntegrationDeepSpeed::test_stage3_nvme_offload"
"[Flax] Change all `shift_tokens_right` to numpy code"
"Unrecognized configuration class GPT2Config for AutoModelForSeq2SeqLM | Microsoft DialoGPT no longer working"
"Getting incompatible shapes when using global_attention_mask in TFLongformerModel"
"How can I generate sentencepiece file or vocabulary from tokenizers?"
"Checkpoints are saved multiple times during hyperparameter tuning / How to get the best model?"
"Adding a Wav2Vec2ForSpeechClassification class"
"Not able to load the custom model after training in Hugging Face"
"Doctest Integration"
"Change create_model_card to use best eval_results when args.load_best_model_at_end==True"
"Blenderbot output logits dimensions mismatch"
"How to finetune mT5"
"hyperparameter search requirements/gpt2 metric"
"Getting Word Embeddings for Sentences using long-former model?"
"Add EncodedInput as an alternative input type of _batch_encode_plus in tokenization_utils_fast"
"ValueError: cannot reshape array of size ... in run_t5_mlm_flax.py data_collator"
"unk_id is missing for SentencepieceTokenizer"
"Embedding layer Pruning implementation"
"Unable to run model parallel training using jax on TPU-VM"
"t5 fast tokenizer save_vocabulary fails without sentencepiece file"
"IndexError: index out of range in self"
"Error in HuggingFace Course \"Fine-tuning a pretrained model\""
"How to override model.generate() function in GenerationMixin class?"
"Alphafold 2.0"
"Seq2SeqTrainer Model parallelism with AWS Sagemaker - not enough values to unpack error"
"GPTNeo Flax - crashes - n> sizes_size"
"We made a toolkit can parallelize almost all the Hugging Face models. But we have some question !"
"Can't load config for [community model] : DeepESP/gpt2-spanish "
"max_length parameter in Wav2Vec2FeatureExtractor doesn't affect"
"Adding m2m100 12B"
"How to use the transformers pre-training model to calculate the probability of a sentence instead of PPL?"
"Error in the model card page"
"Unable to finetune RAG end2end due to error in finetune_rag.py file"
"Documentation of longformer model is confusinng"
"Set dropout for ClassificationHead"
"How to solve the CUDA out of memory"
"Pre training problem, please help me out"
"How to use past_key_values in RAG model?"
"Add ONNX export for gpt_neo models"
"Raise exceptions instead of using assertions for control flow"
"Tapas tokenizer"
"Declaring `classifier_dropout` in model config but not using it"
"Feature Request: El-Attention"
"The network does not change if I resume training from checkpoint while also providing custom optimizer and scheduler"
"[Documentation] Improve docs for hyper-parameter search"
"Error while converting a RoBERTa TF checkpoint to Pytorch"
"Jax/Flax Text-Classification Examples are not working..."
" I can not find transformers v4.9.0"
"Very strange Training Data Loss Pattern when fitting MT5 for Summarization"
"What is the data format of transformers language modeling run_clm.py fine-tuning?"
"Model Request: Blenderbot 2.0"
"https://huggingface.co/facebook/detr-resnet-101-panoptic model has 250 classes ?"
"minor mistake in the documentation of XLMTokenizer"
"DebugUnderflowOverflow crashes with Multi-GPU training"
"tensor size mismatch in NER.py"
"Converting a tensor to a python boolean might cause the trace to be incorrect. We can't record the data flow of python values"
"Any example to accelerate BART/MBART model with onnx runtime\uff1f"
"label list in MNLI dataset"
"Can't use padding in Wav2Vec2Tokenizer. TypeError: '<' not supported between instances of 'NoneType' and 'int'."
"TFBertModel much slower on GPU than BertModel"
"Allow the use of tensorflow datasets having UNKNOWN_CARDINALTY for TFTrainer"
"VisualBert ValueError: visual_embeds can not be of class 'NoneType' when running on text only"
"Error when doing `push_to_hub` two times in a row"
"Incorrect Tokenization behavior when working with Hindi using RobertaTokenizer"
"Flaky tests"
"unable to load cache when network is unavailable"
"`transformers-cli` fails out of the box"
"How to use the parameters of a certain layer"
"Set Experiment Name in `MLflowCallback`"
"FlaxGPT2LMHeadModel doesn't fail on mismatch between tokenizer and model vocab size"
"Generate text from inputs_embeds for seq2seq models like BART or T5"
"Add Model Details for Pipeline API"
"Default process group has not been initialized while using sagemaker data parallel"
"legacy finetune with t5 issues"
"run_mlm_no_trainer.py requires accelerate but not in requirements.txt"
"run_mlm_no_trainer.py requires --model_name_or_path"
"Got `ONNXRuntimeError` when try to run BART in ONNX format"
"How to ignore PAD tokens for NER"
"How could I convert output tensor from transformer to text generation?"
"TypeError: '>' not supported between instances of 'NoneType' and 'int'"
"wav2vec pretrain and fine-tune with huge data"
"Cannot import pipeline after installation"
"Asking for consent to publish `_LazyModule` as a standalone PyPI package on GitHub"
"BatchFeature should cast to `np.float32` by default"
"[Wav2Vec2] Slow pretraining tests are failing on CPU"
"[Speech2Text] Slow tests are failing on master"
"[MPNet] example of fine-tuning MPNet language model on domain specific corpus"
"Possible bug in spm-based tokenizers"
"MT5-base tokenizer can't decode to target language after decoding"
"I donnot want print trainer's logging info"
"Bart Generation"
"Allow multilabel classification mode for widgets in the models repo"
"Possibly wrong API documentation for BigBirdTokenizerFast "
"Finetuning GPT-2 on small datasets"
"Model card updated/deleted"
"New transformers.onnx CLI does not support ONNX quantization"
"run_mlm.py errors when running validation only"
"Trainer accumulates logits"
"Feature Request: Add support for --do_train/eval/predict arguments in the TF examples script for token classification"
"RoBERTa: Truncation error: Sequence to truncate too short to respect the provided max_length"
"Tensorflow GPT-2 model incapable of freezing layers"
"loss sudden increase"
"Distributed TPU training with run_mlm duplicate data "
"Super slow ByT5 Tokenizer"
"an unexpected keyword argument 'output_signature'"
"Add config option to skip 1-D position embeddings in LayoutLM"
"Multi-GPU fails "
"CANINE pre-training"
"tokenizers add_token bug"
"Tensorflow Mixed Precision Training"
"pipeline does not load a (local) model"
"ValueError: Outputs values doesn't match between reference model and ONNX exported model"
"transformers.__spec__ returning None. Causing downstream import errors"
"The Unsupervised denoising training example in T5's doc"
"AttributeError in BERT-Tokenizer"
"Can't set attention_probs_dropout_prob in LEDConfig"
"Training Transformer XL from scratch for CLM"
"Truncating the prefix of a sequence rather than the suffix"
"memory crash with large dataset"
"saved checkpoint for best model and last model needs to be different "
"fill-mask pipeline with tables (TapasForMaskedLM) fails DataFrame type assertion"
"Tokenizer from tokenizers library cannot be used in Trainer"
"Add callback method for substeps during gradient accumulation."
"LEDForSequenceClassification and LEDForQuestionAnswering example codes don't work."
"GPT2 Layers"
"Transformers onnx export error"
"Feature request: Show command line argument defaults"
"How to reproduce XLNet correctly And What is the config for finetuning XLNet?"
"Misleading warning when using DPRContextEncoderTokenizer"
"How to fuse copy mechnism into the GenerationMixin\uff1f"
"Error when trying `push_to_hub` for a fine-tuned model on Colab"
"[Wav2vec Pretrain] KeyError: \u2018attention_mask\u2019"
"Better error message? `CUDA error: CUBLAS_STATUS_ALLOC_FAILED`"
"`PretrainedTokenizer.return_special_tokens` returns incorrect mask"
"Not able use TF Dataset on TPU when created via generator in Summarization example"
"Starting today, I get an error downloading pre-trained models"
"OSError: Can't load config for 'bert-base-uncased"
"trainer is not reproducible "
"rum_mlm crashes with bookcorpus and --preprocessing_num_workers"
"Transformers tokenizer pickling issue using hydra and submitit_slurm"
"ImportError: cannot import name 'BigBirdTokenizer' from 'transformers'"
"BertForQuestionAnswering result not match when multiple run in same input"
"[end2end rag] Slow speed when extending the external KB"
"ZeroDivisionError in NotebookProgressBar.update with small dataset"
"Wav2Vec2 WER remains 1.00 and return blank transcriptions."
"404 Error when loading pretrained model, after finetuning"
"Weird behavior with mBART-50 and Spanish"
"huggingface-hub version conflict"
"`Trainer.evaluate()` crashes when using only tensorboardX"
"Using `model.sample()` and increasing the `max_length` leads to CUDA OOM crash"
"Bugs when fine tuning the gpt2"
"Workaround for training models with really big text files"
"Unable to convert output to interpretable format"
"403 error in colab to download tokenizer"
"`Trainer.train(resume_from_checkpoint=False)` is causing an exception"
"[FLAX] Potential bug in CLM script when using text files"
"Control sequence length for Token Classification with Trainer"
"Validation and Evaluation not cumputed in run_qa.py"
"Documentation: Dataset to Model interface examples"
"tapas-base model is not predicting answers well."
"Fine-Tune Wav2Vec2 for English ASR with \ud83e\udd17 Transformers, loading fine-tune models from local isn't working"
"subclassing a torch.utils.data.Dataset object for a T5 model"
"convert_graph_to_onnx.convert broken for gpt-neo-x.xB since 4.5.0.dev0"
"The transferred onnx model is much bigger than the origin pytorch model"
"pylint error when using `transformers.AutoModelForSequenceClassification.from_pretrained(path)`"
"Training hangs at the very start while using deepspeed"
"kindly adding some documentations on t5-v1_1-base\"\""
"How is Bert fine-tuned on STS-B task?"
"I met an error when I use EncoderDecoderModel."
"Gloabl attention not recognised in longformer pretrained MLM model to get sentence vector?"
"Option for `(Distributed)LengthGroupedSampler` to treat groups as a hard constraint"
"Perceiver IO"
"how to user class_weight in transformers.trainer"
"DataCollatorForWholeWordMask does not return attention_mask"
"Newly trained tokenizers not adding [CLS] and [SEP] tokens"
"VisualBERT - ModuleAttributeError"
"Not getting the same results with run_qa and run_qa_no_trainer scripts"
"HyperParameter search in sagemaker"
"Importing hides underlying error"
"Problem saving tf wav2vec in savedmodel format"
"The traced Encoder of LEDForConditionalGeneration does not allow dynamic batching"
"Unable to resume training from checkpoint on TPU v3-8"
"GPU Out of Memory when repeatedly running large models (`hyperparameter_search`)"
"RobertaForMaskedLM loss calculated wrong(?)"
"TypeError: __init__() got an unexpected keyword argument 'save_strategy'"
"MT5-large model on hub has wrong config"
"Get multiple results from Hugging face pipeline library"
"supporting t5 for question answering"
"How can I convert a `checkpoint.pth` (a model trained with pytorch-pretrained-bert) to huggingface model with `config.json` and `pytorch_model.bin` file?"
"Masked word prediction in new language with mBERT/XLM"
"Getting near constant training loss, T5 not learning anything?"
"transformers-cli depends on torchaudio optional deps"
"Rotate checkpoint `shutil.rmtree(checkpoint)` fails"
"Do the Trainer docs need an update?"
"Spanish NER bad extraction"
"Check in PreTrainedTokenizer can cause incorrect tokenization"
"Script to convert the bart model from pytorch checkpoint to tensorflow checkpoint"
"[DeepSpeed] DeepSpeed 0.4.4 does not run with Wav2Vec2 pretraining script"
"MLM example not able to run_mlm_flax.py"
"TFBertPreTrainingLoss has something wrong"
"How do i pre-train Bert_mlm model [Discussion]"
"Is there any way to train a transformer model from scrat"
"Is there any convenient way to train a transformer from scratch ?"
"Non-English characters not fully supported by GPT-2 HF model"
"RAG: building my own dataset"
"Exporting Fine tuned T5ForConditionalGeneration model to TF-Serving using ONNX"
"Cannot import name 'BEiTForImageClassification' from 'transformers' "
"How to extract the encoded data of feed & forward layer in tfbertmodel?"
"Can not instantiate `PreTrainedTokenizerFast` from instantiated tokenizer object"
"MultiBerts in Huggingface"
"top-k sampling for Flax models"
"t5 base not found."
"Custom Seq2Seq translation model training exits with error"
"respect dtype of the the model when instiating not working"
"[Vision Transformers] examples and pretraining"
"location is currently not available...please share the exact locationDetailed Explanation"
"I modified https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py script few days ago for training electra from scratch. But there were some problems(maybe bugs) i had to solve for this task."
"\u3084\u304b\u3089\u3093"
"Missing `lm_head` parameter in FlaxGPT2LMHeadModel.params"
"can\"t connect ther online datasets.the issue:ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py"
"[Benchmark]"
" AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'  in translation.ipynb notebook"
"Memory accumulation when using hybrid clip script"
"inconsistency of the last element in hidden_states between PyTorch/Flax GPT2(Neo)"
"Multi Lang Marian Translator not working (opus_mt_mul_en)"
"`ModelError` when calling SageMaker Endpoint for prediction using the official notebooks"
"typeerror: textinputsequence must be str"
"Problem about using mBART50 for Russian to Chinese translation"
"Can we directly replace gpt2LMHeadModel with BertLMHeadModel to see bert's performance? #7"
"AutoModel KeyError: 'layoutlmv2'"
"Electra raises UnboundLocalError: local variable 'seq_length' referenced before assignment when inputs are pre-computed embeddings"
"Value error while running run_glue.py example with gpt2 "
" You must login to the Hugging Face hub on this computer by typing `transformers-cli login` and entering your credentials to use `use_auth_token=True`. Alternatively, you can pass your own token as the `use_auth_token` argument in the translation notebook."
"type object 'AutoModelForSequenceClassification' has no attribute 'from_config'"
"torch.jit.trace quantized bigbird leads to 0INTERNAL ASSERT FAILED runtime error "
"RuntimeError: Error(s) in loading state_dict for BeitForImageClassification: \tsize mismatch for classifier.weight"
"dtype"
"how to finetune or test XLM-ProphetNet on XGLUE-NTG task"
"Implement a `batch_size` parameter in the `pipeline` object"
"Pretrain BART MNLI model on Financial Phrasebank"
"Runtime error when training DetForObjectDetection using HFTrainer with GPU."
"Support OpenNMT models"
"Slow tokenizers return overflowing tokens in reversed order"
"Autoregressive differentiable decoding? (no teacher forcing nor self-reconstruction)"
"Unhashable type : dict for visualbert example code."
"AttributeError: 'AlbertModel' object has no attribute 'bias' -Transforms 4.9.2"
"export BART model to ONNX failed with [Segmentation fault (core dumped)]"
"CvT: Convolution based Image Transformers"
"Advice needed: Adding more FSMT models"
"Cannot run  run_mlm.py on a Japanese dataset - AttributeError: module transformers.models.mbart50 has no attribute BertJapaneseTokenizerFast"
"Fine-tuned Robust Wav2Vec 2.0 models"
"is there any <SOS> or <EOS> token in reformer-enwik8?"
"Missing weight in pretrained model `pegasus-xsum`"
" Performance issues in the program"
"Issue with `Speech2TextFeatureExtractor` method `from_pretrained` and `from_dict`"
"RobertaTokenizerFast object has no attribute '_convert_token_to_id'"
"Using `bf16` instead of `fp16`"
"[Docs] Function signatures on website not correctly reflecting current code."
"No module named: Regex while importing GPT2Tokenizer"
"enable mixed precision for Tensorflow training benchmarks"
"[Benchmark]"
"GPT-Neo ONNX Inference with past is broken"
"GPT2 error when we try to run torch.jit.script "
"Bug of PyTorch group_beam_search function"
"how to finetune based huggingface: run_glue.py"
"Conversion of Wav2vec2 model to TFWav2vec2 model"
"T5TokenizerFast not reversible when text contains special tokens"
"Unable to load model by ignoring size mismatch; TypeError: __init__() got an unexpected keyword argument 'ignore_mismatched_sizes'"
"Question about xla_spawn.py script and torch_xla.distributed.xla_multiprocessing"
"[Documentation] PLEASE HELP with very simple tasks!!!"
"Why repeat initializing loss modules in every forward?"
"Incosistent behaviour between fast and slow RoBERTa tokenizers"
"Megatron conversion code converts some weights in fp16 to fp32(or uint8)."
"'torch_dtype' keyword not working with 'AutoModel'"
"Training DetrForObjectDetection failed in a multiple-GPU environment."
"How to use transformers for batch inference "
"Some tokenizers are not really picklable"
"Train Bart model only use one cpu core, Any solutions to use more cores?"
"-100 when calculating perplexity of a model.."
"How do i get the CLS token from the model output?"
"[Optimization] AdaFactor not working on TPU but works on GPU."
"CausalLM vs HeadModel"
"Loading of a model takes much RAM, passing to CUDA doesn\u2019t free RAM"
"Questions on generating using encoder-decoder models"
"Input to a Tensorflow model where a dictionary cannot be used"
"How to run GLUE tasks on my model?"
"\"Resource exhausted\" when loading Flax GPT-Neo 2.7B"
"Typo in M2M100 1.2B model card page, strange translation results and new M2M100 615M model"
"Unable to load 'rembert' checkpoint "
"make test failing"
"Can't install transformers in Conda environment with python 3.9"
"Bert Loses Patience - Batch Inference Doubt"
"codecarbon plugin issues"
"run_translation_no_trainer with MBart: unsupported operand type(s) for /: 'dict' and 'int'"
"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at 'yyy' and are newly initialized"
"Error while trying to run run_wwm_mlm.py using my saved model: TypeError: \u2018NoneType\u2019 object is not iterable"
"BeitForMaskedImageModeling forward not using bool_masked_pos"
"Upgrade `os.path` to use `pathlib.Path` API for `from_pretrained` internals"
"Correct way to use pre-trained models - Any document on this?"
"BERT finetuning \u201cindex out of range in self\u201d"
"Can we use trainer api with custom model layer?"
"Tapas tokenization Different from Tensorflow Code"
"Add ability to include additional model card info within Trainer"
"[model loading] framework-agnostic dtype parameter"
"Why do we need to use `Loss.repeat(eval_batch_size)` in accelerator gather loop? "
"how to finetune mT5 on XGLUE-NTG task"
"Add `--max_length` argument in seq2seq trainer."
"Cannot use RemBert"
"\u8bf7\u95ee\u4e00\u4e0b\uff0c\u60c5\u611f\u5206\u6790\u7684\u6a21\u578b\u53ea\u662f\u9488\u5bf9\u82f1\u6587\u7684\u5417"
"Label Smoothing for Question Answering task"
"ingore_mismatched_sizes Wav2Vec2 unknown argument"
"Printing weights of a pre-trained model "
"Revisions not working as expected"
"additional global attended token in bigbird-roberta"
"Commit v4.9.2 release appears as v4.5.1 in \"transformers-cli env\""
"Docs: TrainingArguments call incorrect"
"`pipeline` backed with ONNX Runtime and quantization for faster inference"
"Question about bart-base model"
"Pretraining T5-v1_1 on Flax"
"GPT2 for classification - Errors encountered while running run_glue.py and (possible) fixes"
"torch longformer to tf longformer"
"DistilBertTokenizer for distilbert-base-multilingual-cased is unable to encode / decode Japanese characters properly adding unnecessary characters in between"
"GPT2 model state dictionary Tensor types are not matching with pytorch"
"__version__ attribute missing in mode config for sentence-transformers/paraphrase-mpnet-base-v2"
"Examples: label mapping for text classication tasks are not written into configuration"
"Support for converting LayoutLM to ONNX"
"Missing on_predict event in TrainerCallback"
"[Feature request] Introduce GenericTransformer to ease deployment of custom models to the Hub"
"Having problem Pre-training GPT models"
"How to use the pretraining task of ProphetNet"
"Errors when fine-tuning RAG on cloud env"
"DestilGTP2 code from pytorch-transformers does not work in transformers, I made a basic example"
"Documentation mismatch in Preprocessing data"
"Handling tag with no prefix for aggregation_strategy in TokenClassificationPipeline"
"Wav2Vec2ForCTC is not BaseModelOutput"
"Wrong weight initialization for TF t5 model"
"Licenses for Helsinki-NLP models"
"GPT-J-6B in run_clm.py"
"model(**batch) returns loss dictionary that cant be divided by gradient_accu"
"bert:What is the tf version corresponding to tensformers?"
"bug in gpt2 notebook (in tensorflow)"
"T5 - Flax - Decreasing performance on pretraining"
"Padding labels is wrong when using `pad_to_multiple_of`"
"How to use BertForSequenceClassification for the Apect Based Sentiment Analysis"
"Bert (sentence classification) output is non-deterministic(have checked previous issue, SET model.eval() )"
"Predicted Start_index < Predicted End_index in BertForQuestionAnswering"
"Cannot run grid search using Trainer API and Ray Tune"
"Does Bart Model can fill <mask> with variable length?"
"Dependency parsing head for pretrained models"
"Import of transformers package throwing value_error"
"Unexpected weights were not initialized from the model checkpoint error "
"CTRL's `config.json` on HF Hub is missing a `model_type`"
"Hyperparameter search function not working with Trainer and mlflow"
"Which files are essential when customize and modify a specific pre-trained model\uff1f"
"[Consistency] Make sure all xxxForSequenceClassification models support problem_type"
"wav2vec2-large-xlsr-53 Tokenizer unable to load"
"AttributeError: '_LazyAutoMapping' object has no attribute '_mapping'"
"TRAINING CUSTOM MODEL USING LAYOUTLMv2!"
"Error using SpecAugment feature masking in Wav2Vec 2.0"
"Zero-shot classification pipeline truncation support"
"Small typo"
"not support Pytorch 1.8.2"
"Hard time installing huggingface for python3.8 cuda10.1"
"How can I convert fairseq checkpoint to huggingface for `XLMProphetModel`?"
"Transformers crashes mypy"
"How to build a custom dataset for LayoutLMv2ForSequenceClassification?"
"TorchScript warning"
"LayoutLMv2Processor padding/truncation issues"
"T5: relative position embeddings"
"PreTrainedTokenizerFast to BertTokenizer"
"Unified freezing interface"
"TrainingArguments default parameters throw error (evaluation_strategy, save_strategy)"
"about 'text-generation': how can I generate sentences with multiply words? "
"How to use multiple PreTrainedModel models in a custom model?"
"git.exc.InvalidGitRepositoryError when running finetune_rag.py"
"AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained'"
"Sentencepiece Unigram tokenizer add tokens"
"Possibility of disabling add_pooling_layer that works for all models"
"13134"
"RuntimeError: Unknown: CUDNN_STATUS_EXECUTION_FAILED"
"No log output to console"
"Cannot Replicate xlm-roberta-large-xnli Results"
"JAX/Flax models should be `jax.jit`ed by default? Or code examples should use jax.jit (~200x speedup)"
"Huggingface Inference API"
"Error with T5 model: Output is always getting truncated with 20 tokens"
"[Benchmark]"
"convert pytorch checkpoints to TF1.x checkpoints (reverse of transformers-cli convert)"
"where processor should i put in a training code?"
"Please add GPT Jaaye model ine right with Transformers website"
"Illegal Instruction Error on  `prepare_inputs_for_generation`  -> gpt neo/ j"
"Difference between `logit_scale` initialisation in Transformers CLIP and the original OpenAI implementation."
"Bb"
"A dead link in GPT2 description"
"Official example not working, are you serious?"
"using TFOpenAIGPTLMHeadModel load pytorch model doesn't work well"
"MisconfigurationException: Found invalid type for plugin None. Expected a precision or training type plugin."
"Could you support other distributed training backends from command line, I'm using GLOO."
"Unexpected result when tokenizing a single token rather than a sentence with multilingual tokenizer"
"Error while saving a variation of roberta-base fast tokenizer vocabulary"
"Adding GPT2 for translation/summarization"
"T5Tokenizer.from_pretrained(\"t5-small\") returning NoneType"
"Multi-GPU memory usage"
"Can I use input embeds to generate text without input ids"
"Translation takes too long - from fine-tuned mbart-large-50 model"
"Add `tokenizer_max_length` to `cardiffnlp/twitter-roberta-base-sentiment`"
"when I overwrite a model,I meet some bugs like this"
"In ViT model,  last_hidden_state is not equal to hidden_states[-1]"
"A Space Always Prefixes The First Token of `xlm-roberta-large` Encoding Results"
"Upcasting of attention computation for reliable pretraining of GPT-2 models"
"Non-BERT dpr tokenizers"
"BertTokenizerFast can not tokenize [MASK] to 4 id"
"[Errno 13] Permission denied: '/.cache'"
"LayoutLMv2 forward pass fails during pytorch.jit.trace forward pass"
"replacing model head failure"
"T5"
"Difference in BART position embeddings with fairseq implementation"
"`BertTokenizer` splits tokens added via `add_tokens(...', special_tokens=False)` to a few subtokens "
"push to hub issue"
"Enable passing config directly to PretrainedConfig.from_pretrained()"
"Cannot run Movement pruning on GLUE"
"add non auto regressive model?"
"Rouge Metric Evaluation in Training/After Training T5 run_summarization.py "
"FlaxCLIPModel is 10x faster than CLIPModel during inference, and 100x slower getting gradients?"
"early_stopping_patience_counter increasing + 2 per epoch in distributed mode"
"Torch size missmatch in GPT-J model (Error)"
"Loading mt5-xxl rasies error related to Pytorch/TF incompatiblity"
"Wav2vec2Processor normalization issues on transformers 4.10.0"
"Insufficient memory occurs during finetune"
"Error in code"
"[Benchmark]Why 'step' consume most time?"
"Huge bug. TF saved model running in nvidia-docker dose not use GPU."
"Loading SentenceTransformers (DistilBertModel) model using from_pretrained(...) HF function into a DPRQuestionEncoder model"
"CodeT5"
"The new impl for CONFIG_MAPPING prevents users from adding any custom models"
"FlaxCLIPModel memory leak due to JAX `jit` function cache"
"ONNXRuntimeError]RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node."
"T5 support for text classification demo code"
"Trainer's create_model_card creates an invalid yaml metadata `datasets: - null`"
"Joint Sequence and Token Classification"
"Package transformers.onnx should handle Tensorflow"
"[SequenceFeatureExtraction] Move padding logic from pure python to numpy"
"Can i covert a MarianMT Model to tensorflow lite model?"
"Load BERT as DPRQuestionEncoder using from_pretrained method"
"RAG issue"
"train loss is not decreasing using TFBertModel"
"RFC: split checkpoint load/save for huge models"
"QUESTION: How to perform 2D interpolation of pre-trained position embeddings for fine-tuning on Vision Transformers?"
"`prediction_loss_only` = False returns `float.detatch()` error witrh HF trainer"
"LayoutLMv2 processing doesn't handle tokenizer overflow"
"Mismatch of implementations of attention mask in transformers and tokenizers "
"How to use transformers pipeline with multi-gpu?"
"Internal links in README.md tables are broken"
"m2m100 conversion failed from fairseq to hf format"
"sentencepiece version need upgrade"
"CANINE model in huggingface transformers performs worse than mBERT?"
"Marian Encoder's last hidden states from MarianMT and TFMarianMTModel don't match"
"Log metrics during training"
"BART"
"single_word option of new tokens are disabled by save_pretrained when we save and reload a tokenizer twice"
"Q About BART bart-large-cnn"
"Custom GPT2 Model won't load after training"
"Trainer error when finetuning Pegasus - cannot import name 'amp' from 'apex' "
"gpt-j input shape after finetuning"
"fix-copies doesn't work well in some cases"
"Option to serialize tokenizers in memory instead of to a directory"
" Unrecognized configuration class <class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM."
"ImportError: cannot import name 'auto_class_factory'"
"distributed training not starting"
"run_summarization.py freezes "
"Inconsistency in  class naming"
"seq2seq trainer gives OOM error while evaluating"
"Installing transformer on docker"
"Push to hub fails to \"update ref\" for GLUE example script"
"how to get 8 hidden layer in bertonnx"
"Adding license file to some of the fine-tuned models "
"Manual download of a pytorch_model.bin results in a zip file"
"`Trainer` loads model weights twice when resuming from checkpoint"
"How to use model.save() in tf2 when using TFBertModel"
"ProphetNet generation inconsistent with batch size?"
"Why we use truncated normal initializer instead of the default glorot_uniform in the tfbert?"
"DeepSpeed: There is no obvious benefit from increasing the number of GPU nodes"
"How to change type_vocab_size?"
"How to freeze a few layers in t5 model  during fine-tuning"
"CPU memory (VRAM) not released after loading model in GPU"
"TF weights for xlm-roberta-base?"
"ResourceExhaustedError: Failed to allocate request for 64.00MiB (67108864B) on device ordinal 0"
"bug in transformers notebook (training from scratch)?"
"Add the fast implementation of `BlenderbotTokenizer`"
"[typo] invalid URL"
"Dead link in docs"
"lmetric = load_metric(\"sacrebleu\") exception"
"Auto model for conditional generation"
"some error when I finetune wav2vec2 by rum_common_voice.py"
"Use `transformers` models as Spark estimators "
"DeepSpeed and HF trainer return hugely different losses and perplexity"
"Syntax for from_pretrained proxies (downloading model behind corp proxy)"
"Fine-Tuning Wav2Vec2 with PyTorch DDP"
"Finetuning BART on a  multi-input sequence to sequence task"
"Image classification script overrides preprocessing configuration with script defaults"
"T5ForConditionalGeneration.from_pretrained load pytorch *.pt checkpoint fails"
"non-negligible difference between GPT2 and TFGP2"
"LEDForSequenceClassification example throws a ValueError on missing decoder_input_ids/embeds"
"DPR AutoModel loading incorrect architecture for DPRContextEncoders"
"AttributeError: 'T5ForConditionalGeneration' object has no attribute 'linear'"
"retain_graph=True required when using a custom BigBird-based model "
"UnicodeDecodeError while loading pretrained model from AutoModel.from_pretrained()"
"CUDA out of memory even for GPT-NEO-125M"
"bug in movement-pruning"
"pipeline fill_mask.py - needs to convert input_ids to cpu before calling numpy"
"modeling_fnet.py config option "
"Wav2vec2 pretraining"
"New Wav2Vec2 padding has slightly backward breaking changes"
"[RFC] adding Tensor and Pipeline Parallelism to transformers"
"Limit number of checkpoint on `examples/pytorch/summarization/run_summarization.py` same as `save_total_limit` on `Trainer`"
"Wav2vec2 with different tokenizer ?"
"Pipeline \u201czero-shot-classification\u201d gives \u201cTypeError: __call__() takes 2 positional arguments but 3 were given.\u201d"
"Fine-tuning GPT-J 6B with 16Gb of VRAM"
"Error while running GPT-J 6B with revision=\"float16\""
"how to finetune huggingface MarianMT transformer model"
"RunTimeError when using prefix_allowed_tokens_fn and top-k/top-p sampling in model.generate"
"Unable to execute RobertaModel from pretrained"
"Unable to call .from_pretrained() for Roberta Model"
"problem with using past_key_values in T5ForConditionalGeneration"
"Documentation Mistake: no_multi_processing for Benchmarks"
" Unable to load weights .from_pretrained() for XLMRoberta Model"
"SpeechEncoderDecoderModel does not return a loss regardless of labels"
"How can i Use transformers to classify a obfuscated text (multinomial classification)"
"latest TPU VM dies on import of TrainingArguments?"
"PIL and soundfile shouldn't be required to run `transformers-cli env`"
"Error when loading weights with a different 'projection_dim' in DPRQuestionEncoder"
"TypeError in tensorflow/run_summarization.py"
"Obfuscated text classification error when using CANINE Transformers"
"Model inside docker gives different results"
"Fix bug in DebertaForMaskedLM"
"Adding target language token in mBART model."
"Can't save model in saved_model format when finetune bert in tensorflow2"
"using translate notebook for my dataset I get this error : ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
"I want to understand the source code of transformers. Where should I start? Is there a tutorial link? thank you very much!"
"FNet model card"
"Add a method for substring(tokens / ids) checking to tokenizers."
"wav2vec 2.0 loaded from a fairseq checkpoint isn't strictly equivalent"
"Scrolling through the docs has become very slow"
"This problem happend when I train the model"
"TypeError: 'LayerNorm' object does not support indexing"
"incorrect loss calculation"
"A version of Trainer that calculates the eval metrics (e.g. accuracy) on the training set as well"
"Unable to match GPT-2 reported perplexity results on CBT, Wikitext-103, and 1BW datasets and doubt on LAMBADA accuracy"
"ByT5 tokenizer gives indices of chars instead of bytes"
"Hidden states not available in S2T (and small typo in S2T documentation)"
"Size mismatch error while changing the  T5Config class default parameters"
"Link is broken in Huggingface model zoo"
"Connection error, and we cannot find the requested files in the cached path"
"HuggingFace Model Hub (summarisation) - models not working locally (404 not found)"
"Empty prompts failing in dev sources"
"Pretrained Wav2Vec2 Model large robust does not load"
"LED for Seq2Seq output shape mismatch between tensorflow and pytorch"
"ByT5: problem with tokenizer.decode()"
"DDP BERT-Base on SQuaD2.0"
"NotebookProgressBar can't display in Pycharm jupyter"
"Feature: Tail Free Sampling"
"GPT2 (117M) results mismatch on PTB, enwik8 and text8 metrics"
"problem when loading local model"
"is the prediction_logits.size() is correct?"
"DetrFeatureExtractor fails if do_normalize set to false"
"Make from_pretrained support parameters defined in the forward pass"
"LengthGroupedSampler: why longest examples go first?"
"(Distributed)LengthGroupedSampler: allow only providing lengths but not a dataset"
"transformers seems to have recently been \"bricked\""
"Transformer Hosted API Broken"
"[examples/pytorch/image-classification] why does it require `torch>=1.9.0`"
"How to use LayoutLM Tensorflow version on Google Colab?"
"Loading wav2vec2 pre-trained models"
"Some weights of BeitModel were not initialized from the model checkpoint"
"AttributeError when running question answering models for result"
"TypeError: forward() got an unexpected keyword argument 'attention_mask'"
"Device error on TokenClassificationPipeline"
"Weird behavior of BertLMHeadModel and RobertaForCausalLM"
"mT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type"
"Could not load from pretrain even with the same code"
"Add MarianMT to models exportable with ONNX"
"Consistent speech model input names for the Seq2SeqTrainer generate function"
"Tokenizer - Raises wrong \"UserWarning: `max_length` is ignored when `padding`=`True`\""
"Bort always predict wrong words"
"Add on option to output a checkpoint every x minutes"
"AttributeError: type object 'EnglishDefaults' has no attribute 'create_tokenizer'"
"Caching strategy for tokenizers to help achieve 1.46 ms inference on BERT QA on CPU"
"Onnx support for Zero Shot Classification Pipeline"
"AttributeError: 'TFDistilBertForSequenceClassification' object has no attribute 'to'"
"TF mT5 model is not adding new tokens into it's vocabulary."
"Multilingual T5 is not adding new tokens"
"How can one use newly added GPTJ6B model for finetuning?"
"Multi-task learning for MLM and token classification"
"Question: Is that possible to embed a tokenizer into the model for tensorflow serving?"
"How to tokenize big dataset "
"[RFC] Add `modeling_xxx_fusion.py` to support kernel fusion"
"Zero-shot classification pipeline change in 4.11.2"
"Default arguments of clm example are confusing"
"Unexpected behaviour with past_key_values in GPT2"
"Update examples/documentation for torch.distributed.run"
"Issues with FNet"
"there is not more toolkit and model  architecture about tabular format in hugginggface, tabular text in NLP = language + layout"
"Speech2Text2 training support"
"Can I train an Rnd2GPT model through HuggingFace \"Encoder-Decoder Model\"?"
"overflow error when initialising transformerXL model (transfo-xl-wt103)"
"How to export BertForMaskedLM to onxx using onxx export script?"
"Can't use add_prefix_space in RobertaTokenizerFast"
"'list' object has no attribute 'tolist error while using 'deepset/xlm-roberta-large-squad2'"
"Computer shuts down when multiple gpus are used"
"Hyperparameter tuning example code is not working either with Ray or Optuna backend. "
"Model isn't saved on \"save_steps\" "
"[Feature] Add an option to use custom pad token_id in `tokenizer.pad()`"
"GPT-J float16 model output stopping after first word"
"\"test\" prefix for rewrite_logs() function of WandbCallback"
"pre-training roberta from scratch"
"Encoder Decoder Model Generation Issue"
"DataCollatorForWholeWordMask does not work with return_tensors = \"np\" and return_tensors = \"tf\""
"Inference of ONNX exported BART"
"modeling utils don't respect `subfolder` setting when looking up model"
"Cannot run regression with DebertaV2ForSequenceClassification and DebertaForSequenceClassification"
"The Pegasus model generator function ignores the temperature parameter."
"Best model not loaded in Trainer.hyperparameter_search"
"mBART50 finetuned many-to-many model failed to generate translation"
"Error: Attribute does not exist"
"`min_length` in `generate` method"
"Wrong isinstance for AutoTokenizers with unspecified tokenizer_class"
"How to export to ONNX distilbert with question-answering head"
"tensorflow LED global_attention_mask shape mismatch error"
"`from_pretrained` doesn't validate its kwargs"
"Getting out of vocabulary tokens for pretrained models."
"Data collator ignores the input attention_mask"
"input decoder embedding for model.generate()"
"'BertEncoder' object has no attribute 'gradient_checkpointing'"
"rgwwfq"
"[performance doc] add tables with DDP/DP/no-DP benchmarks"
"valhalla/distilbart-mnli-12-3' is a correct model identifier listed on 'https://huggingface.co/models'"
"AttributeError: 'SequenceClassifierOutput' object has no attribute 'pooler_output'"
"'BertTokenizer' object has no attribute 'tokens_trie'"
"Licensing of LayoutLM models"
"Wrong decoder_hidden_states from generate() function of BartForConditionalGeneration"
"Pretrained model download slowly\uff0cIt will speed up when I delete the folder $USER/.cache/huggingface"
"The first inference timing for pure PyTorch is unexpectedly fast and should be ignored"
"Incosistent vocab sizes in t5-base model & tokenizer"
"The tftrainer custom_datasets colab does not work with t5model"
"bert obtained different results on mac and linux"
"Few-Shot Learning Attempt Failure on HF Inference API with GPT-J"
"Documentation for exporting custom architecture to ONNX"
"BART with inputs_embeds crashes with shift_tokens_right"
"log_softmax runtimeerror when utilizing generation gradients in beam_search "
"Training on Tpu got stuck at 0%"
"Add support for `push_to_hub` for `AutoFeatureExtractor`"
"Error In Fine-Tuning MarianMT's opus-mt model ValueError: The two structures don't have the same sequence length. Input structure has length 4, while shallow structure has length 3"
"In pre-training a model how can I resume from last saved model?"
"Feature request : add leave=True to dataset.map to enable tqdm nested bars (and whilst we're at it couldn't we get a way to access directly tqdm underneath?)"
"How to load a fine-tuned model and do predictions?"
"Weird implementation of GPT2"
"Bug?/Question? Vocab of RoBERTa different from GPT2"
"LayoutLMv2Processor does not accept the XLMRobertaTokenizerFast"
"examples/legacy/token-classification\u3000doesn't work well"
"Issues with new LayoutLMv2 "
"```input_embeds``` keyword not working properly (GPT2)"
"input params in RobertaForQuestionAnswering"
"ModuleNotFoundError: No module named 'transformers.models.fnet.configuration_fnet"
"ConversationalPipeline Not Compatible With HuggingFace SageMaker Deploy"
"Parameter max_new_tokens is always overshadow by max_length in model.generate()"
"Tokenizers integration into onnx models \ud83e\udd17"
"Trainer: Cannot train with 3+ GPUs / Uneven Memory Consumption"
"Intel OpenVINO inference backend"
"Logit explosion in MobileBertForNextSentencePrediction example from documentation (and all others tried)"
"CLIPProcessor using only single core"
"Attributes explicitly defined in model configurations are now overridden by the default type."
"Use different data collator for train and eval dataset in trainer"
"[performance/precision] adding `jit.script` to activation functions"
"Saving/Reloading the Flax T5 model"
"Discrepancy in tokenization results using HF's AlbertTokenizer and sentencepiece library"
"How should I try to feed new memory states to the next segments"
"TFEncoderDecoderModel loading TF weights issue"
"Integration.py"
"Bert: relative_key position embedding causes error in encoder-decoder setup"
"[Bug?] Warning about weights not being initialized, even though not part of the model"
"FlauBERT cannot perform MLM with customized tokenizer (added tokens to vocabulary)"
"hf wav2vec2 to fairseq"
"minor issues in modeling_clip.py"
"DeBERTa checkpoints contain `config` keys"
"cannot load character bert"
"[feature request] a tool to clone existing models to make new models with small changes"
"Text Generation Pipeline doesn't take Truncation = True"
"Increase the usage of augmented assignment statements"
"How to untie input and output word embeddings (make embeddings independent) for Bart?"
"Enabling Discussion on github"
"Bug in the Flaubert tokenizer_config.json do_lowercase option"
"Running `run_ner_no_trainer.py` with `--label_all_tokens` falsifies seqeval results"
"TypeError: Inputs to a layer should be tensors. Got: last_hidden_state"
"In `Trainer`, `evaluation_strategy` defaults to `no`, but `save_strategy` defaults to `steps`. Why?"
"with op Tile must be a compile-time constant."
"Unexpected generated token probabilities derived from scores"
"Which is better ?"
"Trainer._load_rng_state() misbehavior"
"AttributeError: 'BertTokenizer' object has no attribute 'encode_plus'"
"ignore_mismatched_sizes do not work propoerly"
"gramformer installation error."
"[Feature Contribution] Disjunctive Positive Constraint Decoding (adding `force_tokens` to `model.generate()`)"
"FLAX-T5 - TPU not found Colab"
"Unexpected sequences_scores in BeamSearchDecoderOnlyOutput"
"Shift operation in loss computation for seq2seq model"
"Different result come from local model and API"
"saving the model after each epoch completion"
"`T5ForSequenceClassification`"
"bert onnx to tensorrt wrong"
"Comments typo"
"T5ForConditionalGeneration `prepare_inputs_for_generation` causes problems for `sample` function"
"Automatic Mixed Precision Support for (some) Flax Transformers"
"GPT-J Models Cannot Load If Tokens Have Been Resized Using resize_token_embeddings Method"
"[deepspeed integration] HF Trainer takes over GPUs for DP"
"LayoutLMv2 model not supporting training on more than 1 GPU when using PyTorch Data Parallel"
"Add a parameter \"device \" in Tokenizer.__call__()"
"[BUG] Tokenizer offset"
"make test fails on `tests/test_benchmark_tf.py` "
"Pipeline seems slower in 4.11+"
"Can deepspeed and gradient checkpointing be used at the same time?"
"Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector."
"DebertaForMultipleChoice"
"[Seq2Seq] (Byt5) zero loss"
"[ASR example] Can't reproduce the same WER with the same seed"
"ImportError: cannot import name 'TrOCRProcessor'"
"wrong cache_dir is used when tokenizer is trying to infer config_tokenizer_class"
"Questions when training language models from scratch"
"OnnxRuntime error \" The input tensor cannot be reshaped to the requested shape.\""
"Examples: Tokenize only max_samples used for train/eval/prediction"
"How to save wrapped DistilBERT without using `save_pretrained`?"
"how to reproduce distilbert pretrain on TF2.x?"
"User-defined callback can't use logging"
"[Trainer] Push to hub takes too much space for local `.git` folder"
"run_speech_recognition_ctc.py throwing error when using own dataset"
"Support for new LayoutLMv2 in Auto Classes (specifically AutoModelForMaskedLM)"
"Decoding Large Audio Files Using Wav2Vec2ForCTC Model"
"Extracting Neutral sentiment from Hugginface model"
"The pytorch example question-answering/run_qa_beam_search.py do not work"
"SEW - Masked Spec errors out in training"
"How to train bilingual models?"
"IBert Problems of hugging face pretrained"
"one of the variables needed for gradient computation has been modified by an inplace operation"
"Pipeline feature extraction: tensor size mismatch"
"Argument 'filter_value' for the _get_logits_warper function"
"Can we save tokenized datasets?"
"ValueError when converting dialogpt to onnx format"
"[Flax] Add Flax implementation of `BlenderbotSmallModel`"
"T5-v1.1 loss go to nan when fp16 training was enabled"
"AttributeError: 'str' object has no attribute 'squeeze'"
"Help training TrOCR"
"[T5v1_1] Add lm model pretrained models as well"
"run_t5_mlm_flax.py "
"Trainer batch size auto scaling"
"Couldn't reproduce DistilBERT downstream tasks performance on SQuAD dataset."
"Add option to not load pretrained weights in `AutoModel.from_pretrained()`"
"Does the 'bad_words_ids' argument in the \"generate function\" works? "
"[QuestionGeneration] RuntimeError: Integer division of tensors using div or / is no longer supported"
"Add `inference_mode` back to `image_segmentation`"
"fx: don't copy by reference"
"Can't load tokenizer for 'facebook/hubert-base-ls960'"
"Shapes mismatch triggered at modeling_flax_utils"
"the results of run_glue_no_trainer.py are different from those reported in the paper"
"\"Size mismatch\" when using the same type of model as pretraining during finetuning while \"num_labels\" is diffierent"
"Model doc examples for GPT-j-6b are slightly misleading for the GPU context"
"Hidden states of BertForPreTraining (load from tf ckpt of google original bert) not exactly equal to the output of extract_features.py in google original bert"
"Bart model converted ONNX inference "
"Can't access Huggingface page and cant download models from laptop. Can it be IP ban?"
"input_ids is None when evaluating"
"How can i add a Bi-LSTM layer on top of some model?like xlm_roberta"
"CoNLL2003 ner_tags order mismatch between the dataset from HF and the pretrained model"
"Trainer bug in BertForQuestionAnswering when Pretrained model wasn't trained on NPS"
"Bug in Microsoft TROCR Large"
"CausalLMHead Models do not register head parameters."
"Wrong max_position_embeddings for roberta-large"
"AttributeError: 'T5Config' object has no attribute 'output_scores'"
"TROCR custom dataset"
"Fast tokenizer converter leads to PanicException: no entry found for key"
"Document or add more detail on `DPRPretrainedModel`"
"Please don't remove `prepare_seq2seq_batch` in future versions"
"Question: dropping transformers layers"
"Get FLOP count for a model"
"T5 truncation : `generate()` produce a tensor of maximum 115 length"
"TensorFlow 2.6 error with JAX/FLAX implementation"
"Another metric with the same name already exists."
"About qa example"
"rewrite state_dict in self.model.save_pretrained(), causing the '_metadata' it saved to be missing."
"Add `ElectraForCausalLM` to enable constructing Electra-based `EncoderDecoderModel`"
"how can I use convert_marian_to_pytorch.py to convert translation model from marian to pytorch model?"
"ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.15.1/metrics/sacrebleu/sacrebleu.py"
"TokenClassificationPipeline `TypeError: postprocess() got an unexpected keyword argument 'ignore_labels'`"
"ConvBertForQuestionAnswering hangs on 8x TPU cores using PyTorch / XLA"
"LayoutXLM tokenizer issues after last update"
"QuestionAnsweringPipeline cannot handle impossible answer "
"How to use several pipelines in parallel"
"Request for advanced documentation on the text generation pipeline."
"Mismatch between sentinel token IDs from T5 data collator and T5 tokenizer"
"static masking for BERT or RoBERTa model"
"How to prevent tokenizer from outputting certain information"
"Model.generate encoder decoder"
"transformers GPT-2 has wrong implementation in scale in Attention class"
"Mutable default value for `model_kwargs` in `pipeline` function"
"run_summarization.py - num_update_steps_per_epoch calculation"
"is pooler output vector directly comparable to vector representation of word?"
"Inference with a Dataset using pipeline in a tokenclassification job causes a ValueError(\"At least one input is required.\")"
"tokenize_plus doesn't work when moving from pip to conda"
"Environment errors need better actionable error reporting"
"transformers_4.13.devo giving error during saving model"
"Predictions for pre-tokenized tokens with Roberta have strange offset_mapping"
"`BatchFeature` performance improvement: convert `List[np.ndarray]` to `np.ndarray` before converting to pytorch tensors"
"Pretrained model outputs all zeros on GPU in a docker environment"
"trainer gradient_accumulation_steps"
"question answering pipeline throws error when handle_impossible_answer=True"
"LED models give: `IndexError: index out of range in self`"
"convert_graph_to_onnx.py quantization still has relative imports which break when running as a script."
"Italian roberta model takes 3 minutes to load"
"Why GPT2ForTokenClassification doesn't shift logits and labels?"
"Onnx T5 for Generation"
"Pipelines: batch size"
"BartForConditionalGeneration decoder_input_ids problem"
"Can you get Q/A links from LayoutLM"
"`SegformerFeatureExtractor` trying to access non-existent `.ndim` attribute"
"Performance question for pipelines (feature extraction)"
"Inference API: Can't load tokenizer using from_pretrained, please update its configuration: No such file or directory (os error 2)"
"Avoiding the time consuming for downloading the pre-trained models"
"Empty sentence and minus translation in opus-mt-de-en model"
"Loading RoBERTa pytorch_model.bin checkpoint in fairseq for evaluation"
"layoutlmv2 input tensor shape"
"Electra model from pretrained not loading correctly"
"'tuple' object doesn't have attribute `as_list`"
"how to config gpu for run_text_classification?"
"BEIT masked lm"
"Unable to load/use TFWav2Vec2ForCTC TFLite-model "
"Quantize t5 v1_1 generates nonsense"
"TFEncoderDecoder not handling labels correctly"
"How FNet handle PAD token?"
"The pytorch example summarization/run_summarization.py do not work with MBart"
"Comparison Chart (Table) for all the existing BERT models."
"run_mlm.py Issue | MODEL_FOR_MASKED_LM_MAPPING is None"
"Weird assumptions in the PLM collator"
"Export LayoutLMv2 to onnx "
"run_translation.py englisht-german translation failed. RuntimeError: CUDA error: device-side assert triggered"
"`eos_mask` is possibly supposed to be taken from `decoder_input_ids`"
"Data type error while fine-tuning Deberta v3 Large using code provided"
"Using Bart with input_embeds to generate text without input_id return error"
"Which language is available for EncoderDecoderModel pre-trained model?"
"Token indices sequence length is longer than the specified maximum sequence length "
"Wav2Vec2 Speech Pre-Training After a few epochs the contrastive loss was decreased to zero and the model stopped changing"
"Wav2Vec2 CUDA memory usage doubled in v4.11.3 compared to v4.10.3 with the same batch size"
"--config_overrides doesn't appear to work in run_clm.py when trying to specify a larger GPT model"
"Can `BartForConditionalGeneration` use the `sample()` method ?"
"It seems that `RagSequenceForGeneration.generate` is computing inaccurate loss value"
"force_bos_token_to_be_generated is depricated, should be replaced by forced_bos_token_id in BART documentation"
"[Consistency] Automatically set decoder_input_ids for TFEncoderDecoderModel"
"AttributeError: 'NoneType' object has no attribute 'encode_plus' with XLNet tonkenizer"
"TF models save_pretrained() failed when saved_model=True"
"`np.ndarray` not supported anymore for optional arguments at inference for Bert"
"support for pytorch-directml"
"Quantization with `transformers.onnx`"
"Pipelines fails with IndexError using Bert model with outputs and batch size >= 16"
"Finetune Hubert model : Adding new vocabulary"
"Errors while importing FlaxHybridCLIP checkpoints to FlaxCLIPModel or CLIPModel "
"DebertaTokenizerFast from microsoft/deberta-base returns strange offset_mapping for \u0120 prefixed token"
"[Generation] Make `generate()` method compatible with speech and vision inputs while keeping 100% backward compatibility."
"Exporting `sentence-transformers/LaBSE` to ONNX leads to different output"
"Initial install:  No module named 'tensorflow.python.keras.engine.keras_tensor'"
"Add documentation for exporting TorchScript model to accelerator"
"[Benchmark] Tokenizers as Collate functions vs normal in loop tokenizing"
"`BertTokenizerFast.vocab.keys()` does not return a fixed order sequence"
"`AttributeError: 'BertConfig' object has no attribute 'items'` when saving a tf keras model with `transformers 4.12.4`"
"Longformer slower than Roberta"
"Issue doing multi gpu training with TrOCR Transformer"
"Ecco package - integration with HuggingFace"
"BERT outperformed XLNet"
"(EncoderDecoderModel) Why decoder_start_token_id are different between train and generation?"
"What does \"is_beam_sample_gen_mode\" mean "
"tokenizer"
"Issues with Training VisionEncoderDecoder with Seq2SeqTrainer"
"OpenAIGPTTokenizer does not work with spacy 3.x installed"
"[RFC] Amphere/tf32 defaults for transformers"
"Patrick von Platen"
"Sampling sequences similar to a given sequence"
"How to implement huggingface BERT + CRF layer?"
"GPT2 Generate doesn't pass the user defined past_key_values."
"The multi-node / multi-gpu training and repeat logging on each process"
"[New Model] DocFormer: End-to-End Transformer for Document Understanding"
"Tracing LayoutLMv2 results in wrong input_shape dimension"
"Function `ByT5Tokenizer.convert_tokens_to_string()` fails with certain tokens"
"`EncoderDecoderModel` `generate` for a `ViT` as encoder"
"Pretrained bare model have weights that are not beein used"
"Unable to load DeBERTa-v3 tokenizer"
"Wav2Vec2ForPreTraining in 4.12 broke SpeechBrain implementation"
"How do I preserve HTML structure when putting data through transformers"
"Where do I find the class documentation"
"facebook / wav2vec2-base-100k-voxpopuli fails to load on huggingface.co (also on system)"
"Fine-tune Integer Bert for question answering task"
"Is the index of the vocabulary in Tokenizer the same as the index of WordEmbedding?"
"where can I find the dataset bert-base-chinese is pretrained on?"
"JAVA Predict"
"Loading from the wrong cache? "
"DPR usage of BertPooler"
"Fatal error in event_loop.c"
"tokenizer.save_pretrained() doest not save an important part of tokenizer_config of FlaubertTokenizer"
"Feature request: Add built-in support for autorregressive text generation with ONNX models"
"BART + ONNX torch.jit error iterabletree cannot be used as a value"
"Add TAPAS trained on NQ"
"Cannot find 'blob' directory in your 'transformers' repository"
"FLAX core dump error on CloudTPU when running run_clm_flax.py"
"How to get logits from generate() method ?"
"(TF) InternalError: Multiple CPU devices "
"DeBERTa-v3 does not preserve spaces before/after additional special tokens in convert_tokens_to_string output"
"Add a new gradient regularization feature"
"FillMaskPipeline assumes model return dict but return_dict is not set"
"RuntimeError: The expanded size of the tensor (20) must match the existing size (101) at non-singleton dimension 0.  Target sizes: [20].  Tensor sizes: [101] "
"Distributed Training with Triplet Loss and DistilRoberta encoder"
"Converted TF model cannot generate line breaks anymore"
"bidirectional training for GPT2"
"LayoutXLMProcessor applies the english Tesseract model"
"\u2753 Define tokenizer from `tokenizers` as a `PreTrainedTokenizer`"
"Wav2vec2 finetuned model's strange truncated predictions"
"GPT model `generate()` function not correctly skipping the padding tokens indicated by `attention_mask`"
"Speeding up the models inference by OpenVINO through accurate quantization via NNCF"
"Examples for speech recognition trainings from scratch"
"Computation of mask indices in Wav2vec2Model fails with low probabilities"
"trainer process bar can't move while training"
"Logits warper for batch generation"
"Deepspeed and T5-11B for multitask training"
"Difference in the length of positional embeddings produces different results"
"Is the attention_mask in BertSelfAttention applied correctly?"
"cannot import name 'DataCollatorForSeq2Seq' from 'transformers'"
"Two bugs in AdamW"
"cannot import name 'SpeechEncoderDecoder' from 'transformers' - wav2vec2-xls-r-2b-22-to-16"
"Out of the box GPT-2 CLM hits out of memory on an AWS 8x Nvidia A100 VM"
"CUDA OOM at `self.optimizer.consolidate_state_dict()` in Trainer when using sharded_ddp "
"TAPAS tanh activation on the pooling layer"
"TAPAS tokenizer is unable to handle indexed dataframes"
"Does the word embedding matrix of GPT2 load from the checkpoint,  during the fine-tuning?"
"Huggingface Missing Larger T5 Flax Models"
"Question about the <mask>"
"Can\u2019t run Parallel inference"
"Loading from checkpoint without skipping requires the same number of GPUs"
"Target customized languages using multi-lingual sentence transformer models like \"stsb-xlm-r-multilingual\""
"XLMForSequenceClassification does not work"
"Question about an error occurring while running hf_argparser.py"
"[Probably a bug] - T5TokenizerFast is not the same as T5Tokenizer  - it adds id 1 when using batch encode plus "
"[Bug] `tokenizer.model_max_length` is different when loading model from shortcut or local path"
"[Bug] Issue in AutoModelForSequenceClassification while initialization"
"BertForSequenceClassification : 'module 'Torch' has no attribute 'BoolTensor'"
"Can\u2019t load config for \u2018/data/sentence-transformers_all-mpnet-base-v2\u2019"
"Adafactor does not work with Resnets (or with MAML)"
"GPT2 large trains on 1 GPU but does not fit in two."
"Cannot run Deepspeed inference of GPT-Neo with low_cpu_mem_usage enabled"
"Add PoolFormer Model"
"Deberta's Enhanced Masked Decoder"
"Config overrides option working on any kind of config"
"Version 4.12.5 doesn't work on sagemaker"
"GPT-2 Model wrapped in DataParallel hangs immediately"
"Hubert-base Model with new tokenizer is not converging "
"Error loading Speech2Text2Processor.from_pretrained(\"facebook/wav2vec2-xls-r-2b-21-to-en\")"
"Can you add a fast tokenizer for the ByT5 model? "
"Recover when the `sha` of the local file is not correct."
"HF space specific issue: can't import `AutoModelForSeq2SeqLM` from `transformers==4.6.1` with `torch` installed"
"[Flax] Add Flax implementation of `RoFormer`"
"[Benchmark] HF Trainer on RTX-3090"
" Unable to load mT5 with MT5Model.from_pretrained(\"google/mt5-small\") "
"Got extra_id_%d when generating texts, and %d is negative."
"Scores returned by beam search are not useful"
"Running  Pipeline batching code from documentation returns TypeError: _batch_encode_plus() got an unexpected keyword argument 'batch_size' on Colab. "
"datasets doesn't support NSP? should I implement my custom DataCollator or function for dataset.map?"
"Bigger batch size decrease the throughput."
"Getting ambiguous messages for SummarizationPipeline"
"DeBERTa `p2p` attention type is not used"
"[Quick poll] Give your opinion on the future of the Hugging Face Open Source ecosystem!"
"How to separate multiturn dialog context in blenderbot?"
"load_best_model_at_end failed due to \"size mismatch\" when DeepSpeed is used"
"GPT-NeoX checkpoint conversion?"
"bert-base-uncased weights for BertForPreTraining"
"Add Sliding Window to TokenClassificationPipeline"
"to use FX - your torch version must be *exactly* 1.9, even though fx also works in later versions"
"Dynamic Inputs for fx traced GPTNeoLM "
"Gradient accumulation causing different training curves"
"[Hub] 403 when trying to download models"
"Encapsulate all forward passes of integration tests with \"with torch.no_grad()\""
"Reproducibility issue with Trainer"
"LEDTokenizer doesn't pad `global_attention_mask`"
"Finetuning T5 for 2 tasks with 2 diferent prefixes and different data gives the same result."
"Make `CLIPFeatureExtractor` accept batch of images as `torch.Tensor`."
"Transformer-XL -100 label padding"
"Missing weight parameter when loading from deepspeed stage-2"
"Do we use JSON lines or JSON only for run_translation.py in PyTorch?"
"Wrong link in the documentation (ConvBERT, BART, Fnet)"
"Generate: Passing encoder_hidden_states in with num_beams > 1 raises an error"
"tf.matrix_band_part have been deprecated"
"PT CausalLM models config issue"
"Output sequences are different between SummarizationPipeline and model.generate"
"Loading t5 from config file"
"[Feature request] Doc example copy button - option to remove input prompts and outputs"
"LongformerTokenizer Error"
"Multiple answers for QA "
"The loss function should ignore tokens whose index is set to - 100"
"Occasional Can not load roberta-base tokenizer "
"MarianForCausalLM doc example not working"
"[chinese wwm] load_datasets behavior not as expected when using run_mlm_wwm.py script"
"Erroneous 404 warning when using AutoTokenizer.from_pretrained"
"GPT-NEO Incosistent inference TPU vs GPU"
"DeBERTa V3 Fast Tokenizer"
"Support for Kaldi formatted Audio files, especially \"segments\""
"Difference between vocab_size in model T5forConditionalGeneration \u201ct5-small\u201d and its corresponding Tokenizer \u201ct5-small\u201d"
"Include documentation on linking to transformers docs with Intersphinx"
"Fix docs pointers for DeepSpeed"
"Possible simplification in T5 docs"
"Do we have pretrain (from scratch) scripts for BART? "
"XLNetLMHeadModel has no attribute 'from_config'"
"Add support to DistilBertLMHeadModel"
"TF and PT code confuse in the documentation "
"Batch size affecting output when using GPT2Model"
"SacreBLEU uses incorrect tokenizer for Japanese"
"KeyError: 337 when training a hugging face model using pytorch"
"Addition of Swin Transformer for Computer Vision"
"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
"make docs failing"
"Question about how to modify token embedding before sending to bert"
"finetune_wav2vec2_xlsr_turkish.sh can not find wav"
"Nan when training LayoutLM_V2 Model"
"Is there a way to batch examples by max number of tokens using tokenizers and datasets?"
"Length penalty for beam search"
"Adding new tokens to various models changes tokenization of adjacent elements in strings"
"A bug in T5LayerNorm"
"Failed to import transformers.trainer"
"Errors in running Perceiver example with transformers-4.14.0.dev0"
"Nan when training LayoutLM_V2 Model"
"Support Tensorflow tensors as input in tokenizers - [Ongoing]"
"Share custom pipelines on Huggingface Hub"
"Add tqdm to pipeline"
"Eval rouge = 100.0 on gem/wiki_lingua_english_en"
"Can I  print mlm and nsp loss instead of their sum while train the BertForPretraning model?"
"some error when I run the pytorch example wav2vec2 by rum_common_voice.py"
"Unable so save quantized model for mobile using Speech2Text"
"CUDA error: device-side assert triggered while training Marian MT "
"[Benchmark] google/pegasus-wikihow"
"How does decoder's weight shared with input embeddings?"
"Enable ONNX export for `VisionDecoderEncoderModel`"
"Support on Mixture of expert models"
"Roberta Classification Head"
"a"
"RFC: Integrating bitsandbytes 8-bit optimizer / adding Embedding Norm"
"fp16 flag silently fails"
"How to use Roberta as the Encoder and a randomly initialized TransformerDecoder as the Decoder?"
"preprocessing_num_workers coredump"
"Flax/Roberta - Tokenizer"
"as_target_tokenizer is not an attribute of PreTrainedTokenizerBase and Bart/RobertaTokenizer"
"Unable to save pretrained model after finetuning :  trainer.save_pretrained(modeldir) AttributeError: 'Trainer' object has no attribute 'save_pretrained'"
"Adafactor lacking min_dim_size_to_factor"
"FlaxVisionEncoderDecoderModel has no attribute 'from_encoder_decoder_pretrained' ?"
"Seq2SeqTrainer.evaluation_loop requires `labels` due to DataCollatorForSeq2Seq"
"eval_loss is nan for GPT2 trained with fp16 + deepseed on 8xA40s"
"Segmentation fault (core dumped) when conversion of GPT-J to onnx"
"Subword Tokenization Bug after Non-Space Word Boundaries (AlbertTokenizerFast)"
"Fine-tuning GPT-J-6B in colab: 8-bit weights with low-rank adaptors"
"trainer.create_model_card should be run by process 0 only in distributed training"
"Unable to save trained model in path"
"T5ForConditionalGeneration lm_head not initialized from pretrained checkpoint"
"inconsistent BertTokenizer and BertTokenizerFast"
"Train Bart-Large multi-labels"
"Fine-tune GPT-J with SageMaker Model Parallelism "
"A potential bug in ModuleUtilsMixin.get_extended_attention_mask"
"Huggingface Transformers fastTokenizer for DeBERTa v3"
"[Wav2vec2] RuntimeError: CUDA error: an illegal memory access was encountered"
"Cache the files in get_fast_tokenizer_file()"
"Open discussion for design decisions."
"Fine-tuning Wav2Vec2: Concern that pretrained weights are being reinitialized"
"Error while reproducing example for PerceiverForMultimodalAutoencoding"
"Add `in_chans` to `DetrModel`"
"dataset of Helsinki-NLP/opus-mt-en-zh"
"Model trains on 1 node 8xA100 but hits OOM in 4 nodes 8xA100"
"core dumps run_onnx_exporter.py in gpu."
"TrOCR processor cannot be loaded from AutoProcessor"
"Running MLM pretraining with not \"line_by_line\" big dataset"
"[logging] unable to turn off tqdm logging"
"Turn of do_sample for T0pp in Inference API"
"[doc] bug in docstring conversion"
"GPT-J: Implement Memory Efficient Attention"
"Adding tokens to pretrained model \"Helsinki-NLP/opus-tatoeba-en-ja\" using tokens from vietnamese not working"
"Generate does not take into account config.decoder.eos_token_id"
"[jax] absl issues"
"Can't load tokenizer for 'microsoft/wavlm-base' when using Wav2Vec2Processor as in docs"
"How to generate output using custom embeddings?"
"[Benchmark] Deepspeed +fp16/bf16 on a 8xA100 node"
"How to update config after model inited?"
"ConnectionError"
"Add Deepspeed Transformer kernel, but encounter error  IndexError: tuple index out of range"
"Why I met Type 'seq(tensor(int64))' of operator (MemcpyFromHost) is invalid when using onnxruntime.InferenceSession() in GPU, and How to resolve it? On emergency hold\uff0cthanks!"
"Multiprocessing for pipeline"
"Inference API: return token scores for text-generation models"
"Using Huggingface Trainer in Colab -> Disk Full"
"Facing Problems with RobertaForSequenceClassification.from_pretrained()"
"VQA model inferences"
"AutoTokenizer hash value got change after datasets.map"
"A warning is raised when using DistributedDataParallel of PyTorch"
"Cannot instantiate model under dopamine"
"Question: Object of type EncoderDecoderConfig is not JSON serializable"
"Cannot Convert Megatron GPT checkpoint"
"when I use \"convert_pytorch_checkpoint_to_tf\", I meet some problems"
"Different evaluation results "
"Unable to see total_flos in ViT training logs"
"[doc] post-conversion: incosistent \"default to\""
"Missing parameters when iterating over `module.parameters()`"
"[Speech recognition examples] num_processing_workers not allowed to be set"
"huggingface_pytorch-pretrained-bert_bert.ipynb -- RuntimeError: Cannot find callable bertTokenizer in hubconf"
"GPT-2 generate degenerates into producing garbage after a while"
"Documentation in CTRL linksto 404"
"[Request] PerceiverForTokenClassification for NER task"
"Ability to save model outcomes in tabular format CSV file?"
"Issue with Jiva/xlm-roberta-large-it-mnli"
"How to save the fine-tuned model"
"Custom constructed and trained `tokenizers.Tokenizer` for Albert error."
"Model stopped training once I introduced << report_to = 'wandb' >> in TrainingArguments"
"Convering tf to torch: How to set custom embedding_size when using load_tf_weights_in_bert?"
"How to instantiate custom T5ForConditionalGeneration"
"Finetune M2M on multiple language pairs"
"very large model on multi gpu"
"can I use different pre-trained model for autoTokenizer and autoModel?"
"the shape of trainer.predict().predictions is inconsistent with the input dataset"
"EncoderDecoderModel loss decreased unbelievably and the generated text were repetitive 4.12.0"
"When using bert-base-chinese model, except for the first one, other uppercase English letters that are the same in succession will be ignored. And the input_id of different uppercase English characters is the same"
"Unexpected usage of `next_token_scores` and `beam_scores` in `beam_sample()`"
"BertTokenizer can't split the string in the form of \"word+special_token\"correctly."
"[Benchmarks] index"
"No module named 'transformers.models.fnet.modeling_fnet'"
"How to boost the speed of one sentence Marian Translation(no batches)?"
"AlbertTokenizer doesn't decode special tokens properly"
"How to define the self-attention layer with transformers"
"\"total_flos\" showing much bigger number than expected"
"Unexpected outputs of randomly initialized `T5ForConditionalGeneration`."
"ViTFeatureExtractor PyTorch Batch problem"
"[Trainer] finetuning: larger batch-size leading to a worse train loss"
"AutoTokenizer unable to load pre-trained bert-base-uncased tokenizer"
"Training causal language models from scratch without grouping independent data samples into blocks"
"Which model checkpoint should be selected for evaluation\uff1f"
"[Benchmark] HF Trainer on A100"
"Removing tokens from the tokenizer"
"ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape torch.Size([1, 15, 3072])"
"Issue with `stas/tiny-wmt19-en-de` model"
"BERT model from pipeline hangs with multiprocessing pool"
"Wav2Vec2 bart-large - finetuning failing with ValueError: tokenizer has to be of type....but is <class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'"
"[FX] `symbolic_trace` yields a TraceError for `BertModel`"
"[Trainer] Very different loss reported during training vs. at the end."
"word_ids method is not available on fast tokenizers when using \"prepare_for_model\""
"Inference API: Error with GPU inference"
"Cannot fine-tune google/mobilebert-uncased using native Pytorch"
"Trainer model __init__() got an unexpected keyword argument 'prediction_loss_only'"
"Multilabel Token Classification in trainer"
"How can I update special token ids?"
"Return type of `ViTFeatureExtractor` does not match `return_tensors` parameter when input is `torch.Tensor` or `PIL.Image.Image`"
"Adding additional layers to TFHubertModel throws OperatorNotAllowedInGraphError"
"Feature request: m2m100_418M support on onnx"
"Import Error : cannot import name 'create_repo' from 'huggingface_hub'"
"Not able to log training and validation loss to visualise in tensor-board as tfevents?"
"Question: CANINE with (pre-trained) LM head"
"compile error when installing transformers[flax]"
"How do I change the classification head of a model from multi-label to multi-class?"
"How to use an unpretrained model?"
"[JAX/FLAX]: CLM Tokenizer Training confusion"
"[Benchmark]"
"train_new_from_iterator missing from GPT2Tokenizer"
"pegasus"
"explicitly load local file"
"Script run_mlm_no_trainer.py error"
"Pytorch T5 pre-training script request"
"UserWarning: __floordiv__ is deprecated"
"optimum | ModuleNotFoundError: No module named 'optimum.intel.lpot'"
"Trainer not keeping best model checkpoint with save_total_limit=1"
"GPT-J Tokenizer model_max_length=1024 despite n_positions=2048"
"Weird evaluation result when using distributed training"
"Support custom StoppingCriteria in model.generate"
"Why is Marian to Torch converter hardcoded for tied vocab ?"
"How to efficiently tokenize unknown tokens in GPT2"
"Trying to train the TFWav2Vec2ForCTC model"
"OOM error on Pretraining Albert with batch size 8 "
"Reason for returning dequantized (fp.32) value at every layer of I-BERT."
"ImportError: cannot import name 'CpmTokenizer' from 'transformers'"
"Error while converting distilbart-mnli-12-1 model to ONNX"
"Text generation with TAPAS as encoder"
"[Community Event] Robust Speech Challenge"
"Example script to edit kenlm arpa file does not work correctly in kaggle notebook"
" run_summarization.py download datasets error"
"Electra model class support loading weights from other types of BERTs"
"Error when running a wandb sweeps on run_summarization.py"
"AutoTokenizer | ValueError: Couldn't instantiate the backend tokenizer from one of:"
"Add test suite for flaubert tokenizer"
"[TBD] discrepancy regarding the `tokenize` method behavior - should the unknown token be included or not"
"Error when running TFT5ForConditionalGeneration with tensorflow-cpu==2.8.0-rc0"
"AutoTokenizer | TypeError: an integer is required (got type NoneType)"
"Is it possible to use ONNX for summarisation with BART yet?"
"It is better to add a function to train additon tokens for the pre_trained tokenizer. esp. for the language like Chinese."
"the `tokenize_chinese_chars` argument is not always taken into account with the fast version of the bert tokenizer"
"Add in `model.generate` support to encoder outputs  for flax models"
"Add FastSpeech2"
"Add Model Interpretability Module"
"model.generate with prefix_allowed_tokens_fn throws RuntimeError: probability tensor contains either `inf`, `nan` or element < 0"
"Using the accelerate MLM example still results in CUDA out of memory"
"RuntimeError: expected scalar type Half but found Float"
"Bug in seq2seq fine tuning section of Automatic Speech Recognition Examples"
"processing texts longer than 512 tokens with token-classification pipeline "
"Unclear documentation regarding sequence length for all-MiniLM-L6-v2"
"Bug with max_seq_length argument in training scripts"
"ValueError When Padding Long Sequences To Small Max Length (Pytorch and Tensorflow)"
"Longformer Set Global Attention Layers Non-Trainable"
"run_t5_mlm_flax.py Multi GPU Training"
"`AutoConfig` doesn't support `gpt-neox`"
"can't set gradient_checkpointing=True when using the DistributedDataParallel mode"
"Question about fine-tuning with vision transformer (VIT)"
"Tensorboard missing values"
"Wav2Vec2ForCTC fine-tuning best practices"
"Question about fine-tuning BeitForSemanticSegmentation model"
"GPT2: masked_bias should be sufficiently small instead of -1e4"
"How can i add new_token ?"
"Different generations with the NLP model MarianMT of HuggingFace"
"ValueError: transformers.models.auto.__spec__ is None. causing import errors"
"Add support for BERT SequenceClassification conversion to ONNX"
"ALL YOUR BASE ARE BELONG TO 504."
"DebertaForMaskedLM cannot load the parameters in the MLM head from microsoft/deberta-base"
"[ONNX] transformer.onnx exporting fails for longformer "
"MobileBertForPreTraining means  IB-BERT PreTraining? "
"Wav2vec code not working with kenlm n-gram"
"Is it possible to support Wav2Vec in ZeroShotClassificationPipeline?"
"where is the 4.16.0dev??"
"Copy of the custom modeling file when saving a model"
"Is there any script for running RACE dataset?"
"Wav2Vec2ForPreTraining doc example has None loss"
"wav2vec2_with_lm"
"why training bigbird-512 model is much slower than bert-512?"
"Evaluation Padding_idx Error when using BERTScore and Deepspeed Zero3 but not with Zero2"
"Cannot load BART-base model"
"Export LayoutLMv2 to TorchScript"
"Fine Tunning the Pytorch script"
"Movement Prune"
"Benchmark link in transformers/notebooks/README.md is broken"
"Whether instantiation is premature?"
"Inference of finetuned wav2vec2-xls-r-300m model using the ASR pipeline does not remove special tokens."
"run_tests_pipelines_torch is not deterministic"
"pytorch NER example dataset deleted"
"Self-Attention Layers for Perceiver Decoder"
"Saved slow tokenizers cannot be loaded in `AutoTokenizer` after environment change"
"RagSequenceForGeneration without retriever"
"TrOCR small processors are all broken "
"Padding idx in modeling RoBERTa"
"Question: how to evaluate word similarity with context?"
"Getting error while saving model"
"RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
"Nan when training LayoutLM_V2 Model"
"Robust Speech Challenge Evaluation File does not use entire dataset for metric calculation"
"Minimal model for writing tests."
"Decoding with Wav2Vec2 with Language Model"
"Push to hub training argument not pushing"
"GPT-Neo batch inferencing with sampling results unexpected output"
"modeling_visual_bert in case of self.bypass_transformer=True"
"MarianMT models translating valid Chinese sentences to empty string"
"OSError: You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run `git lfs install` followed by `git lfs pull` in the folder you cloned."
"Deepspeed Wav2vec xlsr bug"
"Using HuggingFace Models for Text Translation of sensitive data"
"Fine-tune wave2vec trained checkpoint"
"Documentation for SegFormer includes improperly-formatted table "
"Perplexity VERY high but generated text coherent"
"preprocessing_num_workers missing in run_summarization_no_trainer"
"how can i write tensorboard when i train model with the script \"run_speech_recognition_ctc.py\" "
"CPU OOM when using IterableDataset with dataloader_num_workers > 0"
"ValueError: No valid checkpoint found in output directory"
"random word masking index shouble be great than 0"
"wav2vec with LM leads to CPU OOM"
"DebertaV2 For run_qa.py"
"A few minor questions regarding run_summarization.py in examples"
"-"
"GeneratorExp aren't supported by torch.jit.script when I try to export a previously trained model  'google/vit-base-patch16-224-in21k'."
"Ignored unknown kwarg option direction, while running run_mlm.py (pytorch)"
"Error when running the T5 pre-training script"
"Unexpected shape of \"past_key_values\" of ProphetNetDecoder.forward()"
"Implementation of activations as subclasses of the torch.nn.Module"
"Issues with custom dataset in Wav2Vec2"
"Run LayoutXLM without image input not possible"
"No module named 'transformers.models.bort'"
"Does huggingface allow to perform n-fold cross validation and custom loss function using Hugging face Trainer and save best model?"
"Validate models are loadable"
"Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can't constant fold SequenceEmpty node 'SequenceEmpty_5330'"
"Evidentiality-guided Generator - Retrieval model"
"Fine Tuned GPT2 model performs very poorly on token classification task"
"can't use with pytorch dataloader"
"Logits size does not match vocabulary size when using pyctcdecode for a fine-tuned wav2vec 2.0 model"
"snapshot_download() got an unexpected keyword argument 'allow_regex'"
"ImportError: cannot import name 'ImageGPTFeatureExtractor' from 'transformers' (unknown location)"
"[activations] pytorch-1.11+ Tanh Gelu Approximation "
"Error with run_seq2seq_qa.py official script (pyarrow.lib.ArrowInvalid: Column 4 named labels expected length 1007 but got length 1000)"
"[deepspeed] `bigscience/T0*` multi-gpu inference with ZeRO"
"Running SQuAD 1.0 sample command raises `IndexError`"
"XLNet config num_labels and _num_labels"
"what is the equivalent manner for those lines?"
"AutoTokenizer does not support XGLMTokenizer"
"[JAX/FLAX]: Language modeling example throws RESOURCE_EXHAUSTED error for large datasets"
"How to use multi-node multi-gpu cluster for large batch beam-search decoding"
"Performance of T5"
"Support for larger XGLM models"
"Notify user if group_by_length won't be used"
"Bug in T5 Tokenizer - Adding space after special tokens"
"XGLM Bug: break: module 'torch.utils' has no attribute 'checkpoint'"
"error when running official run_speech_recognition_ctc.py (ValueError: 'a' cannot be empty unless no samples are taken)"
"Higher memory allocation for GPT-2 forward pass when not using past_key_values."
"Can't push a model to hub"
"tokenizer truncation_side is not set up with from_pretrained call"
"Tokenizers can not pad tensorized inputs"
"line 111 in convert_megatron_bert_checkpoint.py cause error \"AttributeError: 'Namespace' object has no attribute 'get'\""
"Add class weight support for classification"
"Adding RelationExtraction head to layoutLMv2 and layoutXLM models"
"Unseen decoded words during inference with Wav2Vec2ProcessorWithLM"
"Support dynamical input size and shape for TrOcr input image"
"Masking in T5Attention"
"Inspect inner layers of Transformer models as in TensorFlow/Keras"
"Convert T5x models to PyTorch"
"Preprocess/transform logits before caching them for computing metrics."
"New and better T5 checkpoints from scaling transformers paper"
"Issue with long contexts or AutoTokenizer.. Unsure which one is it is more of"
"RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasSgemmStridedBatched (handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)"
"Wav2Vec2 - TypeError: Concatenation operation is not implemented for NumPy arrays"
"token-classification example looses ner_tags labels in trained model"
"Log custom mlflow artifact using trainer"
"Add Adapter Weighs to Flax"
"tf.concatenate does not exist"
"Pretrained model for sequence to sequence question answering"
"Wrong/inconsistent behaviour in EncoderDecoderModel and generate method"
"Isn't `transformers.utils.fx` compatible with torch 1.10+ ?"
"ASR pipeline never takes into account the beam width of ngram"
"Number-specific tokenization changes"
"502 Server Error: Bad Gateway for url: https://huggingface.co/api/models/t5-base"
"How to convert  tf_model.h5 to tf_model.ckpt"
"Support for Monotonic Mulithead Attention based Simultaneous Speech-to-text Translation"
"Does Hugging face defaults allow to log mlflow artifacts and name every run of mlflow log?"
"run_summarization fails with RuntimeError: CUDA error: device-side assert triggered when using multi GPU"
"ImportError: cannot import name 'AdamW' from 'transformers' (unknown location)"
"Perceiver IO : How to preprocess raw audio into vector  "
"Allow training from multiple languages for multilingual seq2seq models (varying forced_bos_token_id)"
"Timestamps for Wav2Vec 2.0 models and/or ASR pipelines"
"ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
"TrainingArguments --learning_rate should not be used to set both \"lr\" and \"warmup_max_lr\" in DeepSpeed"
"Benchmarking with T5 or T5-small fails"
"How can I see the masked words during pre-learning by MLM?"
"Download models from a private hub"
"Add attention_scores as output in RoBERTa"
"T5Tokenizer loses most special tokens after I add a new special token."
"* How to convert pytorch.bin to  *.ckpt files"
"How to load bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6 model in offline mode in jupyter nb"
"Getting Error  "
"Various issues with  accelerate launch command for Large example"
"Unable to Import KeyDataset for Pipeline Iterator in 4.16.2"
"Wav2Vec2 for long audio with N-gram Language model"
"SwinTransformer as encoder and Bart as decoder"
"How to use pipeline with Pytorch framework on Windows?"
"Error converting fine-tuned GPTNeoForCausalLM model to ONNX"
"Porting Compressive Transformer to Huggingface"
"Cannot set deterministic=False for FlaxRobertaPreTrainedModel, therefore dropout doesn't work?"
"vits support? "
"Error when passing encoder_outputs as tuple to EncoderDecoder models"
"Keep waiting for push command to finish at the end of running run_speech_recognition_ctc.py "
"Save DistilBert Model and Convert"
"Move generic PyTorch utils function from modeling_utils.py to pytorch_utils"
"[SpeechRecognition Seq2Seq] CUDA out of memory when training on GPU"
"Not able to load CharacterBERT"
"gptj doc tied weights mistake?"
"Model card reports wrong evaluation metrics when load_best_model_at_end=True"
"Fine-tune blenderbot 2.0 to retrieve from my own documents instead internet"
"DeepMind Retro"
"Improved documentation/blog post around `generate()`"
"DataCollatorWithPadding that pads attention_mask"
"Unable to ignore pad tokens when using `decoder_input_ids` and `decoder_attention_mask` with `BartForConditionalGeneration`"
"unable to pass through create vocabulary_from_data when using a custom dataset loaded from disk which is a result of concatenating multiple datasets"
"BART Large generate predictions are wonky"
"m2m100 model do not support eval / predict on deepspeed + fp16 enviroment?"
"Transformers Pipeline Error"
"PyTorch Cuda Tensors not supported in DETR model"
"Inference using ONNX model exported by transformers.onnx raises [ONNXRuntimeError] INVALID_ARGUMENT : Unexpected input data type"
"i cannot connect the hf webset"
"Tokenizers dictionaries not being saved to cache."
"Documentation error "
"Flax model much slower than Pytorch"
"train_file and validation_file in jsonl format with examples/pytorch/language-modeling/run_clm.py  "
"Add TF implementation of GPT-J model"
"Dynamic padding did not work as expected for custom audio dataset"
"ASR pipelines won't load local Wav2Vec models with language models attached"
"T5 encoder past_key_values"
"How to implement generate function for seperate encoder decoder T5 model?"
"gpt2 error using torch.jit.trace "
"Auto tokenizer for question-context tokenization"
"\u2753 T5 pre-training dataset"
"Documentation of DataCollatorForLanguageModeling"
"T5 AttributeError: 'T5Encoder' object has no attribute 'main_input_name'"
"DDP training hangs with `run_glue.py` and `run_seq2seq.py`"
"Why is my train_samples_per_second graph growing?"
"TPU slow finetuning T5-base"
"unable to pass create vocabulary_from_data when using concatenate_datasets() method to train a combined dataset model "
"make style produces a lot of reformatted lines"
"Loading a fairseq trained wav2vec2 model with transformers"
"Pooled results for DistilBert"
"Add support for ONNX-TensorRT conversion for GPT-J6B (and possible bug in rotary embedding)"
"GPT-NeoX-20B Integration"
"TrOCR not working anymore after 4.16.2 update"
"Inference API with GPT2 {\"error\": \"Unknown error\"}"
"LayoutLMv2Model can not be imported from transformers"
"Is it fine if we do not pass the optimizer through accelerator.prepare() in DDP?"
"GPT-2 pretrained model fails to load when TF v2 behaviour is disabled"
"cannot import name 'CONFIG_MAPPING' from 'transformers' (unknown location)"
"\ud83e\udd17 Transformers **Trainer** API raises exception on train if triggered from an already started ML Flow run."
"Why are certain models with a higher WER (on the eval set) performing better or as good as  models with a lower WER \u2013 when tested on the test set?"
"Add Video Vision Transformer"
"Unable to generate chunks (If length is greater than 512 in bert), we can use to split into chunks"
"DebertaForMaskedLM cannot load the parameters in the MLM head"
"model.generate() using a user specified keyword argument"
"How can I use \"accelerate launch\" command to run  training job on Multi-GPU?"
"one of the variables needed for gradient computation has been modified by an inplace operation"
"Tokenizer prepare_for_model Error inconsistency"
"VisionEncoderDecoder Error during training"
"About `decoder_input_ids` in BART doc"
"Typo in https://api-inference.huggingface.co/docs/curl/html/detailed_parameters.html#summarization-task"
"Incorrect information in \"Getting started\" regarding API tokens"
"Tokenizer offset_mapping problem"
"IndexError while applying stride for ASR"
"Image classification example fails"
"confusion about past_key_values in GPT2 "
"Fine-Tune DETR on custom dataset (less than 250 labels)"
"AutoModelForSequenceClassification not learning if model is initialized inside a function scope"
"Error while finetuning XLM-R on Tensorflow-Keras"
"add image2text generation"
"Misplaced sentence in https://api-inference.huggingface.co/docs/curl/html/detailed_parameters.html#question-answering-task"
"Trainer can't estimate SpeechEncoderDecoder tokens for `floating_point_ops`"
"Converting mBART to ONNX format"
"Drop support for Python 3.6"
"Need more understanding of the function: get_visual_embeddings(image_path)"
"TokenizerFast.from_file() is stuck when loading a large tokenizer.json with tokens added and \"pre-trained\" starting from an existing, trained model"
"Self attention in T5 decoder does not work as expected"
"`DebertaTokenizer` always assigns token type ID 0"
"`XGLMForCausalLM` does not compute `position_ids` correctly when using `inputs_embeds`"
"Can I training an XLM model from scratch by transformers?"
"Add compatibility for Postponed Evaluation of Annotations (PEP 563)"
"Documentation error in Trainer.predict when output_hidden_states is True"
"Problems loading csv dataset in examples `run_summarization.py`"
"BertForSequenceClassification defines criterion at every forward pass"
"Diverging PT-Flax Wav2Vec2 Hidden-States"
"Add support to export Blenderbot models to onnx"
"Add EfficientNet Model - PyTorch"
"Save Finetuned XLM-RoBERTa on Tf-Keras"
"Add TUNet"
"Resnet weights should not be downloaded when building DETR by from_pretrained"
"do not support deepcopy when using deepspeed?"
"[Discussion] Loading and initialising large models with Flax"
"Docstring says target_sizes is optional (in DETR post_process) but the code disagrees"
"shift_tokens_right function missing for mt5 models"
"Cuda error when using T5 with the new Apex FusedRMSNorm "
"LMV2Processor Decoder string not matching with original string"
"how to save a model with additional layers, so it can be loaded using .from_pretrained()?"
"[Bug] Gradient Checkpointing In CLIP Model"
"Trainer is not so compatible with customized optimizer"
"Handling preprocessing with token-classification pipeline"
"A question about BertForSequenceClassification"
"How can I create a longformer model without position embedding?"
"Unable to load a pretrained model from disc using transformers api"
"How to Use Transformers pipeline with multiple GPUs"
"run_glue_no_trainer.py"
"repeat tokenization"
"Evaluate on subset of evaluation dataset during training"
"Add OFA to transformers"
"Return entities extracted from the raw input for TokenClassificationPipeline "
"How to enable multi-heads outputs in segformer?"
"addeing tokens to tokenizer for training and inference worsens  prediction immensely"
"Fine tune TrOCR using bert-base-multilingual-cased"
"Absence"
"BART model parameters should not contain biases for Q, K and V"
"Embedding index getting out of range while running onlplab/alephbert-base model"
"Problems with convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py "
"Issue running run_glue.py at test time"
"Timestamps in AutomaticSpeechRecognitionPipeline not aligned in sample space"
"Output embedding from each self-attention head from each encoder layer"
"Error:\"TypeError: 'NoneType' object is not callable\" with model specific Tokenizers"
"No such file or directory: '../datasets/RE/${DATA}/${SPLIT}/cached_train_BertTokenizer_128_sst-2.lock'"
"Supporting multiple evaluation datasets in `Trainer` and `Seq2seqTrainer`"
"Unable to run Speech2Text example in documentation"
"Adding license file to some of the BERT models"
"IndexError: index out of range in self"
"Mismatch between beam search score transition probabilities and beam sequence scores"
"How to go about utilizing MBART for conditional generation with beam search in ONNXRuntime with TensorRT/CUDA"
"Passing in num_labels to ConvNextForImageClassification.from_pretrained raises size mismatch error"
"label_attention_mask in Bart for conditional sequence generation and other seq2seq models."
"Broken link error when using the CLIPTokenizerFast class"
"Failed to find input with name: attention_mask in the model input def list"
"After vocabulary extension the tokenizer keeps on running. "
"CLIPProcessor with CLIPTokenizerFast"
"Error using evaluation in run_clm.py"
"Missing tags on Docker Hub after version 4.9.1"
"Incomplete padding support for Funnel Transformer."
"RFC -- TF: unpack model inputs through a decorator"
"Flax XLM-RoBERTa"
"query() of generator `max_length` being succeeded"
"camembert tokinizer "
"Non-unique `local` in toctree"
"Privacy&Security: Network Contact Every Model Load"
"issues in `generate()`"
"TypeError: meshgrid() got an unexpected keyword argument 'indexing'"
"parm:logging_dir not works when "
"Unable to run run_glue.py offline"
"Tranformers documentation translation to Spanish"
"is_index_masked and is_index_global_attn in Longformer"
"HfArgumentParser doesn't recognize new list type hint syntax"
"Unable to run PPLM potato example"
"clean_up_tokenization_spaces=False does not behave correctly in AutoTokenizer's decode function "
"mBART tokenizer not following expected target token order"
"Blenderbot 1.0B Distilled eats up memory over many inferences"
"ValueError: DebertaV2Model does not support gradient checkpointing."
"How to get T5 encoder&decoder's hidden states and keep requires_grad=True"
"longformer-large's hidden_states and last_hidden_states have different size in sequence_length"
"EvalPrediction does not allow for \"sources\" parameter which \"sari\" metric requires"
"How to use GPT-2 for predicting the next word in batch"
"Unigram tokenizer Result is Incorrect"
"PerceiverAudioPreprocessor: forward() missing 1 required positional argument: 'pos'"
"error invoking create_optimizer from Jupyter lab"
"Models traced with HFTracer cannot be TorchScripted or serialized"
"add special tokens does not work in GPT2Tokenizer"
"Deadlock when loading the model in multiprocessing context"
"Add custom classifcation dataset - computer vision"
"Bad error message when downloading private model without being logged in."
"Marian cannot be fully serialized because it accesses the filesystem after the object instantiation"
"TFEncoderDecoderModel generate() gvies different results after #15562"
"FeaturesManager assumes only one of Torch or TensorFlow is installed"
"Translation of documentation into Spanish"
"Unclear message with `add-new-model-like` and no flax installed"
"Sharded DDP returns extra predictions EvalPrediction"
"Translate to Spanish of training.mdx"
"Multiclass image classification with ViT - computer vision"
"`mlm` training fails due to large message size for `nested_gather` on torch_xla"
"Recommended way of exporting encoder-decoder model to ONNX with `transformers[onnx]`"
"TrOCR Backslash problem"
"resize_token_embeddings() failed with GPT-J, after sync to the latest DeepSpeed 0.6.1"
"Beam search uses large amounts of VRAM even with depth of 1"
"Learning rate finder for the trainer "
"what's the difference between Megatron-gpt2 and GPT2 inside transformers?"
"JAX 0.2.22: Replace all deprecated `jax.ops` operations with jnp's `at`"
"[CI] switching CI to pytorch-1.11"
"TypeError: forward() got an unexpected keyword argument 'return_dict' BERT CLASSIFICATION HUGGINFACE with tuning"
"how can we use the model output to do predict?"
"raise ValueError(f\"Unrecognized tokenizer_type {tokenizer_type}\") ValueError: Unrecognized tokenizer_type BertWordPieceCase"
"RuntimeError: [enforce fail at CPUAllocator.cpp:68] Out of memory during batched inference"
"The documentation of transformers.generation_utils.GenerationMixin.generate doesn't match the code of it"
"what us the difference between Trainer and Seq2SeqTrainer ?"
"How to add uppercase letter in chinese-roberta-wwm-ext-large vocab?"
"Use decoder_inputs in generate function"
"MarianMTModel is not trained after finished training and loading"
"Text Generation Pipeline not using Target Tokenizer"
"TF: clearer model variable naming"
"bug in model.generate() / beam_search score per token"
"`torch.dtype` not JSON serializable if config is nested dict"
"Add missing type hints"
"Unable to load Wav2Vec2 fine-tuned models from local files"
"How to load google-research/bert .ckpt weight"
"CUDA out of memory when resuming from checkpoint (not always)"
"[INFO|modelcard.py:460] 2022-03-11 19:03:24,502 >> Dropping the following result as it does not have all the necessary fields: {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}"
"Want to push a branch for a PR that fixes Deberta classification dropout and armonizes the dropout class, but have no permissions."
"CUDA Error with Typical Sampling mass of 1.0"
"Question about the why num_beams is multiplied by 2 in Beamsearch"
"Any reasons not using `processors` or `pipelines` modules in example codes?"
"cannot import name 'Text2TextGenerationPipeline"
"Different weights in ViTMAEModel and ViTMAEForPreTraining"
"Issue in Tutorial - \"Fine-tune a pretrained model\""
"RAM OOM with a large save_steps using trainer API for MLM training"
"finetuning without trainer seems not data parallel when using deepspeed?"
"Nystromformer Pretrained model with longer sequence length "
"Improve EncoderDecoderModel docs"
"Add warning message if model uses `input_ids` that include padding tokens, but no `attention_mask` is provided."
"Implement Maximal Update Parametrization (muP)"
"Text2Speech classes"
" `TFBertModel.predict()` and `TFBertModel()` return different results."
"How to resume run_t5_mlm_flax.py script?"
"CLIPVisionModel errors on trying to load openai/clip-vit-base-patch16"
"Truncation when tokenizer does not have `max_length` defined"
"Initialized DETR backbone weights do not match with actual pretrained weights "
"layoutlmv2 visual backbone pretrained on publaynet "
"Gpt2 large for onnx exportation and int8 quantization"
"Training MLM model XLM Roberta large on google machine specs not fast"
"Clinical longformer - IndexError: index out of range in self"
"Using pooler/hidden states output of an AutoModel vs. XForSequenceClassification for training another classifier?"
"SegFormer Quantized Model (Int8) not loading weights properly."
"ValueError: HFArgumentParser on running run_glue.py"
"Question about `query_length` in modeling_t5.py "
"token classification pipeline does not use attention mask, affects predictions"
"Truncation Error is inconsistent between fast and standard (non-fast) tokenizers"
"A question about the cross-attention layer shape in encoder-decoder model"
"How to pass args of generate() method via batch_decode"
"Loading from AutoModel gives ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds"
"Enable the use of private models in example scripts using use_auth_token"
"Load local dataset error"
"NER at the Inference Time"
"is this a bug in TFTrainer.get_train_tfdataset?"
"ViT-mae missing parameter at comments of `copied  from`"
"Weird PanicException when trying to save tokenizer using `tokenizer.save_pretrained`"
"Cannot replicate the scores from the pipeline"
"Do you fine-tunde both encoder and decoder "
"RuntimeError opening google/pegasus-xsum"
"TypeError loading tokenizer for gpssohi/distilbart-qgen-6-6"
"Freezing layers does not work with torch.utils.checkpoint"
"Typo: Missing a coma in document"
"Connection error, when I run a service in a docker image, which is offline"
"ONNX export results for hidden states/attentions are incorrect if enabled"
"`self.encoder_attn` not defined for PyTorch XGLM"
"creating transformer for tamil language"
"apply_rotary_pos_emb gives different results between PT & Flax"
"Changing the default branch from `master` to `main`."
"[Community Event] Doc Tests Sprint"
"How to do mask prediction with ByT5?"
"Finetune Luke"
"ONNXConfig: Add a configuration for all available models"
"Getting a signal: aborted (core dumped) or Segmentation fault (core dumped) error when trying to train on a single GPU"
"Support SageMaker distributed data parallel library v1.4.0"
"Pipeline always returns to device=\"cpu\""
"[modeling_utils] Keys are still reported as \"missing\" during model loading even though specified in `_keys_to_ignore_on_load_missing`"
"ibert seems to be quite slow in quant_mode = True"
"Can we support the trace of ViT and Swin-Transformer based on torch.fx?"
"First token misses the first character in its `offsets_mapping` WHEN `add_prefix_space=True` is used"
"RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's"
"Implement HybridNets: End-to-End Perception Network"
"Arg to begin `Trainer` evaluation on eval data after n steps/epochs"
"T5Tokenizer Fast and Slow give different results with AddedTokens"
"A slow tokenizer cannot add a token with the argument `lstrip=False`"
"[TBD] discrepancy regarding the tokenize method behavior - should the token correspond to the token in the vocabulary or to the initial text"
" The `convert_tokens_to_string` method fails when the added tokens include a space"
"`FeaturesManager.get_model_from_feature` should be a staticmethod"
" [WIP] New Model Add FastPitch 1.1"
"The huggingface web site crashed"
"_tokenizer.decode TypeError: 'list' object cannot be interpreted as an integer"
"Can't load Longformer Encoder Decoder converted from a MBart."
"Casting to device inside of the Tokenizer"
"output_scores causes trainer.predict to error out"
"question about the the specific text generation question"
"Swap `eval_delay signs`"
"Request on application/pipeline for Text Regression"
"Infer padding in GPT2ForSequenceClassification when inputs_embeds and attention_mask are given"
"Is there a way to filter models by task and group the results?"
"Can't seem to run GPT-J in CPU mode: \"LayerNormKernelImpl\" not implemented for 'Half'"
"when I using pytorch to load \"distilbert-base-uncased\", it's get an error:\"HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt\". But load other models can't get this error. "
"Why the lengthy max_length in Dataset effect the performance of T5 ?"
"run_mlm_wwm.py if set(load_result.missing_keys) == set(self.model._keys_to_ignore_on_save): TypeError: 'NoneType' object is not iterable"
"Cache the files in get_fast_tokenizer_file()"
"AST: Audio Spectrogram Transformer"
"How to build a custom question-answering head?"
"Confusing interaction between training dataloaders and datasets in Trainer"
"TypeError: expected str, bytes or os.PathLike object, not NoneType"
"\"Go to definion\" in VS Code doesn't work properly for Transformers"
"TFDecoderEncoder: The following keyword arguments are not supported by this model: ['position_ids', 'token_type_ids']"
"Unable to load BART model, AttributeError: module 'jax.ops' has no attribute 'index_update'"
"R3M: A Universal Visual Representation for Robot Manipulation"
"Unable to install transformers & its related dependencies due Issue with Python Versions"
"Is it possible to train all the models available in hub using deepspeed?"
"Edgeformer"
"How to calculate GPT-2 sentence loss (for each sentence) if batch has 2 or more sentences?"
"`sequences_scores` does not compute the expected quantity for beam search"
"Can't load config for 'DeepPavlov/xlm-roberta-large-en-ru'"
"RuntimeError: params[0] in this process with sizes [253991, 1024] appears not to match sizes of the same param in process 0"
"`Wav2Vec2FeatureExtractor` and `Speech2TextFeatureExtractor` tests fail for truncating `max_length` values"
"Providing a easier way to download pre-trained models."
"Add TF implementation of XGLM model"
"Train on custom translation dataset - machine translation"
"PerceiverIO Attention maps"
"m2m-100 finetuning messes up lang pairs"
"1"
"clean_up_tokenization_spaces=True won't clean up spaces"
"How can I use MLM and NSP to train Bert from scratch?"
"Is there a possible way to moniter the realtime training  in the example?"
"How can i train wav2vec2 with my dataset?"
"HF TF models can be used with TFX"
"Possible bug: Only truncate works in FeatureExtractionPipeline"
"DeBERTa from official example return random logits that change every time I run the same example."
"T5base loss function "
"Deberta gives completelly wrong and random output tested on multiple machines and multiple versions of transformers"
"[Benchmark]"
"How to improve accuracy (Model on TrOCR)"
"Export longformer to ONNX"
"Error when training LayoutLMv2 model"
"TEAMS: Training ELECTRA Augmented with Multi-word Selection"
"after pip install -e .  pip list does not show the transformers package"
"Flax LM scripts doesn't scale-up on TPU Pod."
"RuntimeError: Empty or `None` reference sentence found."
"MNLI metrics overwritten "
"Embedding size mismatch when hyperparameter search"
"run_clm.py crashes server."
"Training MarianTokenizer with sentencepiece"
"Cannot save TFDebertaV2ForSequenceClassification as SavedModel via saved_model"
"MarianMT: doubling batch_size has no effect on time taken"
"segformer variants"
"`Trainer` does not support deepspeed `reduce_bucket_size=\"auto\"` for DS levels <3"
"[TODO] Investigate equivalence tests"
"Batch tensor creation error when finetuning gpt2"
"Trainer: Support scheduler that reduce LR on loss plateau."
"Script to convert huggingface models to deepspeed/megatron checkpoints"
"Tensorflow language modeling example doesn't work with TPU in Colab "
"CANINE model gets different logits for different batch sizes"
"decoder"
"ONNX exported BART model with seq2seq-lm-with-past feature produces error on the initial run."
"Onnx export Data2vexAudio ValueError: Model and config inputs doesn't match"
"Typo in documentation."
"Token classification pipeline results different with tokenizers==0.11.6 vs tokenizers==0.12.0"
"Request: TokenClassification pipeline batch processing over a sequence of already tokenised tests"
"convert_tokens_to_string does not conform to its signature"
"Regression: ONNX export fails on Pytorch ToT (NVIDIA 22.03 pytorch container)"
"GPTJ6-B python process seem to be stuck forever at ```from_pretrained``` method "
"Tensorboard logger logs to same directory when trial is pruned using hp_search and optuna"
"ONNX causal-lm-with-past conversion: attention_mask dtype changed"
"Is gradient checkpointing really needed when fine-tuning LED on 16384 tokens?"
"Issue with aggregation_strategy=\"max\" in NER pipeline"
"Enable reproducibility"
"pytest throws ImportError: cannot import name 'json' from 'itsdangerous'"
"How loss is calculated in MLM transformers training and the calculation/Intuition behind it?"
"t5-large model OOM with FP16, but run well without FP16"
"Error when running \"Quick Tour\" code snippets"
"wav2vec2 : Speech to text conversion fails when using large file "
"Question about Bigbird Random Attention Mechanism/possible bug"
"How to create vocab.txt for run_mlm.py"
"KeyError: 'logits'"
"Force Alignment with Wav2Vec2 models"
"can i use the transformers pretraining script of T5 as mT5 ?  "
"Fine-tuning longformer for Question Answering"
"about attention value"
"How to fine-tune the models on huggingface hub with run_mlm.py"
"Running Error: Trying to backward through the graph a second time"
"Fill-Mask Pipeline BERT on a sentence ending with/without a dot"
"Problem generating pytorch_model.bin for wav2vec2 large model using convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py"
"Why does BART generate additional EOS token at the beginning?"
"ByT5 parallelization"
"debert TypeError: _softmax_backward_data(): argument 'input_dtype' (position 4) must be torch.dtype, not Tensor"
"RFC: `torch==1.12` will toggle `torch.backends.matmul.allow_tf32` to `False`  - what should we do?"
"Why is wandb being logged in and how to turn it off?"
"GPT Neo/J Padding Side Results in Different Generation Outputs"
"[BUG] bf16 incorrectly configured in src/transformers/deepspeed.py"
"Only get generated text from Bart Generate"
"Specifying num_labels in from_pretrained without id2label leads to unexpected error"
"version 4.2.0 vs newer version"
"Convert PyTorch Model to Hugging Face model"
"Trouble exporting resolve_conj operation to ONNX"
"change num_attention_heads=[1, 2, 5, 8] to [1,2,4,8]"
"KeyError: 'loss' while trying to finetune Bart model on custom dataset for summarization."
"[Community event] HugGAN sprint for training AI art with free compute"
"3-dimensional attention_mask in LongformerSelfAttention"
"Fill-in-the-Blank Text Generation of T5"
"Improving T5 Docs"
"`bigscience/T0` multi-gpu inference exits with return code -9"
"Can't load the model for 'bert-base-uncased'."
"state.best_metric does not update in EarlyStoppingCallback"
"BART can only generate a maximum of 20 tokens"
"Add missing tokenizer test files [:building_construction: in progress]"
"`transformers.ViltProcessor` requires torch >= 1.10"
"MegatronBertForMaskedLM"
"Wav2Vec2 Conformer Encoder"
"Multiclass evaluation not working"
"After using HFTracer, the Bert model can't be trained"
"A tool to generate this library with (a) specific model(s)"
"Finetuned Wav2Vec2+LM Inference on my `wav` audio files "
"ModelForSequenceClassification (Roberta/Bert) don't support mean_pool"
"Tokenizers setter of ids of special tokens don't work"
"MT5ForConditionalGeneration has model.config.max_length=20 by default. Why?"
"add dataset metadata to model card"
"Bug in Marian model (or tokenizer) in transformers==4.18.0"
"ASR Pipeline: End of transcripts missing when chunking enabled "
"Can't load a local finetuned state dict anymore without loading the official pretrained weights first "
"Adding new tokens to RobertaTokenizer gives very strange results - probably a bug."
"The accuracy of test set is different in training and evaluating Bert"
"`FlaxBartForConditionalGeneration` should not require `input_ids` when `encoder_output` is provided"
"Trying to Train Lonformer but from standard transfomer file, error AttributeError: module 'wandb' has no attribute 'run'. Even when I have not install Wandb"
"`LongT5`: Efficient Text-To-Text Transformer for Long Sequences"
"Option to change Ray's gridsearch scope"
"`FlaxBartForConditionalGeneration` has a `.encode` method but `BartForConditionalGeneration` does not"
"Can't load pretrained TrOCR model"
"Cannot train M2M100 using run_translation.py and DeepSpeed ZeRO stage 3 "
"`\"histogram_cpu\" not implemented for 'BFloat16'` when using deepspeed and reporting to wandb"
"A question about the position of language indicator tokens of mBART"
"ViLT Fine-tuning Bug: ValueError: operands could not be broadcast together with shapes"
"Question: Add Embedding layer to BERT"
"Enable ONNX support for multiple-choice classification heads"
"ViLT vs VIT Classifier heads question"
"Optional keys in TrainingArguments aren't always labelled as such"
"group_texts function in language-modeling seems get wrong"
"Cuda Memory leak (OOM) when using HF Trainer DDP mode"
"AutoConfig.from_pretrained can fail with Tokenizers"
"AutoModelForMaskedLM produces NaN if no token is masked"
"ValueError if answer in truncated table rows and columns in Tapas tokenization"
"Predicting incorrect loss when eval data size is not a multiple of batch size"
"[modeling] keys to ignore revisited"
"ResumableUploadAbortException: 409 The object has already been created in an earlier attempt and was overwritten, possibly due to a race condition."
"[FlaxBartForCausalLM] Embed tokens not loaded in Flax decoder model from encoder-decoder weights"
"[PegasusConfig] wrong default vocab_size"
"[Flax] Torch fp16 model weights not upcast when loaded in Flax"
"Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2Model"
"KeyError when using AutoTokenizer for facebook/detr-resnet-*"
"Tensor size mismatch in RoBERTa"
"pointer to transformer (big) model"
"Large differences between T5 weight initialization in TF and torch"
"Batch size < GPU number when training with Trainer and deepspeed."
"`translation_XX_to_YY` pipeline warnings about no max_length when both max_length and truncation are provided"
"ValueError: Reference at 'refs/heads/master' does not exist"
"How to use Wav2Vec2ProcessorWithLM in pipeline?"
"NER training crash"
"Missing commas causing concatenation"
"Invalid CLS masking in question answer pipelines top K calculation"
"Some tests misusing assertTrue for comparisons"
"loading roberta from local file"
"How to initialize BigBird Encoder-Decoder model with weights of full_attention Transformer Encoder model like BERT"
"How to set different LOCAL_RANK env variable values for multiple GPUs of a single node machine with Accelerate"
"Question about google/bigbird-pegasus-large-pubmed"
"Why do we need to expand the token_type_ids?"
"Resuming Language Model Pre-training spends too much time skipping data"
"Special Tokens Not Working as Expected in Bert Tokenizer"
"Batching does not work with any XGLM model (simple script to reproduce issue included)"
"Quick question about efficiently initializing model parameters"
"[Request] Add Wav2Vec support for onnx conversion"
"Problem at using CLIPFeatureExtractor from transformers.models.clip.feature_extraction_clip"
"Small Bug: text2text_generation and text_generation result in duplicate results"
"DataCollatorForLanguageModeling using the wrong symbol for masking"
"\ud83d\udc1bDiverse Beam Search BUG"
"Add support for moving Trainer out of GPU memory"
"ValueError: too many values to unpack (expected 2)"
"Blenderbot export issue in ONNX and C++"
"use transformers on apple mac m1 (TF backend)"
"[Benchmark]"
"Missing activation Function"
"Confusion about past_key_values and attention_mask in GPT2Attention"
"huggingface-cli login in dockerfile"
"Inference/prediction ValueError using BART"
"(TF) model.generate to tf.function for tf serving"
"Tranformers documentation translation to Portuguese"
"Repeated Generation"
"GPT2.generate() with custom input_embeds argument returning tensor (1*max_length) instead of (batch_size*max_length)"
"Dropping support for Python 3.6"
"no attribute ViTFeatureExtractor"
"TF: XLA model output differs when certain outputs are passed"
"Converting PyTorch to ONNX model doubles file size for Deberta v3. Not case of renaming."
"CodeT5 tokenizer.model_max_length is 1000000000000000019884624838656"
"AutoFeatureExtractor not respecting override parameters (for LayoutLMv2FeatureExtractor)"
"Range Error for BERT Masked Language Modeling on IMDB"
"get the error \"_forward_unimplemented() got an unexpected keyword argument 'labels'\""
"Could not load model deepset/minilm-uncased-squad2"
"facebook/detr-resnet output tensor contains all nan value "
"Docs link to deepspeed infinity redirect to a 404"
"`offset_mapping` is strange for non-ascii token"
"How to convert the bert model in tf1 ckpt format which converted by the pytorch version to tf1/tf2 SavedModel?"
"Checkpoints are NOT save in GCS (Google Cloud Storage)"
"Language model example hanging"
"Module import takes too long"
"DeBerta unable to load from cache"
"OpenAI's Jukebox for music generation "
"The bare ViLT Model not working with CUDA ?"
"Quick tour_ AutoModel Introduction"
"[tracker] Sharding huge models process and current status"
"Tensorflow to Onnx change batch and sequence size"
"KeyError: loss when pretraining using BertForPreTraining"
"LED Model returns AlgorithmError when using SageMaker SMP training"
"Datasets: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'"
"ValueError: cannot find context for 'fork' when processor_with_lm.batch_decode(_logits)"
"T5base memory usage for interface"
"Source link of transformers.pipeline is broken"
"Missing whitespaces at RuntimeError message"
"Name of LayerNorm parameter in RobertaLMHead."
"Mask Token Spacing in RobertaTokenizer"
"push_to_hub on custom tokenizer causing a flood of \"deadlock\" messages"
"Missing `f` prefix on f-strings"
"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking"
"[BigScience176B] Model conversion from Megatron-LM to transformers"
"A bug in modeling_ibert.py"
"[Generation] `length_penalty` means `beam_alpha`"
"ZeroShotClassificationPipeline not using GPU"
"Adding tokens to `RobertaTokenizer` is fast, but loading the extended tokenizer from disk takes tens of minutes"
"tokenizer return_special_tokens does not work correctly with custom special tokens"
"Segmentation fault whenever trying to load model"
"Labels shift in seq2seq example"
"Pretraining code of LayoutLMv2"
"LayoutLMV3 "
"cannot import name 'Data2VecForCTC' from 'transformers'"
"config.json not found!"
"How to train over VERY LARGE dataset?"
"Word limit with mBART-50 translation"
"Can't reproduce training of wav2vec2-large from documentation"
"cannot import name 'RegNetModel' from 'transformers'"
"AttributeError: 'DataParallel' object has no attribute 'save_pretrained'"
"Issue in reformer: Reformer doesn't depend on its key feature -- `LSHSelfAttention`"
"Trainer: TypeError: an integer is required (got type NoneType)"
"Bug: Finetuning large models resume checkpoint error"
"Data collator using in Parallel training & Disable to use DistributedDataParallel"
"Exporting DeBerta using custom onnx configuration"
"how huggingface process uneven input tensors"
"Getting a fixed size embedding from the last hidden state."
"Beginning word ids"
"Warning tells you you will get indexing errors in T5 for going beyond max length"
"Memory calculator for transformer models"
"The current equivalent of transformers.models.bert.modeling_bert.gelu"
"Undocumented distributed inference behaviour for `run_summarization.py`"
"Question on model_max_length (DeBERTa-V3)"
"Padding vs truncation logging mixup"
"Text Generation for decoder"
"HuggingFace/BigBird RuntimeError: Internal: src/sentencepiece_processor.cc"
"BertEmbeddings import missing for Torch in __init__ file"
"Transfomers Pipline: Batching does not work for Sentence-Pair Text Classification "
"[Data2Vec] Incompatibility with the original implementation"
"[Data2Vec] Incompatibility with the original implementation"
"Optionally return past key values from generate"
"Missing torch.no_grad in run_xxx_no_trainer.py"
"wavlm s3prl emotion recognition"
"force_words_ids not working"
"Bert: relative_key position embedding causes error for long sequences"
"Adding a ISSUE_TEMPLATE for the translation of docs"
"Training a tokenizer - add argument for preprocessing the input"
"[Trainer]: Resume training with `save_strategy=\"epoch\"` does not load RNG state"
"Multi GPU training crashes when running run_mlm_wwm.py"
"Make DETR `pixel_values` input optional"
"Unable to reproduce gigawords results from google/pegasus-gigaword"
"error with Vision Transformer (ViT)"
"No separation between Torch and TF examples in create_a_model.md"
"Collection of Tokenizer issues"
"ValueError: too many values to unpack (expected 2) using BERT to training"
"AutoConfig.from_pretrained(\"model\", return_unused_kwargs=True) returns `\"_from_auto\": True` field against specification"
"Chinese parentheses can't be handled by fast tokenizer"
"End2End RAG training hangs"
"symbol not found in flat namespace '__ZNSt8ios_base4InitC1Ev'"
"Incorrect check for MLFlow active run in MLflowCallback"
"KeyError \"labels\" occurring in distill_classifier.py official example notebook"
"Why training time is much more than same task in Fairseq?"
"When will the official 4.19 release be?"
"comet ml integration - add option to continue training"
"Error in mBART50 fast tokenizer `convert_tokens_to_string` method"
"Broken 'export' using transformers.onnx"
" CUDA out of memory in evaluation_loop"
"Import transformers and datasets not possible"
"set_verbosity_info not working"
"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio"
"Docs link to example scripts is broken"
"Tokenization in run_mlm is not correct (maybe)?"
"TypeError: forward() got an unexpected keyword argument 'labels' with mt5-small"
"Socket Timeout when using DDP "
"How to deal with multiple sequences with T5ForConditionalGeneration "
"DebertaV2Tokenizer wasn't added to the __init__ file  "
"Error on loading saved optimizer after training (zero-3)"
"In a multi-gpu setup, use a fraction of GPUs for training and the other fraction for some other computations "
"Pretrain on Wav2vec2 getting parameters did not receive grad for rank0."
"No way to get ONLY the generated text, not including the prompt."
"GPT2: Perfect training and evaluation loss, but terrible test-time performance"
"Training issue of a Transformer based Encoder-Decoder model based on pre-trained BanglaBERT"
"Encoder-Decoder model after fine tuning on Turkish dataset, generation gives the same results regardless of the input"
"is it possible to split the embedding layers from bert during fine tuning and inference"
"is it possible to split the embedding layers from bert during fine tuning and inference"
"MarianTokenizerFast requested"
"fixing VisibleDeprecationWarning"
"Resumption of training using the same parameters as before fails with CUDA out of memory error."
"Dataset streaming example not working"
"changing Encoder's embedding table in MarianEncoder"
"Speed up Hugging Face Models with Intel Extension for PyTorch*"
"Match the indexing output of `TokenClassificationPipeline` with labels (e.g. `ner_tags`)"
"Overflow when normalizing start and end logits in QA pipeline"
"'BaseModelOutput' object has no attribute '_OrderedDict__map' when using Wav2Vec 2.0"
"Missing sentencepiece model for remBert"
"pre-training deberta model on TPUs"
"[Trainer]: Resume training not consistent with large effective batch size"
"Mismatching between sequences and scores in beam_search"
"Inconsistent output of code example in Proprocessing Chapter in Docu"
"Set Transformer"
"How to change the Text embedder(Layoutlmv2Tokenizer) in huggingface LayoutLMv2 model?"
"ImportError: cannot import name 'ESMForMaskedLM' from 'transformers'"
"T5 zero-shot classification pipeline"
"model google/muril-base-cased has effectively infinite model_max_length"
"ValueError: The tokens {'null'} are defined in the tokenizer's vocabulary, but not in the decoder's alphabet. Make sure to include {'null'} in the decoder's alphabet."
"Forward outputs on multiple sequences is wrong"
"Unable to retrieve layers from model in tensorflow"
"Fine tunning error in /models/t5/modeling_t5.py"
"Mistake in the BART doc & inconsistency between code & doc"
"[run_seq2seq_qa.py] various issues"
"Different logits for single/batch inputs on T5ForConditionalGeneration"
"almost all codes that related to generation in examples/pytorch/**_no_trainer.py have bugs"
"a memory leak in qqp prediction using bart"
"Add UL2: Unifying Language Learning Paradigms"
"Add Visual Question Answering (VQA) pipeline"
"Thanks"
"CUDA out of memory in Seq2SeqTrainer class "
"Bug of the text-classification in examples"
"-1e9 constants in T5 implementation"
"LayoutLMv2 Fast Tokenizer improperly aligns labels for non-first subwords with 0 offsets"
"(T5) tf.function wrapped model.generate() does not produce the same result as non-wrapped model.generate()"
"ALBEF: Align Before Fuse"
"Add RWKV2 (fast)"
"bug in modeling_tf_wav2vec2"
"add FAN model (vision)"
"[Longformer] Issues with \"is_index_masked\" when using single encoder layer"
"Distributed Support for OPT models in transformers"
"Question answering pipeline: error for long text sequences when `max_seq_len` is specified"
"Error in Loading the Feature extractor "
"gpt2 model parallelization test failed"
"Support MobileBert model in transformer.onnx package"
"torch.cuda.amp.autocast not worhing in huggingface nlp models."
"RAG - ValueError: Columns ['embeddings'] not in the dataset. Current columns in the dataset: ['title', 'text'] "
"run_clm.py exits with error -9 on checkpoint restart"
"TROCR truncating output string"
"Missing of token_type_ids parameter in OPTForCausalLM.forward"
"Problem with Adding LayerNorm after BART's Encoder for Summarization"
"OSError Directory not empty error in Trainer.py on checkpoint replacement"
"Loading facebook/regnet-y-040 FeatureExtractor fails mysteriously unless pillow is installed"
"Swin Transformer V2"
"How to input word2vec embeddings to gpt2 model?"
"Add pipeline for cross-modal / uni-modal ranking"
"`LayoutXLMProcessor` returns unexpected `offset_mapping` "
"GPT-neo generate is ignoring passed position ids"
"TF: all models can run in Graph mode"
"Accepting torch device objects in the Pipeline init "
"Misleading error when from_pretrained fails, says there are flax weights when there aren't"
"ValueError: If training, make sure that config.axial_pos_shape factors: (512, 1024) multiply to sequence length. Got prod((512, 1024)) != sequence_length: 1024. You might want to consider padding your sequence length to 524288 or changing config.axial_pos_shape for ReformerForSequenceClassification"
"about the opt model KeyError: 'opt'"
"Error while finetuning XLM-RoBERTa on Tensorflow-Keras"
"Pipeline inference with text pair is broken"
"404 Errors on Loading Artifacts due to mispellings could suggest a model/tokenizer/dataset to the API user in the error message."
"UNETR: Transformers for 3D Medical Image Segmentation"
"implement onnx config for SqueezeBert"
"M1 MacOS: \"OSError: Can't load config for 'bert-base-uncased'\""
"automatically update config file for models"
"There is a minor bug in run_pretrain.py for Wav2Vec2 example"
"Shape mismatch with documentation for cross attentions tensor when performing sequence generation with encoder-decoder model"
"issue with loading pretrained model using DeepSpeed Zero Stage 3 "
"add doctests for data2VecText"
"Google's Trillson Audio Classification"
"Add support for MacOS Apple Metal \"mps\" backend"
"Generate.py doesn't support gpt-j"
"Illegal instruction (core dumped) error: PowerPC8"
"failed doctests examples for data2vec_text, 5 failed, 2 passed"
"failed doctests examples for data2vec_audio, 1 failed, 4 passed"
"ImportError: cannot import name 'AutoProcessor' from 'transformers' "
"ZeroDivisionError: division by zero"
"[BigBird] random attention in FlaxBigBird is not random"
"Example of TFMarianMTModel is not working very well"
"VisualBERT: Low accuracy on VQA v2"
"Remove/ablating particular head in a Transformer model"
"About model loading without parameter"
"TFGenerationMixin.generate should support a parameter such as logit_mask"
"Nana123"
"Export Generated Text 1 Token at a Time"
"`_fast_init` overwrites weights passed to custom model"
"tokenizer object incorrectly modified in PreTrainedTokenizerFast.train_new_from_iterator() "
"Text2TextGeneration Pipeline : Batch size and num_return_sequences are not working together"
"fill-mask target for full words not enabled?"
"'lm_head.weight' is improperly not initialized when loading BART weights into BartForCausalLM"
"Same sequence gets different token probabilities depending on whether it's generated from sampling or beam search"
"OPT-350M Throws Error On Load after Finetuning"
"Allow creation of tokenizer from a vocab dictionary"
"AutoTokenizer _batch_encode_plus method don't have add_prefix_space argument"
"[Deepspeed alternative] PatrickStar"
"Inconsistency multiple mask in fill-mask"
"check min version"
"typo IBERT in `__repr__`"
"[RFC] Scan & Gradient checkpointing in Flax"
"illegal hardware instruction"
"Error in TAPAS Tokenizer "
"No 'Translation template'"
"Unable to instantiate ImageGPTFeatureExtractor"
"NotebookProgressCallback doesn't work in databricks notebooks properly--should either be fixed or removed from trainer automatically if it is a databricks runtime"
"TFBartForConditionalGeneration.generate is very very slow\uff0cbut not BartForConditionalGeneration.generate\u3002"
"can't run (TF)BartForConditionalGeneration.generation on GPU, it's speed very very very slow"
"wav2vec2 multi-node training problems in a shared file system"
"Different behaviours for `tf/flax` and `pt` on `generate(max_length = len of input id)`"
"DEIT -Some weights of the model checkpoint at facebook/deit-base-patch16-224 were not used when initializing DeiTMode"
"XGLM onnx support"
"Inconsistent behavior in generate when output_scores=True"
"The tokenizer config for OPT-30B is missing a pad token"
"raise ValueError(\"You have to specify either input_ids or inputs_embeds\") "
"Logits size does not match vocabulary size when fine-tuning Hubert large with pyctcdecode"
"Mismatch of special token ids between config and tokenizer config"
"Make all configs nicely readable"
"OPT produce NaN during batched generation"
"Train Transformer XL language modeling with padding "
"ImportError: cannot import name 'OptionalDependencyNotAvailable' from 'transformers.utils'"
"attention_mask hold float values in [0,1] in T5"
"May i just train a translation task with T5 from scratch without pretrain a language model?"
"NaN in GPT NeoX model (generation)"
"gpt-neo-1-3B large memory increase during training even with a small training dataset"
"XLM-Roberta offset mapping is off by one in case of whitespace-subwords"
"ProphetNet inconsistent with changing batch ordering"
"Tranformers documentation translation to Italian"
"fx.symbolic_trace not working for Roberta"
"Spanish docs - Links don't work"
"transformers model doesn't output zeros for padded subtokens"
"ValueError: AlbertForMaskedLM does not support gradient checkpointing."
"Add MVP model"
"Error building docs locally:  No matching distribution found for ray[tune]; extra == \"docs\""
"Any attempt to export any model to onnx returns an ATOL value of nan."
"Number of channels in the ViTMAE model"
"Add support for pruning whole layers in transformer models."
"PyTorch JIT trace on Swin Transformer pretrained checkpoint fails"
"Training hangs in the end while calling dist.barrier()"
"MarianMTModel no longer has postprocess_next_token_scores function"
"modeling_swin.py img_mask doesn't have expected torch dtype "
"model.parallelize() for OPT models "
"Performance (perplexity) decrease after conversion megatronGTP2 to hugging face model"
"Transformers get stuck in from_pretraind"
"How can i use bpe tokenizer  in t5 pretrain from scratch "
"_batch_encode_plus() got an unexpected keyword argument 'is_pretokenized' using BertTokenizerFast"
"`do_eval` is True when setting `do_predict`=True"
"GPT-2 Forward w/ and w/o caching of past values gives different results"
"bad_words_ids not working"
"Training large huggingface models on Azure with CUDA? [OPT]"
"Finetuning AudioFrameClassification model"
"Flax OPT batch generation test"
"NameError: name 'save_offload_index' is not defined when use --model_revision sharded"
"Loading BertModel from BertForMaskedLM without randomly initializing weights"
"Support returning raw logits in `generate`"
"Not able to import tensorflow OPT"
"Is the addition of the 'OPTforSequenceClassification' class scheduled?"
"center_crop in image_utils.py is broken for inputs that are not PIL Images"
"Issues with mypy when using Transformers"
"MarianMT Doesn't export to ONNX correctly"
"How to use finetuner.py to train t5-large model"
"require_accelerate wrapper missing? "
"Loading sharded model in `tf` from pytorch checkpoints"
"Will data be loaded into multiple GPUs automatically?"
"TFRemBertModelTest.test_resize_token_embeddings not working"
"[RAG] token discrepancy between question token which should be input to generator and the one actually encoded in postprocess_docs()"
"Lazy load in pipelines"
"word_ids() is not available when using FlauBERT"
"Descriptors cannot not be created directly."
"Repetitive sampling generations from opt1.3b but not from opt350m"
"Character limit when tokenizing?"
"T5ForConditionalGeneration does not require resize_position_embeddings when input sequence length is longer than 512?"
"Save a Pytorch Bert finetuned model with custom forward function and heads with Hugginface"
"TokenClassification with DestillBert does not learn"
"Add a stop_sequence option to text generation pipeline"
"Shard checkpoint for `tf` and `flax`"
"Trouble parallelizing GPT-NeoX"
"Permission denied"
"LayoutLMv3 not downloading via official code samples"
"Microsoft's SpeechT5 for Spoken Language Processing (ASR, TTS, ST...)"
"MT5Model(MT5Config()) fails with AttributeError: 'MT5Config' object has no attribute 'is_gated_act'"
"Enabling vilt and flava auto feature extractor."
"resize_token_embeddings in BartForConditionalGeneration doesn't change lm_head size"
"could not run distribution cpu training on two CPU sockets using miprun"
"Loading repository after rename does not work (with old name)"
"Probable bug with `torch_dtype=\"auto\"`"
"ONNX support for gpt-neox"
"TrainingArguments with pytorch on Mac: AttributeError: module 'torch.distributed' has no attribute 'is_initialized'"
"Summarization output different when using pipelines and the on-website inference engine."
"BigBird tokenizer - special tokens not being masked during MLM"
"Pyramid Vision Transformer"
"sharded_ddp with auto_wrap is secretly a no-op"
"Error with inferencing with fine-tuned model loaded from keras load_model"
"OSError: Can't load config for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing a config.json file"
"trocr model performs worse than unilm version"
"SSLError: HTTPSConnectionPool(host='huggingface.co', port=443)"
"FX tracing of HubertForSequenceClassification fails with TypeError: 'HFProxy' object cannot be interpreted as an integer"
"Unable to run models bert/roberta/others w. FP16"
"Add Checkpoint Loading from MLflow Model Registry"
"AttributeError: 'TFBertForQuestionAnswering' object has no attribute 'save_pretrained"
"GPTNeoXForCausalLM examples fail to run"
"UnboundLocalError when running run_glue.py"
"Abnormal behavior of OPT except OPT-350m"
"Can we do adaptive pretraining on BERT-related models using transformers?"
"Bug with .from_pretrained in distributed mode on high-ram Colab instances + Accelerate"
"[TRACKER] Add BLOOM on `pipeline`"
"Transformer Vit-MAE hard coded image channels"
"[Default pipelines] Add a revision tag to all pipelines"
"run_ner.py is slower than run_ner_no_trainer.py"
"fine-tunes RoBERTa on WikiText-2"
"AutoTokenizer fails to do_lower_case"
"Problems when producing distilBERT"
"Typo in adding_a_new_model README"
"trainer fails when fsdp = full_shard auto_wrap"
"Truncation + max_length not working for GPT2TokenizerFast"
"Disregard"
"how can I use emformer checkpoint?"
"clm example training script uses larger train/eval data than it should"
"GPT-2 based models generation breaks when adding new special tokens"
"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)"
"In run_mlm.py the group_texts function incorrectly splits the text into lists of chars"
"Difference in the number of data during deep learning"
"TypeError: can't pickle _thread.lock objects"
"Need the ability to modify PSM values for Tesseract call in LayoutLM V2/ XLM / V3 Processor"
"Add Flax implementation for BLOOM"
"Cannot run run_qa.py due to \"ImportError: cannot import name 'send_example_telemetry'\""
"mBART generate random strings in end of sentence"
"QuestionAnsweringPipeline returns full context in Japanese"
"DensePhrase: StopIteration: Caught StopIteration in replica 0 on device 0."
"SegFormer feature extractor  `do_normalize=False`"
"`max_length` and `stopping_criteria` in generate()"
"Input Packing"
"SimMIM output num_channels should not be hardcoded"
"CI Tests are failing in \"run_tests_pipelines_tf\""
"Issue with trainer.py class line #1460, 2643 and 1745.  "
"Issue with trainer.py Line#1022,1025,1035,1043,1051,1059,1061"
"The messy code generated by opt125m."
"importing 'LongT5Model' from 'transformers'"
"InvalidGitRepositoryError while running distillation train example"
"GPT-NEOX RuntimeError"
"Is there Any difference of performance when finetuning bert use the huggingface or the google official code?"
"Problem with GPU"
"Add easy extensibility of `logits_processor` to `generate`"
"Test DataLoader never uses multiple workers"
"`Trainer` has a weird way of determining whether a TPU device is present"
"Text classification pipeline outputs differ with 4.20"
"GPT-NeoX missing Tokenizer"
"Problem during the training with the parameter train_dataset. (Dict/Tensor problem)"
"feat: pipeline registry for supporting custom pipelines"
"How to checkpoint TFAutoModelForSequenceClassification every k batches"
"Snacky Brain Bites for HF Transformers"
"Converting a tensor to a Python boolean might cause the trace to be incorrect when converting gpt2 to onnx format"
"RHO loss"
"Dataset Format for training RAG on custom Dataset"
"KeyError: 'src_texts' in train_distil_marian_enro.sh"
"TF element-wise equals requires tf.equals() instead of == "
"assertEqual of non-frozen parameters in test_resume_training_with_frozen_params"
"OPT-350m cannot be loaded from local files generated using the save_pretrained method"
"__init__() got an unexpected keyword argument '_name_or_path'"
"IndexError with Reformer Model when padding the sequence"
"Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors"
"Big Model Inference: OOM with simple forward pass"
"Properties of unset special tokens return the string 'None' in non verbose Tokenizers"
"`seed_generator` would casue tensorflow gpu memory growth immediately"
"Unable to Load the Pre-Trained Model using Spark-Submit "
"AutoTokenizer vs. BertTokenizer"
"Constrained Beam Search outputs duplication and weird results"
"push_to_hub returns \"OSError: error: RPC failed; HTTP 408 curl 22 The requested URL returned error: 408\""
"Cannot import name 'load_offloaded_weights' from 'accelerate.utils'"
"How to use LayoutLMv3 for Document Layout Detection  task?"
"RNG states in checkpoint corrupted"
"hf_BigBird failing on torchdynamo"
"DisjunctiveConstraint fails in corner case"
"Multi-modal VisualBERT can be used for classification task?"
"LayoutLMv3Model output shape is different"
"Issue in wav2vec2ForPretraining"
"Batch mismatch in the given course example"
"Get different weights from model.get_input_embeddings()"
"Raise AttributeError on training deberta models with gradient checkpointing"
"ViTForImageClassification"
"Add `trace_device` argument to `smp.DistributedModel` call in `Trainer()`"
"Fail when using pipeline for the inference of DeBERTa-Vx ORTModels"
"LayoutLMv2 training on sagemaker error: undefined value has_torch_function_variadic"
"Bad readme for HF model codeparrot/codeparrot-small"
"Text generation: Unexpected behavior when input ends with newlines"
"RegexTokenizer"
"layoutxlm model can not convert to onnx"
"Calling `generate` on a `T5ForConditionalGeneration` returns `n` tokens but `n-1` scores"
"run_clm with gpt2 and wiki103 throws ValueError: expected sequence of length 1024 at dim 1 (got 1012) during training."
"Inference API failing: `\"Unknown error in run_once : postprocess() got an unexpected keyword argument 'return_all_scores'`"
"Wav2Vec2ProcessorWithLM degraded performance when transcribing multiple files"
"KeyError: 'logits'"
"Copied \"Fine-tuning a masked language model\" tutorial, got error on last step - training"
"Exception encountered when calling layer \"tf_bert_for_pre_training\" (type TFBertForPreTraining)"
"Attention gradients for models"
"Replicating RoBERTa-base GLUE results"
"Pruning function in T5Attention doesnt affect _relative_position_bucket"
"Ambiguous positional embedding management in LongformerEmbeddings"
"Trainer in `run_image_classification.py` removes necessary `\"image\"` column for evaluation"
"Deploying a  pytorch-pretrained-bert on mobile"
"new Transformer update causes an error with TPU XLA implementation"
"Wav2vec model further training [RuntimeError: you can only change requires_grad flags of leaf variables]"
"Silero Models License Infringement"
"Training loss doesn't decrease on TPU while works fine on GPU"
"\"num_examples\" incorrect when using IterableDataset"
"\"zero-shot-image-classification\"  pipeline with `VisionTextDualEncoderModel` needs manual feature_extractor and tokenizer input"
"TF: XLA generation not working properly in some models"
"Getting only <|endoftext|> token  in GPT-NEOX-20B model"
"Unable to fine-tune WMT model"
"Decision Transformer Position Embedding Incorrect Implementation"
"Consider adding  \"middle\" option for tokenizer truncation_side argument"
"Trainer.predict multiple progress bars"
"codegen-16B-mono (Salesforce) fails to load tokenizer and model"
"tune save checkpoint throwing error due to float32"
"`dlopen: cannot load any more object with static TLS` after installing sentencepiece"
"ERORR: \"Missing XLA Configuration\" while running the script?"
"Suggestion for introducing \"shift_labels\" argument for Trainer"
"IPEX integration in Trainer breaks with PyTorch 1.12"
"TrainingArguments does not support `mps` device (Mac M1 GPU)"
"openai's CLIP model not working with pytorch 1.12 in some environments"
"Grid search ProgressCallback leads to encoding issue on Windows"
"rust_model.ot not saved by save_pretrained()"
"Training with fp16 precision gives nan in Longt5"
"Exception: EOF while parsing a string at line 1 column 8862550"
"--save_on_each_node doesn't save pytorch_model.bin when deepspeed zero3 is used"
"issue installing SentencePiece"
"BART with multiple encoders. "
"Result of T5 tokenizer doesn't match"
"input_inds greater than the vocab_size in Question answering example"
"can not covert LayoutLMv2ForRelationExtraction model to onnx"
"Data collator not working with Masking language model of Bart type"
"Optional type of lengths causes slow speed in LengthGroupedSampler"
"Add DFFT"
"Negative CTC loss while training TFWav2Vec2ForCTC model"
"Constraint decoding using long constraints, model keeps resetting the constraint"
"LongT5 Models Are Not Initialized With Pretrained Weights"
"Trainer dataloader_drop_last=True looks can not work with distributed setting"
"trainer.push_to_hub() doesn't work"
"`log_metrics` method of `Trainer` is using `print` instead of `logging`"
"Evaluation results of `run_qa.py` are anormally low"
"Pretraining BART language model"
"Cannot compile T5 for inferentia"
"Model  LayoutLMv3  - error feature_extractor not find pytesseract"
"Distributed training for streaming dataset"
"Flax models should allow `inputs_embeds`"
"Error occur when using DDP and include_inputs_for_metrics"
"[Wav2Vec2ForCTC] multi-GPU training's backward hangs"
"ValueError: You have to specify pixel_values in CLIP for ver >= 4.18.0 "
"EfficientFormer"
"checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete] TypeError: slice indices must be integers or None or have an __index__ method"
"Add Support for \"No Language Left Behind\" (NLLB)"
"Trainer.train always saving model every 500 steps, regardless of input training args [fixed, user error]"
"Different sentiment class probabilities for sequential processing vs batch processing"
"NaN in XGLM Softmax with FP16"
"A basic NLP question regarding NER task"
"Configurations should support id2label as a list"
"Include `timeout` attribute (related to DDP) to TrainingArguments"
"NLP HydraNet"
"[bloom] Add alibi cache for fixed inputs"
"LED Model returns AlgorithmError when using SageMaker SMP training #16890"
"Add TF implementation of LongT5 model"
"An example for finetuning FLAVA or any VLP multimodel using trainer (for example for classification)"
"StoppingCriteria \"scores\" is always None"
"Warnings for unexpected parameters when resuming training."
"Composite models (encoder-decoder) behave differently across PyTorch and TensorFlow"
"Cannot successfully convert convnext models to onnx using transformers.onnx"
"Have 'Random Crop' option for truncation_side for Tokenizer "
"attention_mask bug when training Wav2Vec2ForCTC with DeepSpeed"
"Export bert to onnx failed"
"dictionary update sequence element #5 has length 1; 2 is required"
"Adding TF Implementation of BEiT"
"AttributeError: 'TrainingArguments' object has no attribute 'generation_max_length'"
"RuntimeError - invalid multinomial distribution (with replacement=False, not enough non-negative category to sample) "
"TypeError: to_json_file() got an unexpected keyword argument 'use_diff'"
"LayoutLMv2ForRelationExtraction is missing in transformers"
"Can't convert Flax T5 model to PyTorch"
"[logging] Turn off loss logging, while keeping progress bar and logging to third-party application"
"TFWav2Vec2ForCTC breaks when not run eagerly"
"OOM error when training with trainer"
"gpt2 results with past_key_values not the same as when computed from scratch"
" \u6c42\u6559\uff1aload BERT \u6a21\u578b\u62a5\u9519 "
"\"ValueError: initial_value must be specified.\" error when compiling bert for text classification"
"Word offsets of some fast tokenizers are not compatible with token classification pipeline label aggregation"
"Cannot set up development environment on Python 3.10"
"LayoutLMv3 image preparation code snippet does not work with PDFs"
"Model parallelism for m2m100"
"Pipelines returns inconsistent results when using non-default model"
"how to frozen TFGPT2LMHeadModel Embedding matrix?"
"TypeError: TextInputSequence must be str"
"Bloom-6b3 not utilizing much from GPU"
"todo: enable CI to run torchdynamo/tensorrt tests"
"DeltaLM"
"model.generate doesn't validate kwargs"
"model does not work after loss change"
"Expected behaviour for MBartTokenizer as target tokenizer"
"\u5982\u4f55\u5c06xlnet\u4f5c\u4e3a\u5d4c\u5165\u5c42\u7f6e\u4e8e\u5176\u4ed6\u6a21\u578b\u524d"
"ViT modeling file is missing drop path present in PyTorch image models"
"Please Add a Fast Flaubert_tokenizer class as well to leverage fast_tokenizer methods"
"[TRACKER] Add alibi tests on BLOOM"
"Question for implementation of resize in image-classification examples."
"DeBERTa for MaskedLM appears to be producing random results"
"the Sigopt api is outdated in transformers trainer.py, the old api could not work"
"MLflow fails to log to a tracking server"
"core dumped"
"Inference for TFMarianMTModel (en to Romance language translation) is slow and inaccurate"
"Confusing documentation for argument class_labels in MaskFormerForInstanceSegmentation.forward()"
"MaskFormer documentation - `is_thing_map`"
"LongT5 Summarization Example Not Working"
"pretrain longT5"
"Stride in BERT fast tokenizer doesn't work as I expected"
"longt5 error in step 13 when torch.distributed.launch"
"Fine tune TrOCR for persian language"
"Cannot save TFSwinForImageClassification as SavedModel"
"Getting error following the official docs for T5 translation fine-tuning: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'new_zeros'"
"Group_texts in run_clm.py will add shorter than block_size groups on intermediately sized training sets."
"Can't Run UL2"
"NLLB model file for the 600M model"
"Model Loading Imbalance"
"ImportError: cannot import name 'BloomTokenizer' from 'transformers'"
"Cannot save TFTapasModel  as SavedModel "
"failed to use PyTorch jit mode due to: forward() is missing value for argument 'position_ids'."
"Test summary with previous PyTorch/TensorFlow versions"
"Same training time for different values of sliding window in Longformer"
"run_summarization_no_trainer"
"Longformer EncoderDecoder (LED)-Large model finetuning for summarization results in </s><s><s><s><s><s><s><s><s><s><s>... output"
"add Decision Transformer ONNX config to Transformers"
"when i use TFGPT2LMHeadModel, how can i build labels and input_ids?"
"Exported DeBERTa ONNX model is incorrect"
"[TRACKER] Add BLOOM Meg-DS optimizer states"
"The saved trained albert-base-v2 model does not work properly"
"torch.jit.trace can trace shared weights, no need to clone weights when tracing"
"length_penalty behavior is inconsistent with documentation"
"Argument inconsistency between processor and tokenizer"
"TFAutoModel does not work with gpt2 and .generate"
"The problem in BATCH generation of GPT model"
"Private model usage problem "
"Save and load"
"Support private (Opacus) training of BART by altering BartLearnedPositionalEmbedding's forward method"
"BLOOM model parameters mentioned in hub-docs"
"Tokeniser support in java "
"transformers[tf-cpu] fails because torch isn't installed"
"Running `examples/pytorch/summarization/run_summarization.py --help` gives `TypeError: can only concatenate tuple (not \"str\") to tuple`"
"Tensorflow example squad's run_qa.py miss token_type_ids inputs"
"Can't load tokenizer for longt5-xl"
"VisualBERT, visual feature projection."
"Conflict between pyctcdecode and Wav2Vec2ProcessorWithLM"
"Longformer, BigBird take same time to run in sparse mode as well as full-mode"
"ONNX runtime error after export of Deberta v3 SequenceClassification model"
"TF2 DeBERTaV2 runs super slow on TPUs"
"Add callback that saves only best checkpoints"
"Flax Support NLLB (or M2M100) model"
"Onnx Runtime Errors With LongT5"
"Not able to load the Facebook OPT model"
"RuntimeError: \"triu_tril_cuda_template\" not implemented for 'BFloat16'"
"Behavior of shift_tokens_right on padded input_ids"
"How to convert pytorch bart model to tf1.x ?"
"Can not import Trainer "
"Can't pickle local object when running official benchmark"
"Expected input batch_size (16) to match target batch_size (262144)"
"OPT vocab size of model and tokenizer does not match"
"cannot import name 'TrainingArguments' from 'transformers'"
"This code block will not be executed "
"Define metric for save the best model"
"My model's Hosted Inference API is returning \"Internal Server Error\" and when fetching the API it eternally loads."
"Debertav2 debertav3 TPU : socket closed"
"Does 'convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py' transfer data2vec-text model's parameter well?"
"Kindly provide a sample dataset used in layoutlmv3."
"'FlavaModelOutput' object has no attribute 'contrastive_logits_per_image'"
"T5 generate with do_sample doesn't work on DeepSpeed Stage 3"
"How to  freeze  GPT-2 model  layers with Tensorflow/Keras?"
"Initializing attention weights in T5"
"Run_mlm.py for fine-tuning generator(mlm) of electra "
"Numpy arrays used instead of jax array in example"
"Converting batch into jax arrays during training is inefficient"
"Converting facebook/opt-13b to onnx"
"Couldn't run the run_clip.py successfully"
"Error while loading a pre-trained wav2vec2 model"
"Support NLLB's LID model"
"Update push_to_hub to leverage HTTP API"
"Migrate metrics used in all examples from Datasets to Evaluate"
"layoutlmv3-base-chinese tokenizer could not be loaded."
"Extracting embeddings through 'pipeline' and 'feature-extraction' not outputting correct length of values."
"`activation_dropout` in OPT is never used"
"Only use one gpu when generating the hidden states (text embeddings) by \"Model output\" api"
"Loading sharded model with torch_dtype='auto' causes TypeError"
"\"AttributeError: cls.seq_relationship.weight not found in PyTorch model\" when converting pytorch to tensorflow"
"Decouple inference from preprocessing and postprocessing steps from pipeline"
"FeatureExtractor in Multimodal & ViT based models"
"Transformers documentation translation to French"
"tensorflow-aarch64 is not a viable candidate tensorflow version"
"T5 Tokenizer misses a space"
"DeepSpeed Stage 2, Gradient computed twice for this partition."
"Fail to reproduce tf2 bert-large f1-score on SQuADv1.1"
"AutoTokenizer behavior changing when enable_full_determinism is set to zero"
"Efficient Attention"
"The doc sample for training on Speech Encoder Decoder does not work"
"Global/local import with replicated name in the Trainer leading to UnboundLocalError"
"Add hallucination filter in generate()"
"Unable to set up developer environment on Mac M1"
"generate with tf.function (xla) not working for tf model export"
"How can I use DDP mode without `torch.distributed.launch` command when I use transformers.Trainer"
"UnicodeDecodeError when using run_mlm_flax.py"
"Transformers 4.21.0: Can't load XLMRoberta checkpoints"
"Implement pytorch model with BetterTransformer in pytorch1.12?"
"Potential memory leakage of TensorFlow Swin model on kaggle!"
"Getting Torchvision Transforms of `feature_extractor`s"
"NLLB-200 is too slow"
"    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))"
"LayoutLM-based visual question answering model, weights, and pipeline"
"Summarisation example fails to run on given example. Missing positional argument TypeError"
"Encoder Decoder Model gives same generation results after finetuning"
"HPO could be enabled by a HPO configuration file(yaml or json) instead of adding code explicitly in example.py"
"`local_files_only` is not passed to `_from_pretrained` in `PreTrainedTokenizerBase.from_pretrained`"
"Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)"
"Mismatch between logits from generate and forward with an attention mask for most GPT models"
"Incorrect learning rate when using 'cosine_with_restarts' scheduler type"
"Is run_clip.py an example of fine-tune or an example of training a vision-text model from scratch\uff1f"
"GFT: Generative Fundamental Training"
"Can't run the example in https://huggingface.co/transformers/v4.9.2/model_doc/blenderbot.html#transformers.BlenderbotModel"
"Cannot restore `sequences_scores` from `scores` and `beam_indices` returned by `t5-base`"
"GPT-J evaluation with multiple GPUs crashes"
"Incorrect assertion in pipeline test test_dbmdz_english()"
"PreTrainedTokenizerFast with tokenizer object is acting on original tokenizer object"
"Fine-tuning a pretrained model did not follow as expected from the blog posting"
"Sharded Multi-GPU MT5 training with the Seq2SeqTrainer fails (4.21.0)"
"`assertion failed: stride < max_len` when using tokenizer with text_pair"
"Tranformers documentation translation to Japanese \ud83c\uddef\ud83c\uddf5"
"run_clip.py RuntimeError"
"Cannot replicate T5 performance on WMT14"
"BartLearnedPositionalEmbedding's forward method signature obstructs private (Opacus) training of BART"
"Increase notebooks page visibility"
"Make Tokenizers serializable to TF SavedModel format"
"BlenderBot-Distil-400M training fails if the input or target length exceeds a certain threshold, even when truncation and padding is on"
"Update no_trainer scripts to include gradient accumulation"
"Update no_trainer scripts to include gather_for_metrics"
"Conversion from TF BERT Checkpoint to HF Model Breaks"
"Add zero-shot object detection pipeline"
"Add depth estimation pipeline"
"'MarianTokenizer' object has no attribute 'target_encoder'"
"Unable to Infer on Bloom Model-2b5 using Deepspeed  "
"understand differences in tokenization "
"BartForConditionalGeneration output is not dependent on input when trained from scratch"
"Multi-GPU setting: Expected to mark a variable ready only once (RuntimeError)"
"mypy typing not working for AutoModelForMaskedLM when used with Trainer"
"How to embed relational information in a Transformer for NMT task?"
"Fused Softmax Kernels"
"TFEncoderDecoderModel can not be trained with TF Keras fit() method"
"Fine tuning TensorFlow DeBERTa fails on TPU"
"How to do batch inference in GPT-J"
"Not able to use DistilBERT in VisualTextDualEncoder"
"RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): initialization failed"
"TF to ONNX export fails with CLI using example from docs"
"TF to ONNX export fails with large models"
"Wav2Vec 2.0 model output logits related audio pad?"
"The Document of LongT5 confilcts with and its example code of prefix"
"Add Mask2Former"
"Suppress `reset_parameters` of `torch.nn.Linear,Conv2d...` inside `no_init_weights`"
"bert-large-uncased gives `(1024) must match the size of tensor b (512) at non-singleton dimension 1` error"
"https://github.com/huggingface/transformers/blob/f0d496828d3da3bf1e3c8fbed394d7847e839fa6/src/transformers/models/funnel/modeling_funnel.py#L1004"
"Small typo in docs/README.md"
"How to use run_glue.py with tensorboard?"
"Tqdm not working with question-answering pipeline"
"FSDP - TypeError: load_state_dict() got an unexpected keyword argument 'strict'"
"VisionEncoderDecoderModel gradient checkpointing"
"layoutlmv3 processor "
"onnx run error at translation model"
"[Summary] Regarding memory issue in tests"
"[New Model] LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding"
"[New Model] Donut: Document Understanding Transformer"
"AutoModel(s) do not respect the `revision` flag while loading custom models"
"AttributeError: 'LayoutLMForTokenClassification' object has no attribute 'config'"
"Thoughts on updating package metadata"
"Typo in configuration"
"fail to import import transformers.trainer due to libssl.so.10: cannot open shared object file: No such file or directory"
"wav2vec2 : No MSELoss implementation"
"OWL-ViT outputs are offset for non-square images"
"Illegal instruction: 4 error when importing TextClassificationPipeline"
"Segformer ouput size"
"Module 'seqeval' doesn't exist on the Hugging Face Hub either"
"Transformers Documentation translation to German (de)"
"Trainer Bug"
"How to load a fine-tuned model and inference after running run_clip.py\uff1f"
"Segformer, can't save checkpoint in saved_model format"
"Cannot get WER during WavVec2 fine-tuning"
"how to customize the encoder_output when using the generate function in BART?"
"NAN values appears when including a new padding token in my tokenizer"
"IterableDatasets result in nan loss in eval with dataloader_num_workers>=1 and multi-gpu"
"Optuna hyperparameter does not sync trial/hyperparameters when using torchrun single-node, multi-process"
"OSError in linux server"
"RuntimeError: Error(s) in loading state_dict for Wav2Vec2ForCTC"
"`transformers.convert_graph_to_onnx.quantize` fails in unit tests"
"BartForConditionalGeneration is erroneous either at .forward or at .generate"
"Post-processing for HumanEval code generations not working properly"
"Bug in DonutFeatureExtractor"
"Big Bird cannot be converted to ONNX"
"How to load multiple TXT training files when pre-train RoBERTa from scratch"
"variable name:_name = \"label\" if \"label\" in features[0].keys() else \"labels\" when training custom NER"
"`local_files_only=True` not work"
"Community Integration: Colossal-AI for Large AI Models"
"Parallelization for OPT model"
"can't run the wav2vec2-base-960 example"
"OWL-ViT memory usage grows linearly with each prediction"
"XLA generation error with repetition_penalty"
"Passing optimizer to Trainer constructor does not work"
"Support output_scores in XLA TF generate"
"CLIP output doesn't match the official weight"
"Add TF VideoMAE"
"_tf_available for customized built tensorflow"
"`AttributeError: 'BertTokenizer' object has no attribute 'tokens_trie'"
"When resuming from checkpoint with Trainer using a streamed dataset, use the Datasets API to skip"
"Generate: deprecate the use of model `config` as a source of defaults"
"BigBird inference: same input data gives different outputs "
"DeBERTa can't load some parameters"
"_no_load_in_8bit module list have custom ignored layers "
"Refactor Pytorch `model.generate` method to work on TPU"
"BartTokenizer add_tokens feature."
"No module named 'evaluate'"
"Cannot import pipelines from transformers"
"Unexpected keyword argument 'trust_remote_code' when using `table-question-answering` pipeline"
"TFClipModel fails to train because of None loss"
"Padding offsets mapping via `tokenizer.pad`"
"Why does setting `--fp16 True` not save memory"
"Is a bug in TFT5ForConditionalGeneration._shift_right func?"
"Implement a new model: Point-BERT \ud83c\udf1f"
"FileNotFoundError: [Errno 2] No such file or directory: '/home/chaizhihua/.cache/huggingface/hub/models--.--resources--ltp/refs/main'"
"device_map='\"auto\" fails with in big_modelling.py"
"'topk_cpu not implemented for half' when using topk with bitsandbytes 8-bit quant"
"[RAG] - TypeError: init_retrieval() missing 1 required positional argument: 'distributed_port' for PL==1.5.10"
"Make TFNoRepeatNGramLogitsProcessor XLA compatible "
"Adding a documentation to save the best checkpoint during the training in summarization example project"
"The phenomenon of being suddenly killed during RAG evaluation processing"
"TFSegformer fail to convert to onnx "
"`QuestionAnsweringPipeline`: different behaviors when with `handle_impossible_answer=True`"
"Typo in TrOCR documentation"
"add timesformer model"
"Error Attention Mask Size for Customized Encoder-Decoder Architecture Using Multimodal Encoder"
"KeyError 'overflow_to_sample_mapping' when using LayoutXLM with regular Tokenizer + return_overflowing_tokens"
"The training loss(logging steps) will drop suddenly after each epoch? Help me plz! Orz"
"Aggregation strategy does not respect original utterance"
"Progress bars shown despite disable_tqdm=True in Trainer"
"Cannot import ORTModelForSeq2SeqLM"
"Bloom 176B with deepspeed-inference: Cuda illegal memory access"
"Add image-guided object detection support to OWL-ViT"
"run finetune_rag.py -- errror OSError: Not enough disk space. Needed: 139.13 GiB (download: 66.09 GiB, generated: 73.03 GiB, post-processed: Unknown size)"
"Automodel \"from_config\" fails when config is loaded from PretrainedConfig.from_dict(config)"
"AdamW algorithm is not the same as in the referenced paper"
"Add MSN checkpoints to ViT"
"AttributeError: 'T5Config' object has no attribute 'exponential_decay_length_penalty'"
"'UserWarning: Module is put on CPU' when use FSDP by Accelerate"
"Possible issue with learning rate scheduler while using Fairscale in Seq2Seq Trainer"
"Training loss of BART is going to nan in transformers>=4.21.0"
"TF: Can't create sharded XGLM model"
"Incorrect auto feature extractor for videomae"
"TAPAS model usage issue"
"Memory increment and release when loading model via PretrainedModel.from_pretrained"
"Raise ValueError if given max_new_tokens to `Seq2SeqTrainer.predict()`"
"circular import issue when importing with `transformers` and `happytransformer`"
"UnimplementedError: The Conv2D op currently does not support grouped convolutions on the CPU."
"[BUG] Getting different sentence embeddings when using model on CPU and GPU"
"CLIPTextModel gives invalid output for zeroed attention mask"
"Support mixed precision FP16 in TF Segformer | Nan loss"
"torchmetrics support"
"`load_tf_weights` doesn't handle the weights added to the TF models at the top level"
"Swin trace is not correctedly for dynamic batch."
"Changing a single example for BLOOM 176-B affects forward pass for other examples in a batch"
"SSLError"
"Inconsistencies between `nn.functional.interpolate` and `tf.image.resize`"
"Feature to highlight or color code the text from the NER output of token classification having offsets using python"
"New update breaks T5, gpt2, opt models (probably all models actually) if bitsandbytes is installed"
"Identifying backend compatibility versions"
"ONNX test suite is slow - run in 5.5 hours"
"Memory is not released when moving model to CUDA"
"model with longformer encoder cannot be saved due to OperatorNotAllowedInGraphError"
"Wav2Vec2ProcessorWithLM in pipeline issue"
"Examples do not seem to work on any spaces right now (possible downtime?)"
"[HF Trainer] [new optimizer] add `AnyPrecisionAdamW` (bf16)"
"Add a vit-based ocr model to hugging face"
"ValueError: Unknown layer: Custom>TFViTMainLayer when using a Google transformer model in Streamlit"
"Add support for open_clip"
"Adding multiprocessing option to transformers.pipelines.automatic_speech_recognition "
"Input_embeds for Albert"
"BUG for beam_indices from model.generate()"
"Dropout in OPT embedding layer"
"Cannot import OPTForSequenceClassification on Kaggle notebooks (transformers 4.20.1, huggingface_hub 0.8.1)"
"BartForSequenceClassification: Use eos_token or cls_token?"
"loading CLIPVisionModel from openai/clip-vit-base-patch32"
"Learning rate is given as tensor, cannot serialize TrainerState in order to save checkpoint."
"How can I control data feeding order to model using Huggingface Trainer?"
"mlflow can log a maximum of 100 parameters on Azure ML"
"we want to submit a PR about Source-Free compression training on huggingface NLP model\uff0cWhere would you suggest submitting it?"
"LayoutLMV3 Tokenizer Inserts Odd Characters"
"BigBirdTokenizer Serialization Error on Spark"
"Can't disable INTEGRATION_TO_CALLBACK"
"Are there any higher version transformers compatible with transformers==3.0.2"
"Flax BART training fails when evaluating"
"Tried running Stable Diffusion GRisk GUI.exe and no go."
"BART decoder output length changes"
"Generating with Flax fails when using Causal Language models"
"Can't find \u2018romanian_postprocessing.md\u2019 file"
"While trying to train seq2seq distillation model i am getting an error message that __init__() got an unexpected keyword argument 'weights_summary'."
"Incorrect size of input for 1st strided window length in `Perplexity of fixed-length models`"
"Longformer TF int32 vs int64"
"Larger Logits != Larger Probability"
"BART example does not produce expected masks"
"NaN when training t5-large with bf16 on multiple GPUs"
"Converting ruGPT3 model (based on GPT2) to ONNX format"
"Failed to import transformers.models.bart.modeling_tf_bart because no module named 'keras'"
"Cannot Import BigBirdModel"
"facebook/wav2vec2-xls-r-300m-21-to-en  TypeError: expected str, bytes or os.PathLike object, not NoneType"
"Follow ups to DocumentQuestionAnswering Pipeline"
"[ViT] Add note about interpolation of position encodings"
"I render only black screen"
"Getting the heat map out of VILT (Figure 4 in the paper)"
"Output scores in TranslationPipeline"
"wrong wav2vec link in audio classification blog"
"adamw_bnb_8bit is actually Adam8bit and doesn't respect TrainingArguments weight_decay"
"About the evaluation_loop function of trainer"
"BertTokenizer slowly on latest versions"
"Pipeline GPT-NeoX only returns \"BB\" from any prompt then nothing for subsequent calls of the pipeline"
"Resize position embeddings in PreTrainedModel"
"Update decision transformers to gym 0.26"
"Latest Wav2Vec2 pretraining script runs on first GPU only"
"Encoder-decoder model is not working correctly for the latest versions"
"The configuration is not a valid json file"
"Pre-processing re-runs for each process"
"Allow custom head size for self attention in BERT"
"Choice of variable name in custom model affects model initialization"
"add a python module called \"loss.py\" for custom losses published in papers"
"generate() - documentation of `length_penalty' is misleading (and actually wrong) "
"AttributeError: 'DistributedDataParallel' object has no attribute 'generate'"
"How to parallelize large model(like t5-11b) at transformer version 3.0.2"
"Top_P sampling samples an extra token when the cum sum of probabilities is exactly equal to top_p"
"KeyError when initialize the tokenizer with Atutokenizer for ernie-1.0-base-zh`"
"MBART tokenizer not behaving as example"
"Flax BERT finetuning notebook no longer works on TPUs"
"wiki_dpr model which features are using (i,e there are so many models are there)"
"Calling DetrFeatureExtractor will modify its inputs"
"RuntimeError from fine-tuning the automodel"
"MaskFormerFeatureExtractor doesn't process instance segmentation maps correctly"
"Add Molecular Attention Transformer"
"genered_predictions.txt produced by run_summarization script may be of incorrect length"
"position_ids cannot be specified for GPTNeoXForCausalLM"
"Old import clause in seq2seq trainer"
"ConvNextModel doesn't work well on M1 mac for batches containing more than 2 images"
"TypeError: __init__() got an unexpected keyword argument 'evaluate_during_training'"
"AttributeError: 'TrainingArguments' object has no attribute 'main_process_first'"
"Is AMD supported for transformers and text generation?"
"Increased Memory Consumption In Containers"
"LEDForSequenceClassification fine-tuning model gives: IndexError: index out of range in self"
"How to find the accuracy of the generated questions from the text ?"
"Unknown error while running \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"!"
"Add Deformable DETR to the object detection pipeline tests"
"Minor inconsistency in \"Transformers-based Encoder-Decoder Models\" blog post"
"Wrong `special_tokens_mask` when using `facebook/opt-350m`"
"Summarization pipeline giving different outputs when num_beams=1 or num_beams not set (should default to 1)"
"Save last/best model during training"
"Wav2Vec2 Conformer loss nan and wer 1 issue"
"BertLMHeadModel (w/ relative position embedding) does not work correctly when use_cache = True"
"i was trying to create custom tokenizer for some language and got this as error or warning.."
"why need  slice outputs tensor in prediction_step function in Trainer"
"Loading tokenizer using from_pretrained seems to be broken for v4"
"AMOS"
"Possibility to access initial indices of the data during the Training"
"Zero-Shot Classification - Pipeline - Batch Size"
"PhraseConstraints apearing only directly after input or at the end of the generated sentence"
"Small Typo in Docs GenerationMixin for use_cache parameter"
"add Unified-IO"
"`--with_tracking`  doesn't seem to work"
"some data is dropped when encoding by LayoutLMv3Processor"
"v4.22.1 ErnieForMaskedLM Bug"
"[Tracker] [bnb] Supporting `device_map` containing GPU and CPU devices"
"TypeError: __init__() got an unexpected keyword argument 'has_model_config'"
"Add BPE Wav2Vec2CTCTokenizer"
"Allow custom signature while saving TF models"
"Activation checkpointing for TFGPT2DoubleHeadsModel"
"Some feature requests for the Trainer"
"Flax vs torch benchmark on Wav2vec2"
"DPR pooler weights not loading correctly"
"Problem trying to migrate cache "
"GPT Neox Japanese is in the release notes for v4.22.X but does not appear to be in the v4.22.X package."
"HfArgumentParser support yaml parser"
"CLIPTokenizer behaves inconsistently depending on whether ftfy is installed or not"
"Adding TensorFlow port of LeViT"
"document-question-answering pipeline does not work with some models"
