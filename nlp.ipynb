{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd770f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import transformers\n",
    "import matplotlib as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65494e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'created_at': '2018-11-05T21:35:51Z',\n",
       "  'author': 'ZhaoyueCheng',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Thanks a lot for the port! I have some minor questions, for the run_squad file, I see two options for accumulating gradients, accumulate_gradients and gradient_accumulation_steps but it seems to me that it can be combined into one. The other one is for the global_step variable, seems we are only counting but not using this variable in gradient accumulating. Thanks again!'},\n",
       " '0_0': {'created_at': '2018-11-05T22:01:47Z',\n",
       "  'author': 'ZhaoyueCheng',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'It also seems to me that the SQuAD VERSION can not reproduce the google tensorflow version performance.'},\n",
       " '0_1': {'created_at': '2018-11-05T23:20:46Z',\n",
       "  'author': 'abeljim',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'It also seems to me that the SQuAD VERSION can not reproduce the google tensorflow version performance.\\n\\nWhat batch size are you running?'},\n",
       " '0_3': {'created_at': '2018-11-06T02:05:22Z',\n",
       "  'author': 'abeljim',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'Just ran on 1 GPU batch size of 10, the result is {\"exact_match\": 21. COMMIT , \"f1\": 41. COMMIT }\\nActually it might be with the eval code Ill look into it'},\n",
       " '0_4': {'created_at': '2018-11-06T03:48:21Z',\n",
       "  'author': 'ZhaoyueCheng',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': \"Sure, Thanks, I'm checking for the reason too, will report if find anything.\"},\n",
       " '0_5': {'created_at': '2018-11-06T03:56:34Z',\n",
       "  'author': 'abeljim',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'The predictions file is only outputting one word. Need to find out if the bug is in the model itself or write predictions function in run_squad.py. The correct answer always seems to be in the nbest_predictions, but its never selected.'},\n",
       " '0_6': {'created_at': '2018-11-06T05:33:16Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'What performance does Hugging Face get on SQuAD using this reimplementation?'},\n",
       " '0_7': {'created_at': '2018-11-06T07:47:08Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': \"Hi all,\\nWe were not able to try SQuAD on a multi-GPU with the correct batch_size until recently so we relied on the standard deviations computed in the notebooks to compare the predicted hidden states and losses for the SQuAD script. I was able to try on a multi-GPU today and there is indeed a strong difference.\\nWe got about the same results that you get: F1 of VERSION and exact match of VERSION .\\nI am investigating that right now, my personal guess is that this may be related to things outside the model it-self like the optimizer or the post-processing in SQuAD as these were not compared between the TF and PT models.\\nI will keep you guys updated in this issue and I add a mention in the readme that the SQuAD example doesn't work yet.\\nIf you have some insights, feel free to participate in the discussion.\"},\n",
       " '0_8': {'created_at': '2018-11-06T13:17:08Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': \"If you're comparing activations, it may be worth comparing gradients as well to see if you receive similarly low gradients standard deviations for identical batches. You might see that the gradient is not comparable from the last layer itself ( FILEPATH ); you may also see that gradients only become not comparable only after a particular point in backpropagation, and that would show perhaps that the backward pass for a particular function differs between PyTorch and Tensorflow\"},\n",
       " '0_9': {'created_at': '2018-11-07T22:06:12Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': \"Ok guys thanks for waiting, we've nailed down the culprit which was in fact a bug in the pre-processing logic (more exactly this dumb typo URL ).\\nI took the occasion to clean up a few things I noticed while walking through the code:\\n\\nthe weight initialization was not optimal (tf. truncated_normal_initializer(stddev= VERSION ) was translated in weight.data.normal_( VERSION ) instead of weight.data.normal_(mean= VERSION , std= VERSION ) which likely affected the performance of run_classifer.py also.\\ngradient accumulation loss was not averaged over the accumulation steps which would have required to change the hyper-parameters for using accumulation.\\nthe evaluation was not done with torch.no_grad() FILEPATH .\\n\\nThese fixes are pushed on the develop branch right now.\\nAll in all I think we are pretty good now and none of these issues affected the core PyTorch model (the BERT Transformer it-self) so if you only used extract_features.py you were good from the beginning. And run_classifer.py was ok apart from the sub-optimal additional weights initialization.\\nI will merge the develop branch as soon as we got the final results confirmed (currently it's been training for 20 minutes ( VERSION epoch) on 4GPU with a batch size of 56 and we are already above 85 on F1 on SQuAD and 77 in exact match so I'm rather confident and I think you guys can play with it too now).\\nI am also cleaning up the code base to prepare for a first release that we will put on pip for easier access.\"},\n",
       " '0_10': {'created_at': '2018-11-08T02:08:05Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'USER This is awesome - thank you! Do you know what the final SQuAD results were from the training run you started?'},\n",
       " '0_12': {'created_at': '2018-11-09T11:20:01Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': 'Using the same HP as the TensorFlow version we are actually slightly better on F1 than the original implementation (on the default random seed we used):\\n{\"f1\": 88. COMMIT , \"exact_match\": 81. COMMIT }\\nversus TF: {\"f1\": 88. COMMIT , \"exact_match\": 81. COMMIT }\\nI am trying BERT-large on SQuAD now which is totally do-able on a 4 GPU server with the recommended batch-size of 24 (about 16h of expected training time using the FLAG option and 2 steps of gradient accumulation). I will update the readme with the results.'},\n",
       " '0_13': {'created_at': '2018-11-12T01:18:21Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': \"Great, I saw the BERT-large ones as well - thank you for sharing these results! How long did the BERT-base SQuAD training take on a single GPU when you tried it? I saw BERT-large took ~18 hours over 4 K-80's\"},\n",
       " '0_14': {'created_at': '2018-11-12T07:34:28Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '0',\n",
       "  'text': \"Hi Ethan, I didn't try SQuAD on a single-GPU. On four k-80 (not k40), BERT-base took 5h to train on SQuAD.\"},\n",
       " '1': {'created_at': '2018-11-06T05:30:36Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'When describing how you reproduced the MRPC results, you say:\\n\"Our test ran on a few seeds with the original implementation hyper-parameters gave evaluation results between 82 and 87.\"\\nand you link to the SQuAD hyperparameters ( URL ).\\nIs the link a mistake? Or did you use the SQuAD hyperparameters for tuning on MRPC? More generally, I\\'m wondering if there\\'s a reason the MRPC dev set accuracy is slightly lower (in [82, 87] vs. [84, 88] reported by Google)'},\n",
       " '1_0': {'created_at': '2018-11-06T07:36:42Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '1',\n",
       "  'text': \"Hi Ethan,\\nThanks we used the MRPC hyper-parameters indeed, I corrected the README.\\nRegarding the dev set accuracy, I am not really surprised there is a slightly lower accuracy with the PyTorch version (even though the variance is high so it's hard to get something significant). That is something that is generally observed (see for example the work of Remi Cadene) and we also experienced that with our TF->PT port of the OpenAI GPT model.\\nMy personal feeling is that there are slight differences in the way the backends of TensorFlow and PyTorch handle the operations and these differences make the pre-trained weights sub-optimal for PyTorch.\"},\n",
       " '1_1': {'created_at': '2018-11-06T13:11:49Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '1',\n",
       "  'text': 'Great, thanks for clarifying that. Regarding the slightly lower accuracy, that makes sense. Thanks for your help and for releasing this!'},\n",
       " '1_2': {'created_at': '2018-11-06T13:19:40Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '1',\n",
       "  'text': 'Maybe it would help to train the Tensorflow pre-trained weights for e.g. one epoch in PyTorch (using the MLM and next-sentence objective)? That may help transfer to other tasks, depending on what the issue is'},\n",
       " '1_3': {'created_at': '2018-11-07T23:42:51Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '1',\n",
       "  'text': 'Hi USER , actually the weight initialization fix (tf. truncated_normal_initializer(stddev= VERSION ) was translated in weight.data.normal_( VERSION ) instead of weight.data.normal_(mean= VERSION , std= VERSION ) fixed in COMMIT ) has brought us back to the TensorFlow results on MRPC (between 84 and 88%).\\nI am closing this issue.'},\n",
       " '1_4': {'created_at': '2018-11-08T02:04:37Z',\n",
       "  'author': 'ethanjperez',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '1',\n",
       "  'text': 'USER Great to hear - thanks for working to fix it!'},\n",
       " '2_0': {'created_at': '2018-11-07T23:43:42Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '2',\n",
       "  'text': 'Thanks, I update the readme.'},\n",
       " '3': {'created_at': '2018-11-08T22:01:57Z',\n",
       "  'author': 'bkgoksel',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi, I tried running the Squad model this morning (on a single GPU with gradient accumulation over 3 steps) but after 3 hours of training, my job failed with the following output:\\nI was running the code, unmodified, from commit COMMIT \\nIs this an issue you know about?\\n FILEPATH :50:03 - INFO - __main__ - device cuda n_gpu 1 distributed training False\\n FILEPATH :50:18 - INFO - __main__ - *** Example ***\\n FILEPATH :50:18 - INFO - __main__ - unique_id: COMMIT \\n FILEPATH :50:18 - INFO - __main__ - example_index: 0\\n FILEPATH :50:18 - INFO - __main__ - doc_span_index: 0\\n FILEPATH :50:18 - INFO - __main__ - tokens: [CLS] to whom did the virgin mary allegedly appear in 1858 in lou ##rdes france ? [SEP] architectural ##ly , the school has a catholic character . atop the main building \\' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\\n FILEPATH :50:18 - INFO - __main__ - token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\\n FILEPATH :50:18 - INFO - __main__ - token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\\n FILEPATH :50:18 - INFO - __main__ - input_ids: 101 2000 3183 2106 1996 6261 2984 9382 3711 1999 8517 1999 10223 26371 2605 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :50:18 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n\\n... [truncated] ...\\n[PROGRESS]\\n File \" FILEPATH \", line 929, in \\n main()\\n File \" FILEPATH \", line 862, in main\\n loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\\n File \"/ FILEPATH \", line 477, in __call__\\n result = self.forward(*input, **kwargs)\\n File \"/ FILEPATH \", line 467, in forward\\n start_loss = loss_fct(start_logits, start_positions)\\n File \"/ FILEPATH \", line 477, in __call__\\n result = self.forward(*input, **kwargs)\\n File \"/ FILEPATH \", line 862, in forward\\n ignore_index=self.ignore_index, reduction=self.reduction)\\n File \"/ FILEPATH \", line 1550, in cross_entropy\\n return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\\n File \"/ FILEPATH \", line 1403, in nll_loss\\n if input.size(0) != target.size(0):\\nRuntimeError: dimension specified as 0 but tensor has no dimensions\\n\\nException ignored in: \\nTraceback (most recent call last):\\n File \"/ FILEPATH \", line 931, in __del__\\n self.close()\\n File \"/ FILEPATH \", line 1133, in close\\n self._decr_instances(self)\\n File \"/ FILEPATH \", line 496, in _decr_instances\\n cls.monitor.exit()\\n File \"/ FILEPATH \", line 52, in exit\\n self.join()\\n File \"/ FILEPATH \", line 1053, in join\\n raise RuntimeError(\"cannot join current thread\")\\nRuntimeError: cannot join current thread'},\n",
       " '3_1': {'created_at': '2018-11-09T08:17:26Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '3',\n",
       "  'text': 'Hi Kerem, yes I fixed this bug yesterday in commit COMMIT (a bug with batches of dimension 1)\\nYou can try again with the current version and it should be fine.\\nI got good results with these hyperparameters last night:\\npython run_squad.py \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG \\\\\\n FLAG \\\\\\n FLAG \\n FLAG $ FILEPATH \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG 12 \\\\\\n FLAG 3e-5 \\\\\\n FLAG VERSION \\\\\\n FLAG 384 \\\\\\n FLAG 128 \\\\\\n -- FILEPATH /\\nI found:\\n{\"f1\": 88. COMMIT , \"exact_match\": 81. COMMIT }\\nFeel free to reopen the issue if needed.'},\n",
       " '4': {'created_at': '2018-11-09T02:23:34Z',\n",
       "  'author': 'howardhsu',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'Is there a plan to have an FP16 for GPU so to have a larger batch size or longer text documents support?'},\n",
       " '4_0': {'created_at': '2018-11-10T15:07:52Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '4',\n",
       "  'text': 'Yes probably. I am testing fp16 right now. If it works well I will push it to the repo.'},\n",
       " '4_1': {'created_at': '2018-11-12T16:06:47Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '4',\n",
       "  'text': \"Ok I've added FP16 support (see updated readme)\"},\n",
       " '4_2': {'created_at': '2018-11-14T01:34:15Z',\n",
       "  'author': 'howardhsu',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '4',\n",
       "  'text': 'Thanks for this quick updates.'},\n",
       " '4_3': {'created_at': '2018-12-20T18:42:10Z',\n",
       "  'author': 'Ashish-Gupta03',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '4',\n",
       "  'text': \"I'm not able to work with FP16 for pytorch BERT code. Particularly for BertForSequenceClassification, which I tried and got the issue\\nRuntime error: Expected scalar type object Half but got scalar type Float for argument ISSUE_REF target\\nwhen I enabled fp16.\\nAlso when using\\nlogits = logits.half() labels = labels.half()\\nthen the epoch time also increased.\"},\n",
       " '5': {'created_at': '2018-11-09T06:13:08Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I'm pretty sure this comment:\\n URL \\nshould instead say:\\n# Sizes are [batch_size, 1, 1, to_seq_length] \\n# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length] \\n\\nWhen masking out tokens for attention, it doesn't matter what happens to attention from padding tokens, only that there is no attention to padding tokens.\\nI don't believe the code is doing what the comment currently suggests because that would be an implementation flaw.\"},\n",
       " '5_0': {'created_at': '2018-11-09T08:31:25Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '5',\n",
       "  'text': 'Yes! fixed the comment'},\n",
       " '6': {'created_at': '2018-11-10T13:23:31Z',\n",
       "  'author': 'antxiaojun',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"if I convert code to python2 version of code, it can't converage ; Would you present py2 code?\"},\n",
       " '7': {'created_at': '2018-11-10T17:16:01Z',\n",
       "  'author': 'rawatprateek',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'If I am running only evaluation and not training, there are errors as tr_loss and nb_tr_steps are undefined.'},\n",
       " '8': {'created_at': '2018-11-13T15:09:33Z',\n",
       "  'author': 'lukovnikov',\n",
       "  'author_location': 'BE',\n",
       "  'type': 'issue',\n",
       "  'text': \"BERTConfig is not used for BERTIntermediate's activation function. intermediate_act_fn is always gelu. Is this normal?\\n URL\"},\n",
       " '8_0': {'created_at': '2018-11-13T15:11:03Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '8',\n",
       "  'text': 'Yes, I hard coded that since the pre-trained models are all trained with gelu anyway.'},\n",
       " '8_1': {'created_at': '2018-11-13T15:14:35Z',\n",
       "  'author': 'lukovnikov',\n",
       "  'author_location': 'BE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '8',\n",
       "  'text': \"ok. but since config is there anyway, isn't it cleaner to use it (to avoid errors for people using configs that use a different activation for some reason) ?\"},\n",
       " '8_2': {'created_at': '2018-11-13T15:17:39Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '8',\n",
       "  'text': \"Yes we can, I'll change that in the coming first release (unless you would like to submit a PR which I would be happy to merge).\"},\n",
       " '8_3': {'created_at': '2018-11-13T15:18:30Z',\n",
       "  'author': 'lukovnikov',\n",
       "  'author_location': 'BE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '8',\n",
       "  'text': \"yeah let me clean up and I'll PR\"},\n",
       " '9': {'created_at': '2018-11-14T06:30:59Z',\n",
       "  'author': 'koukoulala',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Can you push the pytorch code for the pre-training process,such as MLM task, please?\\nI really want to study, but I can't understand tensorflow, it's so complex.\\nthanks!!!\"},\n",
       " '9_0': {'created_at': '2018-11-17T21:55:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '9',\n",
       "  'text': \"Hi, I don't have plan for that in the near future.\"},\n",
       " '10_0': {'created_at': '2018-11-14T08:24:49Z',\n",
       "  'author': 'TIANRENK',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': 'But I print the model.embeddings.token_type_embeddings it was Embedding(16,768) .'},\n",
       " '10_1': {'created_at': '2018-11-14T08:31:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': 'which model are you loading?'},\n",
       " '10_4': {'created_at': '2018-11-14T08:38:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': \"I'm testing the chinese model.\\nDo you use the config.json of the chinese_L-12_H-768_A-12 ?\\nCan you send the content of your config_json ?\"},\n",
       " '10_7': {'created_at': '2018-11-14T08:45:13Z',\n",
       "  'author': 'TIANRENK',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': 'I see you have \"type_vocab_size\": 2 in your config file, how is that?\\n\\nYes,but I change it in my code.'},\n",
       " '10_8': {'created_at': '2018-11-14T08:46:12Z',\n",
       "  'author': 'TIANRENK',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': \"is your pytorch_model.bin the good converted model of the chinese one (and not of an English one)?\\n\\nI think it's good.\"},\n",
       " '10_9': {'created_at': '2018-11-14T08:46:35Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': 'Ok, I have the models. I think type_vocab_size should be 2 also for chinese. I am wondering why it is 16 in your pytorch_model.bin'},\n",
       " '10_10': {'created_at': '2018-11-14T08:48:21Z',\n",
       "  'author': 'TIANRENK',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': 'I have no idea.Did my model make the wrong convert?'},\n",
       " '10_11': {'created_at': '2018-11-14T08:48:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': \"I am testing that right now. I haven't played with the multi-lingual models yet.\"},\n",
       " '10_12': {'created_at': '2018-11-14T08:51:29Z',\n",
       "  'author': 'TIANRENK',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': \"I am testing that right now. I haven't played with the multi-lingual models yet.\\n\\nI also use it for the first time.I am looking forward to your test results.\"},\n",
       " '10_14': {'created_at': '2018-11-14T09:04:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': 'are you supplying a config file with \"type_vocab_size\": 2 to the conversion script?'},\n",
       " '10_16': {'created_at': '2018-11-14T09:54:21Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '10',\n",
       "  'text': \"Ok, I think I found the issue, your BertConfig is not build from the configuration file for some reason and thus use the default value of type_vocab_size in BertConfig which is 16.\\nThis error happen on my system when I use config = BertConfig('bert_config.json') instead of config = BertConfig.from_json_file('bert_config.json').\\nI will make sure these two ways of initializing the configuration file (from parameters or from json file) cannot be messed up.\"},\n",
       " '11': {'created_at': '2018-11-15T16:53:12Z',\n",
       "  'author': 'rsanjaykamath',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'issue',\n",
       "  'text': 'Traceback (most recent call last): | FILEPATH [00:00 0:\\nFile \"/ FILEPATH \", line 289, in isnan\\nraise ValueError(\"The argument is not a tensor\", str(tensor))\\nValueError: (\\'The argument is not a tensor\\', \\'None\\')\\n\\nCommand:\\nCUDA_VISIBLE_DEVICES= FILEPATH \\n-- FILEPATH \\n-- FILEPATH \\n-- FILEPATH \\n FLAG \\n FLAG \\n FLAG \\n-- FILEPATH \\n-- FILEPATH \\n FLAG 3e-5 \\n FLAG 2 \\n FLAG 384 \\n FLAG 128 \\n FLAG outputs \\n FLAG 4 \\n FLAG 2 \\n FLAG \\nError while using FLAG only.\\nWorks fine without the argument.\\nGPU: Nvidia GTX 1080Ti Single GPU.\\nPS: I can only fit in train_batch_size 4 on the memory of a single GPU.'},\n",
       " '11_0': {'created_at': '2018-11-15T20:59:44Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '11',\n",
       "  'text': \"Thanks! I pushed a fix for that, you can try it again. You should be able to increase a bit the batch size.\\nBy the way, FILEPATH \\nThe recommended batch_size to get good results (EM, F1) with BERT large on SQuaD is 24. You can try the following possibilities to get to this batch_size:\\n\\nkeeping the same 'real batch size' that you currently have but just a bigger batch_size FLAG 24 FLAG 12\\ntrying a 'real batch size' of 3 with optimization on cpu FLAG 24 FLAG 8 FLAG \\nswitching to fp16 (implies optimization on cpu): FLAG 24 FLAG 6 or 4 FLAG \\n\\nIf your GPU supports fp16, the last solution should be the fastest, otherwise the second should be the fastest. The first solution should work out-of-the box and give better results (EM, F1) but you won't have any speed-up.\"},\n",
       " '11_1': {'created_at': '2018-11-17T21:56:46Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '11',\n",
       "  'text': \"Should be fixed now. Don't hesitate to re-open an issue if needed. Thanks for the feedback!\"},\n",
       " '11_2': {'created_at': '2018-11-18T10:17:01Z',\n",
       "  'author': 'rsanjaykamath',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '11',\n",
       "  'text': 'Yes it works now!\\nWith\\n\\n FLAG 24 FLAG 8 FLAG \\n\\nI get {\"exact_match\": 83. COMMIT , \"f1\": 90. COMMIT } which is pretty close.\\nThanks for this amazing work!'},\n",
       " '12': {'created_at': '2018-11-15T23:47:04Z',\n",
       "  'author': 'elyase',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Recently the Google team added support for Squad VERSION :\\n FILEPATH @ COMMIT \\nWould be great to also have it available in the Pytorch version.'},\n",
       " '12_0': {'created_at': '2018-11-17T21:57:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '12',\n",
       "  'text': \"Hi, I don't have plan for that in the near future but feel free to open a PR.\"},\n",
       " '13': {'created_at': '2018-11-16T08:15:33Z',\n",
       "  'author': 'koukoulala',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"just want to study codes, don't need to have same pre-train performance.\"},\n",
       " '13_0': {'created_at': '2018-11-17T21:57:19Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '13',\n",
       "  'text': \"Hi, I don't have plan for that in the near future.\"},\n",
       " '14': {'created_at': '2018-11-16T18:50:27Z',\n",
       "  'author': 'ylhsieh',\n",
       "  'author_location': 'TW',\n",
       "  'type': 'issue',\n",
       "  'text': 'There is an option save_checkpoints_steps that seems to control checkpointing. However, there is no actual saving operation in the run_* scripts. So, should we add that functionality or remove this argument?'},\n",
       " '14_1': {'created_at': '2018-11-17T22:02:08Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '14',\n",
       "  'text': \"You are right this argument was not used. I removed it, thanks. These examples are provided as starting point to write training scripts for the package module. I don't plan to update them any further (except fixing bugs).\"},\n",
       " '14_3': {'created_at': '2019-03-04T08:19:57Z',\n",
       "  'author': 'valsworthen',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '14',\n",
       "  'text': 'At that time I got:\\nAddSent\\n FILEPATH \\n FILEPATH \\nAddOneSent\\n FILEPATH \\n FILEPATH'},\n",
       " '14_4': {'created_at': '2019-03-04T08:25:12Z',\n",
       "  'author': 'Jasperty',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '14',\n",
       "  'text': 'At that time I got:\\nAddSent\\n FILEPATH \\n FILEPATH \\nAddOneSent\\n FILEPATH \\n FILEPATH \\n\\nThanks a lot! Do you release your paper? i want to cite your result and paper in my paper.'},\n",
       " '14_5': {'created_at': '2019-03-04T08:38:23Z',\n",
       "  'author': 'valsworthen',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '14',\n",
       "  'text': 'Unfortunately it was not part of a paper, just preliminary results.'},\n",
       " '15': {'created_at': '2018-11-17T06:23:28Z',\n",
       "  'author': 'susht3',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'i download the model from bert, it only has model.ckpt.data,model.ckpt.meta and model.ckpt.index, i donnot which to load, what is checkpoint file for convert.py?'},\n",
       " '15_1': {'created_at': '2020-05-14T08:30:59Z',\n",
       "  'author': 'danyaljj',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '15',\n",
       "  'text': 'USER what was your fix?'},\n",
       " '15_2': {'created_at': '2020-06-09T05:30:51Z',\n",
       "  'author': 'dan-hu-spring',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '15',\n",
       "  'text': \"I encountered a similar issue and didn't find a solution with ALBERT. I tried using the export_checkpoint.py file in ALBERT and sent that into the convert_tf_checkpoint_to_pytorch command and there was no error. However the resulting pytorch.bin output was unusable :\\\\\"},\n",
       " '15_3': {'created_at': '2020-06-09T23:59:02Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '15',\n",
       "  'text': 'USER do you mind opening a new issue with your issue, so that we may take a look?'},\n",
       " '16': {'created_at': '2018-11-17T06:51:54Z',\n",
       "  'author': 'susht3',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'convert samples to features, is very slow'},\n",
       " '16_0': {'created_at': '2018-11-17T11:46:00Z',\n",
       "  'author': 'zhhongzhi',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '16',\n",
       "  'text': 'Running on a GPU, I find that dumping extracted features takes up most time. So you may optimize it yourself.'},\n",
       " '16_1': {'created_at': '2018-11-17T22:02:38Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '16',\n",
       "  'text': \"Hi, these examples are provided as starting point to write your own training scripts using the package modules. I don't plan to update them any further.\"},\n",
       " '17_0': {'created_at': '2018-11-17T22:03:43Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '17',\n",
       "  'text': \"Hi I don't plan to add that in the near future but feel free to open a PR if you would like to share an additional example.\"},\n",
       " '17_1': {'created_at': '2019-01-15T14:27:27Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '17',\n",
       "  'text': 'Necrobumping this for reference, as this is addressed in URL'},\n",
       " '18': {'created_at': '2018-11-18T02:10:15Z',\n",
       "  'author': 'KeremTurgutlu',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Is there a way to use any of the provided pre-trained models in the repository for machine translation task?\\nThanks'},\n",
       " '18_0': {'created_at': '2018-11-18T08:48:33Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': \"Hi Kerem, I don't think so. Have a look at the fairsep repo maybe.\"},\n",
       " '18_1': {'created_at': '2018-11-26T12:17:16Z',\n",
       "  'author': 'JasonVann',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': \"USER hi there, I couldn't find out anything about the fairsep repo. Could you post a link? Thanks!\"},\n",
       " '18_2': {'created_at': '2018-11-26T12:41:09Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': \"Hi, I am talking about this repo: URL \\nHave a look at their Transformer's models for machine translation.\"},\n",
       " '18_3': {'created_at': '2019-02-20T10:30:54Z',\n",
       "  'author': 'alphadl',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'I have conducted several MT experiments which fixed the embeddings by using BERT, UNFORTUNATELY, I find it makes performance worse. USER USER'},\n",
       " '18_4': {'created_at': '2019-02-20T18:05:48Z',\n",
       "  'author': 'SinghJasdeep',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'Hey!\\nFAIR has demonstrated that using BERT for unsupervised translation greatly improves BLEU.\\nPaper: URL \\nRepo: URL \\nOlder papers showing pre-training with LM (not MLM) helps Seq2Seq: URL \\nHope this helps!'},\n",
       " '18_5': {'created_at': '2019-03-01T01:23:26Z',\n",
       "  'author': 'gtesei',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'These links are useful.\\nDoes anyone know if BERT improves things also for supervised translation?\\nThanks.'},\n",
       " '18_6': {'created_at': '2019-04-13T07:04:35Z',\n",
       "  'author': 'echan00',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'Does anyone know if BERT improves things also for supervised translation?\\n\\nAlso interested'},\n",
       " '18_7': {'created_at': '2019-05-05T03:41:59Z',\n",
       "  'author': 'nyck33',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'Because BERT is an encoder, I guess we need a decoder. I looked here: URL \\nand it seems Openai Transformer is a decoder. But I cannot find a repo for it.\\n URL \\nI think Bert outputs a vector of size 768. Can we just do a reshape and use the decoder in that transformer notebook? In general can I just reshape and try out a bunch of decoders?'},\n",
       " '18_8': {'created_at': '2019-06-03T19:57:33Z',\n",
       "  'author': 'tacchinotacchi',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': \"These links are useful.\\nDoes anyone know if BERT improves things also for supervised translation?\\nThanks.\\n\\n URL seems to suggest that it does improve the results for supervised translation as well. However this paper is not about using BERT embeddings, rather about pre-training the encoder and decoder on an Masked Language Modelling objective. The biggest benefit comes from initializing the encoder with the weights from BERT, and surprisingly using it to initialize the decoder also brings small benefits, even though if I understand correctly you still have to randomly initialize the weights for the encoder attention module, since it's not present in the pre-trained network.\\nEDIT: of course the pre-trained network needs to have been trained on multi-lingual data, as stated in the paper\"},\n",
       " '18_9': {'created_at': '2019-07-05T03:04:53Z',\n",
       "  'author': 'torshie',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': \"I have managed to replace transformer's encoder with a pretrained bert encoder, however experiment results were very poor. It dropped BLEU score by about 4\\nThe source code is available here: URL , implemented as a fairseq user model. It may not work out of box, some minor tweeks may be needed.\"},\n",
       " '18_10': {'created_at': '2019-10-07T05:24:05Z',\n",
       "  'author': 'sailordiary',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'Could be relevant:\\nTowards Making the Most of BERT in Neural Machine Translation\\nOn the use of BERT for Neural Machine Translation'},\n",
       " '18_11': {'created_at': '2019-11-01T15:53:02Z',\n",
       "  'author': 'Bachstelze',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'Also have a look at MASS and XLM.'},\n",
       " '18_12': {'created_at': '2021-11-09T23:15:56Z',\n",
       "  'author': 'lileicc',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '18',\n",
       "  'text': 'Yes. It is possible to use both BERT as encoder and GPT as decoder and glue them together.\\nThere is a recent paper on this: Multilingual Translation via Grafting Pre-trained Language Models\\n URL \\n URL'},\n",
       " '19': {'created_at': '2018-11-18T08:28:52Z',\n",
       "  'author': 'xiaoda99',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'URL \\nWith this code, all parameters are decayed because the condition \"parameter_name in no_decay\" will never be satisfied.\\nI\\'ve made a PR ISSUE_REF to fix it.'},\n",
       " '20_0': {'created_at': '2018-11-19T03:20:09Z',\n",
       "  'author': 'zlinao',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '20',\n",
       "  'text': 'need to specify the path of vocab.txt for:\\ntokenizer = BertTokenizer.from_pretrained(args.bert_model)'},\n",
       " '20_2': {'created_at': '2018-11-19T08:39:20Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '20',\n",
       "  'text': \"Hi,\\nWhy don't you guys just do tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') as indicated in the readme and the run_classifier.py example?\"},\n",
       " '20_3': {'created_at': '2018-11-19T11:11:53Z',\n",
       "  'author': 'zlinao',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '20',\n",
       "  'text': \"Hi,\\nWhy don't you guys just do tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') as indicated in the readme and the run_classifier.py example?\\n\\nYes, it is easier to use shortcut name. Thanks for your great work.\"},\n",
       " '21_0': {'created_at': '2018-11-18T21:14:38Z',\n",
       "  'author': 'elyase',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '21',\n",
       "  'text': 'This is expected behaviour and is how the multilingual and the uncased models were trained. From the original repo:\\n\\nWe are releasing the BERT-Base and BERT-Large models from the paper. Uncased means that the text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers.'},\n",
       " '21_1': {'created_at': '2018-11-19T08:39:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '21',\n",
       "  'text': 'Yes this is expected.'},\n",
       " '22': {'created_at': '2018-11-19T04:39:04Z',\n",
       "  'author': 'bprabhakar',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I was wondering if there's a proper way of detokenizing the output tokens, i.e., constructing the sentence back from the tokens? Considering the fact that the word-piece tokenisation introduces lots of #s.\"},\n",
       " '22_0': {'created_at': '2018-11-19T14:18:46Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '22',\n",
       "  'text': \"You can remove ' ##' but you cannot know if there was a space around punctuations tokens or uppercase words.\"},\n",
       " '22_1': {'created_at': '2018-11-20T09:07:39Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '22',\n",
       "  'text': \"Yes. I don't plan to include a reverse conversion of tokens in the tokenizer.\\nFor an example on how to keep track of the original characters position, please read the run_squad.py example.\"},\n",
       " '23': {'created_at': '2018-11-19T15:26:20Z',\n",
       "  'author': 'mdasadul',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'issue',\n",
       "  'text': 'I was trying to use BERT as a language model to assign a score(could be PPL score) of a given sentence. Something like\\nP(\"He is go to school\")= VERSION \\nP(\"He is going to school\")= VERSION \\nWhich is indicating that the probability of second sentence is higher than first sentence. Is there a way to get a score like this?\\nThanks'},\n",
       " '23_0': {'created_at': '2018-11-20T09:06:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': \"I don't think you can do that with Bert. The masked LM loss is not a Language Modeling loss, it doesn't work nicely with the chain rule like the usual Language Modeling loss.\\nPlease see the discussion on the TensorFlow repo on that here.\"},\n",
       " '23_3': {'created_at': '2020-05-27T07:36:47Z',\n",
       "  'author': 'orenpapers',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'USER Did you managed to do it?'},\n",
       " '23_4': {'created_at': '2020-05-27T14:52:31Z',\n",
       "  'author': 'mdasadul',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'Yes please check my tweet on this USER \\n...\\nOn Wed, May 27, 2020, 1:37 PM orko19 ***@***.***> wrote:\\n USER Did you managed to do it?\\n\\n \\n You are receiving this because you were mentioned.\\n Reply to this email directly, view it on GitHub\\n ,\\n or unsubscribe\\n \\n .'},\n",
       " '23_5': {'created_at': '2020-05-27T15:21:22Z',\n",
       "  'author': 'orenpapers',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'USER Do you mean this one?\\n URL \\nI see this it for GPT-2, do you have a code for BERT?'},\n",
       " '23_7': {'created_at': '2020-06-01T10:50:45Z',\n",
       "  'author': 'orenpapers',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': \"USER I get the error:\\nTypeError: forward() got an unexpected keyword argument 'masked_lm_labels'\\nAlso, can you please explain why for following steps are necessary:\\n\\nunsqueeze(0)\\nadd torch.no_grad()\\nadd model.eval()\"},\n",
       " '23_8': {'created_at': '2020-07-07T05:54:05Z',\n",
       "  'author': 'nlp-sudo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'The score is equivalent to perplexity. Hence lower the score better the sentence, right?'},\n",
       " '23_9': {'created_at': '2020-07-07T06:12:41Z',\n",
       "  'author': 'mdasadul',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'Yes that is right\\nMd Asadul Islam\\nMachine Learning Engineer\\nScribendi Inc\\n...\\nOn Mon, Jul 6, 2020 at 11:54 PM nlp-sudo ***@***.***> wrote:\\n The score is equivalent to perplexity. Hence lower the score better the\\n sentence, right?\\n\\n \\n You are receiving this because you were mentioned.\\n Reply to this email directly, view it on GitHub\\n ,\\n or unsubscribe\\n \\n .'},\n",
       " '23_10': {'created_at': '2021-03-14T11:48:11Z',\n",
       "  'author': 'orenschonlab',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'USER I get the error:\\n return math.exp(loss.item() / len(tokenize_input))\\nValueError: only one element tensors can be converted to Python scalars\\n\\nAny idea why?'},\n",
       " '23_11': {'created_at': '2021-03-14T13:15:31Z',\n",
       "  'author': 'mdasadul',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': \"Yes, your sentence needs to be longer than 1 word. PPL of 1 word sentence\\ndoesn't mean anything. Please try with longer sentences\\nMd Asadul Islam\\nMachine Learning Engineer\\nScribendi Inc\\n...\\nOn Sun, Mar 14, 2021 at 7:48 AM orenschonlab ***@***.***> wrote:\\n USER I get the error:\\n\\n return math.exp(loss.item() / len(tokenize_input))\\n ValueError: only one element tensors can be converted to Python scalars\\n\\n Any idea why?\\n\\n \\n You are receiving this because you were mentioned.\\n Reply to this email directly, view it on GitHub\\n ,\\n or unsubscribe\\n \\n .\"},\n",
       " '23_12': {'created_at': '2021-03-14T14:02:03Z',\n",
       "  'author': 'orenschonlab',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': \"USER I have a sentence with more than 1 word and still get the error\\nsentence is ' Harry had never believed he would'\\ninput_ids is tensor([[ 101, 4302, 2018, 2196, 3373, 2002, 2052, 102]])\"},\n",
       " '23_13': {'created_at': '2021-03-15T05:10:43Z',\n",
       "  'author': 'EricFillion',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'Below is an example from the official docs on how to implement GPT2 to determine perplexity.\\n URL'},\n",
       " '23_14': {'created_at': '2021-03-15T08:25:31Z',\n",
       "  'author': 'orenschonlab',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'USER But how can it be used for a sentence, not for a dataset?\\nMeaning I want the perplexity of the sentence:\\nHarry had never believed he would'},\n",
       " '23_16': {'created_at': '2021-03-15T17:54:08Z',\n",
       "  'author': 'EricFillion',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'USER But how can it be used for a sentence, not for a dataset?\\nMeaning I want the perplexity of the sentence:\\nHarry had never believed he would\\n\\nI just played around with the code USER posted above. It works perfectly and is nice and concise. It outputted the same scores from the official documentation for short inputs.\\nIf you\\'re still interested in using the method from the official documentation, then you can replace \"\\'\\\\n\\\\n\\'.join(test[\\'text\\'])\" with the text you wish to determine the perplexity of. You\\'ll also want to add \".item()\" to ppl to convert the tensor to a float.'},\n",
       " '23_17': {'created_at': '2021-07-23T03:01:25Z',\n",
       "  'author': 'kaisugi',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '23',\n",
       "  'text': 'This repo is quite useful. It supports Huggingface models.\\n URL'},\n",
       " '24': {'created_at': '2018-11-19T16:35:08Z',\n",
       "  'author': 'ruotianluo',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'I have a reasonable truncated normal approximation. (Actually that is what tf does).\\n URL'},\n",
       " '24_0': {'created_at': '2018-11-20T09:09:23Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '24',\n",
       "  'text': 'We could try that. Not sure how important it is though. Did you try it?'},\n",
       " '24_1': {'created_at': '2018-11-26T09:42:42Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '24',\n",
       "  'text': 'Ok I think we will stick to the normal_initializer for now. Thanks for indicating this option!'},\n",
       " '26': {'created_at': '2018-11-20T03:52:35Z',\n",
       "  'author': 'weiyumou',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'I think I spotted a typo in the README file under the Usage header. There is a piece of code that uses BertTokenizer and the typo is on this line:\\ntokenized_text = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\\nI think tokenized_text should be replaced with text, since the next line is\\ntokenized_text = tokenizer.tokenize(text)'},\n",
       " '27': {'created_at': '2018-11-20T08:38:03Z',\n",
       "  'author': 'vpegasus',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi, guys, I try the run_squad example with\\nTraceback (most recent call last): | FILEPATH [00:00 \\n main()\\n File \" FILEPATH \", line 904, in main\\n param.grad.data = FILEPATH \\nAttributeError: \\'NoneType\\' object has no attribute \\'data\\'\\n\\nI find one of the param.grads is None, so the param.grad.data doesn\\'t exist.\\nby the way I down load the data by myself from the urls in this prject. my os is ubuntu VERSION , pytorch VERSION gpu 1080t\\nanyone else encounters this situation?\\nwanna help, please, thx in advance...'},\n",
       " '27_0': {'created_at': '2018-11-20T09:01:51Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '27',\n",
       "  'text': \"Oh you're right. I've just fixed that. you can try to pull the current master and test again.\"},\n",
       " '27_1': {'created_at': '2018-11-20T23:04:24Z',\n",
       "  'author': 'vpegasus',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '27',\n",
       "  'text': 'USER it works, thanks'},\n",
       " '28_0': {'created_at': '2018-11-20T19:20:24Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '28',\n",
       "  'text': 'My current workaround is to set the env var PYTORCH_PRETRAINED_BERT_CACHE to a different path per process before import pytorch_pretrained_bert. But I think the module itself should handle this properly'},\n",
       " '28_1': {'created_at': '2018-11-21T09:08:13Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '28',\n",
       "  'text': 'I see, thanks for the feedback. I will find a way to make that better in the next release. Not sure we need to store the model gzipped anyway since they mostly contains a torch dump which is already compressed.'},\n",
       " '28_2': {'created_at': '2018-11-26T09:23:02Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '28',\n",
       "  'text': \"Ok, I've added a cache_dir option in from_pretrained in the master to specify a different cache dir for a script. I will release the updated version today on pip. Thanks for the feedback.\"},\n",
       " '28_3': {'created_at': '2018-11-27T09:15:35Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '28',\n",
       "  'text': 'Thanks for fixing this.\\n FILEPATH , so I think directly add the following import in run_classifier.py and run_squad.py is more appropriate in my case\\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\\n\\nwhich is included in my PR: ISSUE_REF'},\n",
       " '29': {'created_at': '2018-11-20T09:48:09Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI am trying to understand the bert_model arg in run_classify.py. In the file, I can see\\ntokenizer = BertTokenizer.from_pretrained(args.bert_model)\\n\\nwhere bert_model is expected to be the vocab text file of the model\\nHowever, I also see\\nmodel = BertForSequenceClassification.from_pretrained(args.bert_model, len(label_list))\\n\\nwhere bert_model is expected to be a archive file containing the model checkpoint and config.\\nPlease help to advice the correct use of bert_model if I have my pretrained model converted locally already.\\nThanks!'},\n",
       " '29_0': {'created_at': '2018-11-20T13:07:14Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '29',\n",
       "  'text': 'Hi, please read this section of the readme.'},\n",
       " '30_0': {'created_at': '2018-11-20T13:06:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': 'Your log is very hard to read. Can you format it cleanly?'},\n",
       " '30_2': {'created_at': '2018-11-20T13:19:35Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': \"It seems like a failed resource allocation.\\nMaybe you don't have enough RAM or your GPU is too small ?\"},\n",
       " '30_3': {'created_at': '2018-11-20T13:25:13Z',\n",
       "  'author': 'SparkJiao',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': \"My GPU has 12400 MB and I think that's enough, may be I should use 'yield' to input the data one by one? I will load less data to try, thanks u a lot!\"},\n",
       " '30_4': {'created_at': '2018-11-21T09:02:51Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': 'Ok feel free to re-open the issue if you still have troubles.'},\n",
       " '30_5': {'created_at': '2020-04-29T16:57:20Z',\n",
       "  'author': 'zyfedward',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': 'Hi USER \\nI met the same issue here, how did you resolve this?'},\n",
       " '30_6': {'created_at': '2020-06-03T08:48:33Z',\n",
       "  'author': 'nv-quan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': 'I have the same issue, did you resolve this? USER USER'},\n",
       " '30_7': {'created_at': '2020-06-03T17:45:52Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': 'USER , do you mind opening a new issue with the template so that we may help?'},\n",
       " '30_8': {'created_at': '2020-06-03T23:25:15Z',\n",
       "  'author': 'SparkJiao',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '30',\n",
       "  'text': 'I have forgot how to reproduce the problem but the index_select error usually happened due to wrong index. You can use a smaller batch size and run the script on CPU to check the full traceback since the traceback while using GPU is delayed.'},\n",
       " '31': {'created_at': '2018-11-20T17:04:09Z',\n",
       "  'author': 'Maaarcocr',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"I have fine-tuned the TF model on SQuAD v1 and I've made the weights available at: URL \\nI get VERSION FM using these weights on SQuAD dev. (If I recall correctly I get roughly 82 EM).\\nI think it may be beneficial to have these weights here, so that people could play with SQuAD and BERT without the need of fine-tuning, which requires a decent enough setup. Let me know what you think!\"},\n",
       " '31_0': {'created_at': '2018-11-21T09:02:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '31',\n",
       "  'text': \"Thanks for the details.\\nThis PyTorch repo is starting to be used by a larger community so we would have to be a little more precise than just rough numbers if we want to include such pre-trained weights.\\nIf you want to add your weights to the repo, you should convert the weights in the PyTorch repo model and get evaluation results on SQuAD with the PyTorch model so everybody has a clean knowledge of what they are using. Otherwise I think it's better that people do their own training and know what are the capabilities of the fine-tuned model they are using.\\nFeel free to come back and re-open the issue if this something you would like to do.\"},\n",
       " '31_1': {'created_at': '2019-04-18T20:10:37Z',\n",
       "  'author': 'wasiahmad',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '31',\n",
       "  'text': 'USER On SQuAD VERSION , BERT (single) scored VERSION EM and VERSION F1 as reported in their paper but when I fine-tuned BERT using run_squad.py I got {\"exact_match\": VERSION , \"f1\": VERSION }. Why there is a difference? What I am missing?'},\n",
       " '32_0': {'created_at': '2018-11-21T09:06:03Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': \"I think it should work. You should get a [1, 2] tensor of logits where predictions[0, 0] is the score of Next sentence being True and predictions[0, 1] is the score of Next sentence being False. So just take the max of the two (or use a SoftMax to get probabilities).\\nDid you try it?\\nThe model behaves better on longer sentences of course (it's mainly trained on 512 tokens inputs).\"},\n",
       " '32_1': {'created_at': '2018-11-21T09:40:15Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'Closing that for now, feel free to reopen if there is another issue.'},\n",
       " '32_3': {'created_at': '2019-01-31T21:51:51Z',\n",
       "  'author': 'dirkgr',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': \"This is not super clear, even wrong in the examples, but there is this note in the docstring for BertModel:\\n INLINECODE : a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\\n classifier pretrained on top of the hidden state associated to the first character of the\\n input ( INLINECODE ) to train on the Next-Sentence task (see BERT's paper).\\n\\nThat seems to suggest pretty strongly that you have to put in the CLF token.\"},\n",
       " '32_5': {'created_at': '2019-03-15T19:03:26Z',\n",
       "  'author': 'dariocazzani',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'Those are the logits, because you did not pass the next_sentence_label.\\nMy understanding is that you could apply a softmax and get the probability for the sequence to be a possible sequence.\\nSentence 1: How old are you?\\nSentence 2: The Eiffel Tower is in Paris\\ntensor([[- VERSION , VERSION ]], grad_fn= )\\nSentence 1: How old are you?\\nSentence 2: I am 193 years old\\ntensor([[ VERSION , - VERSION ]], grad_fn= )\\nFor the first example the probability that the second sentence is a probable continuation is very low.\\nFor the second example the probability is very high (I am looking at the first logit)'},\n",
       " '32_6': {'created_at': '2019-03-18T06:34:55Z',\n",
       "  'author': 'pengjiao123',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'predictions = model(tokens_tensor, segments_tensors )\\nI try the code more than once,why I have the different result?\\nsometime predictions[0, 0] is higher ,however, the same sentence pair,predictions[0, 0] is lower.'},\n",
       " '32_7': {'created_at': '2019-03-18T07:51:20Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'Maybe your model is not in evaluation mode (model.eval())?\\nYou need to do this to desactivate the dropout modules.'},\n",
       " '32_8': {'created_at': '2019-03-18T08:05:59Z',\n",
       "  'author': 'pengjiao123',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'It is OK.THANKS A LOT.'},\n",
       " '32_9': {'created_at': '2019-06-04T04:06:29Z',\n",
       "  'author': 'AIGyan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': \"error: --> 197 embeddings = words_embeddings + position_embeddings + token_type_embeddings 198 embeddings = self.LayerNorm(embeddings) 199 embeddings = self.dropout(embeddings) The size of tensor a (21) must match the size of tensor b (14) at non-singleton dimension 1\\nThe above issues get resolved, when I added few extra 1's and 0's to make the shape similar tokens_tensor and segments_tensors. Just wondering am I using in a right way.\\nMy predictions output is a tensor array of size 21 X 30522 .\\nAnd what I believe the example is to predict the word which is [MASK] . Can you also please guide how to predict the next sentence?\"},\n",
       " '32_10': {'created_at': '2019-08-02T05:45:20Z',\n",
       "  'author': 'yuchuang1979',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'Maybe your model is not in evaluation mode (model.eval())?\\nYou need to do this to desactivate the dropout modules.\\n\\n USER Actually even when I used model.eval() I still got different results. I observed this when I use every model of the package (BertModel, BertForNextSentencePrediction etc). Only when I fixed the length of the input (e.g. to 128), I can get the same results. In this way I have to pad 0 to indexed_tokens so it has a fixed length.\\nCould you explain why is like this, or did I make any mistake?\\nThank you so much!'},\n",
       " '32_11': {'created_at': '2019-08-02T06:25:43Z',\n",
       "  'author': 'Alexadar',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'Maybe your model is not in evaluation mode (model.eval())?\\nYou need to do this to desactivate the dropout modules.\\n\\n USER Actually even when I used model.eval() I still got different results. I observed this when I use every model of the package (BertModel, BertForNextSentencePrediction etc). Only when I fixed the length of the input (e.g. to 128), I can get the same results. In this way I have to pad 0 to indexed_tokens so it has a fixed length.\\nCould you explain why is like this, or did I make any mistake?\\nThank you so much!\\n\\nMake sure\\n\\ninput_ids, input_mask, segment_ids have same length\\nvocabulary file for tokenizer is from the same config dir as your bert_config.json\\n\\nI had symilar symptoms when vocab and config was from diferent berts'},\n",
       " '32_12': {'created_at': '2019-10-27T01:16:26Z',\n",
       "  'author': 'pbabvey',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'I noticed that the probability for longer sentences, regardless of how much they are related to the same subject, is higher than the shorter ones. For example, I added some random sentences to the end of the first or second part and observed significant increase in the first logit value. Is it a way to regularize the model for the next sentence prediction?'},\n",
       " '32_13': {'created_at': '2019-10-29T18:03:49Z',\n",
       "  'author': 'ehsan-soe',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'USER I am observing the same thing.\\nare the probabilities length normalized?'},\n",
       " '32_16': {'created_at': '2019-11-14T19:06:12Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'USER have you seen ISSUE_REF and is it related to your issue?'},\n",
       " '32_17': {'created_at': '2019-11-14T20:24:21Z',\n",
       "  'author': 'parth126',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'USER have you seen ISSUE_REF and is it related to your issue?\\n\\nYes it was the same issue. And the solution worked like a charm.\\nMany thanks USER'},\n",
       " '32_18': {'created_at': '2019-11-19T09:02:20Z',\n",
       "  'author': 'AjitAntony',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '32',\n",
       "  'text': 'USER thanks for the information'},\n",
       " '33_0': {'created_at': '2018-11-21T09:39:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '33',\n",
       "  'text': \"Hi, you can use the multilingual model as indicated in the readme with the commands:\\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual')\\nmodel = BertModel.from_pretrained('bert-base-multilingual')\\nThis will load the multilingual vocabulary (which should contain korean) that your command was not loading.\"},\n",
       " '34': {'created_at': '2018-11-21T10:36:49Z',\n",
       "  'author': 'antxiaojun',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"attributeError: 'BertForPreTraining' object has no attribute 'global_step'\"},\n",
       " '34_0': {'created_at': '2018-11-21T10:57:30Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '34',\n",
       "  'text': 'Maybe some additional information could help me help you?'},\n",
       " '34_2': {'created_at': '2018-11-21T11:10:58Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '34',\n",
       "  'text': \"Hum I will see if I can let people import any kind of TF model in PyTorch, that's a bit risky so it has to be done properly.\\nIn the meantime you can add global_step in the list line 53 of convert_tf_checkpoint_to_pytorch.py\"},\n",
       " '34_4': {'created_at': '2019-01-04T10:00:29Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '34',\n",
       "  'text': 'thanks USER sir. it was resolved.'},\n",
       " '34_5': {'created_at': '2019-04-02T08:43:20Z',\n",
       "  'author': 'shivamakhauri04',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '34',\n",
       "  'text': 'I added the global_step to the skipping list in the modelling.py . Still facing the error. Am I missing something?'},\n",
       " '35': {'created_at': '2018-11-21T15:10:45Z',\n",
       "  'author': 'avisil',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Thanks for the great code..However, the run_squad.py for BERT Large seems to not have the vocab_file and bert_config_file (or other) FILEPATH ?\\nAlso, it is looking for a pytorch model file (a bin file). Does it need to be there?\\nI also had to add this line to the file to make BERT base to run on Squad VERSION :\\nparser.add_argument(\\' FLAG \\', action=\"store_true\", default=True, help=\"Lowercase the input\")'},\n",
       " '35_0': {'created_at': '2018-11-26T08:57:23Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '35',\n",
       "  'text': 'Yes, the readme example was for an older version. I have updated them with the simplified parameters used in the current release. Thanks.'},\n",
       " '36_1': {'created_at': '2018-11-23T11:21:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '36',\n",
       "  'text': \"Thanks, it's fixed on master and will be included in the next release.\"},\n",
       " '37': {'created_at': '2018-11-24T00:49:45Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi,\\nI have a question about Multi-GPU vs Distributed training, probably unrelated to BERT itself.\\nI have a 4-GPU server, and was trying to run run_classifier.py in two ways:\\n(a) run single-node distributed training with 4 processes and minibatch of 32 each\\n(b) run Multi-GPU training with minibatch of 128, and all other hyperparams keep the same\\nIntuitively I believe a and b should yield the closed accuracy and training times. Below please find my observations:\\n\\n(a) runs ~20% faster than (b).\\n(b) yields a better final evaluation accuracy of ~4% than (a)\\n\\nThe first looks like reasonable since I guess the loss.mean() is done by CPU which may be slower than using NCCL directly? However, I don't quite understand the second observation. Can you please give any hint or reference about the possible cause?\\nThanks!\"},\n",
       " '37_0': {'created_at': '2018-11-26T09:03:23Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '37',\n",
       "  'text': \"Hi,\\nThanks for the feedback, it's always interesting to compare the various possible ways to train the model indeed.\\nThe most likely cause for (2) is that MRPC is a small dataset and the model shows a high variance in the results depending on the initialization of the weights for example (see the original BERT repo on that also). The distributed and multi-gpu setups probably do not use the random generators in the exact same order which lead to different initializations.\\nYou can have an intuition of that by training with different seeds, you will see there is easily a 10% variation in the final accuracy...\\nIf you can do that, a better way to compare the results would thus be to take something like 10 different seeds for each training condition and compare the mean and standard deviation of the results.\"},\n",
       " '37_1': {'created_at': '2018-11-27T09:22:06Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '37',\n",
       "  'text': 'Thanks for your feedback!\\nAfter some investigations, it looks like t_total is not set properly for distributed training in BertAdam. The actual t_total per distributed worker should be divided by the worker count.\\nI have included the following fix in my PR ISSUE_REF \\n t_total = num_train_steps\\n if args.local_rank != -1:\\n t_total = t_total // torch.distributed.get_world_size()\\n optimizer = BertAdam(optimizer_grouped_parameters,\\n lr=args.learning_rate,\\n warmup=args.warmup_proportion,\\n t_total=t_total)'},\n",
       " '38': {'created_at': '2018-11-24T07:27:50Z',\n",
       "  'author': 'labixiaoK',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi, firstly, admire u for the great job. but I encounter 2 problems when i use it:\\n1. UnicodeDecodeError: 'gbk' codec can't decode byte 0x85 in position 4527: illegal multibyte sequence,\\nsame problem as ISSUE 52 when I excute the BertTokenizer.from_pretrained('bert-base-uncased'), but I successfully excute BertForNextSentencePrediction.from_pretrained('bert-base-uncased'), >. ```\\ntoken_type_ids: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\ntypes indices selected in [0, 1]. Type 0 corresponds to a INLINECODE and type 1 corresponds to\\na INLINECODE token (see BERT paper for more details).\\nbut in the following example, in **line 784**--> INLINECODE , why the '2' appears? I am confused. Otherwise, is the situation similar to '0, 1, 0 ' correct ? Or it should be similar to [ COMMIT ] , that is continuous '0' and continuous '1' ?\\nty.\"},\n",
       " '38_0': {'created_at': '2018-11-26T08:54:47Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '38',\n",
       "  'text': 'Hi,\\n(1) is solved on master. I will release a new release soon with the fixes on pip. In the mean time you can install from sources if you want.\\nI fixed the typo in the docstring you mention in (2), thanks, it should be a 1 instead of a 2.'},\n",
       " '39_0': {'created_at': '2018-11-25T09:12:22Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '39',\n",
       "  'text': 'Hi Jian, can you give me a small (self-contained) example showing how to get this error?'},\n",
       " '39_2': {'created_at': '2018-11-26T08:52:00Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '39',\n",
       "  'text': 'Thank you, you are right, I fixed that on master. It will be in the next release.'},\n",
       " '40': {'created_at': '2018-11-26T10:56:18Z',\n",
       "  'author': 'elyase',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'FILEPATH @ COMMIT'},\n",
       " '40_0': {'created_at': '2018-11-30T22:28:32Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '40',\n",
       "  'text': 'Hi USER , this model is now added in the new release VERSION .\\nI also added the other new model by Google (bert-large-cased)'},\n",
       " '41': {'created_at': '2018-11-26T21:50:15Z',\n",
       "  'author': 'ptrichel',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"The function convert_to_unicode is not in tokenization.py but used to be there in VERSION . When fine tuning with run_classifier.py, you get an ImportError: cannot import name 'convert_to_unicode'.\\n URL\"},\n",
       " '41_0': {'created_at': '2018-11-26T22:33:47Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '41',\n",
       "  'text': 'Fixed in master, thanks!'},\n",
       " '42': {'created_at': '2018-11-28T08:44:24Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nrecently, I am researching about Keyphrase generation. Usually, people use seq2seq with attention model to deal with such problem. Specifically I use the framework: URL which is implementation of URL .\\nNow I just change its encoder part to BERT, but the result is not good. The experiment comparison of two models is in the attachment.\\nCan you give me some advice if what I did is reasonable and if BERT is suitable for doing such a thing?\\nThanks.\\nRNN vs BERT in Keyphrase generation.pdf'},\n",
       " '42_0': {'created_at': '2018-11-28T09:27:45Z',\n",
       "  'author': 'waynedane',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'have u tried transformer decoder ?instead of rnn decoder.'},\n",
       " '42_1': {'created_at': '2018-11-28T09:33:34Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'not yet, I will try. But I think rnn decoder should not be such bad.'},\n",
       " '42_2': {'created_at': '2018-11-28T09:43:01Z',\n",
       "  'author': 'waynedane',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'not yet, I will try. But I think rnn decoder should not be such bad.\\n\\nemmm,maybe u should used mean of last layer to initialize decoder, not the last token representation of last layer.\\nI am also very concerned about the results of using transformer decoder. If you are done, can you tell me? Thank you.'},\n",
       " '42_3': {'created_at': '2018-11-28T09:55:02Z',\n",
       "  'author': 'waynedane',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'I think the batch size of RNN with BERT is too small. pleas see\\n\\n URL \\nline 377-378'},\n",
       " '42_4': {'created_at': '2018-11-28T10:15:42Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': \"I don't know what you mean by giving me this link. I set to 10 really because of the memory problem. Actually, when sentence length is 512, the max batch size is only 5, if it is 6 or bigger there will be memory error for my GPU.\"},\n",
       " '42_5': {'created_at': '2018-11-28T10:16:57Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'not yet, I will try. But I think rnn decoder should not be such bad.\\n\\nemmm,maybe u should used mean of last layer to initialize decoder, not the last token representation of last layer.\\nI am also very concerned about the results of using transformer decoder. If you are done, can you tell me? Thank you.\\n\\nYou are right. Maybe the mean is better, I will try as well. Thanks.'},\n",
       " '42_6': {'created_at': '2018-11-28T10:52:16Z',\n",
       "  'author': 'waynedane',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'May i ask a question? R u chinese?23333'},\n",
       " '42_7': {'created_at': '2018-11-28T10:57:07Z',\n",
       "  'author': 'waynedane',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'Cause for one example, it has N targets. We wanna put all targets in the same batch. 10 is too small that the targets of one example would be in different batches probably.'},\n",
       " '42_11': {'created_at': '2018-11-29T01:38:11Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': '80% ?f1 score ? encoder \\n\\nwaynedane 2018 11 28 11:14 :\\n...\\n ,bert wiki kp20k mini\\n bert, accuracy 80%, encoder?\\n\\n \\n You are receiving this because you authored the thread.\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .'},\n",
       " '42_13': {'created_at': '2018-11-29T02:15:53Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'mini bert ?'},\n",
       " '42_18': {'created_at': '2018-11-29T03:17:11Z',\n",
       "  'author': 'waynedane',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'checkpoint, , ?\\n\\n Junseong Kim bert model ,'},\n",
       " '42_19': {'created_at': '2018-11-29T03:21:56Z',\n",
       "  'author': 'whqwill',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': 'checkpoint'},\n",
       " '42_20': {'created_at': '2018-11-29T07:47:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': \"Hi guys,\\nI would like to keep the issues of this repository focused on the package it-self.\\nI also think it's better to keep the conversation in english so everybody can participate.\\nPlease move this conversation to your repository: URL or emails.\\nThanks, I am closing this discussion.\\nBest,\"},\n",
       " '42_22': {'created_at': '2019-07-22T01:50:08Z',\n",
       "  'author': 'Accagain2014',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '42',\n",
       "  'text': \"hi, USER I have some doubts about the usage manner of bert with RNN.\\nIn bert with RNN method, I see you only consider the last term's representation (I mean the TN's) as the input to RNN decoder, why not use the other term's representation, like T1 to TN-1 ? I think the last term's information is too less to represent all the context information.\"},\n",
       " '43': {'created_at': '2018-11-28T14:53:01Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi!\\nIn the config definition URL \\nin the Example usage of BertForSequenceClassification in modeling.py, there's things I don't understand:\\n\\nvocab_size in not an acceptable parameter name, by looking at the BertConfig class definition URL \\n\\neven by changing vocab_size into vocab_size_or_config_json_file, for the choice of the other params given in the example i.e.\\n vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024\\nI get:\\nValueError: The hidden size (512) is not a multiple of the number of attention heads (6)\\nI think that something similar may be true for the other classes as well, BertForQuestionAnswering, BertForNextSentencePrediction, etc.\\n\\nAm I missing something?\"},\n",
       " '43_0': {'created_at': '2018-11-30T22:29:24Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '43',\n",
       "  'text': 'Hi USER , you are right, I updated the docstrings in the new release VERSION .'},\n",
       " '44': {'created_at': '2018-11-28T17:04:39Z',\n",
       "  'author': 'johann-petrak',\n",
       "  'author_location': 'GB',\n",
       "  'type': 'issue',\n",
       "  'text': \"I have downloaded the model and vocab files into a specific location, using their original file names, so my directory for bert-base-cased contains:\\nbert-base-cased-vocab.txt\\nbert_config.json\\npytorch_model.bin\\n\\nBut when I try to specify the directory which contains these files for the FLAG parameter of extract_features.py I get the following error:\\nValueError: Can't find a vocabulary file at path ...\\n\\nWhen I specify a file that exists and is a proper file, the error messages seem to indicate that the program wants to untar and uncompress the files.\\nIs there no way to just specify a specific directory that contains the vocab, config, and model files?\"},\n",
       " '44_1': {'created_at': '2018-11-30T17:30:58Z',\n",
       "  'author': 'johann-petrak',\n",
       "  'author_location': 'GB',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '44',\n",
       "  'text': 'Thank you, FILEPATH ?'},\n",
       " '44_2': {'created_at': '2018-11-30T17:36:12Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '44',\n",
       "  'text': 'Yes :-) There is a new release planned for tonight that will fix this (among other things, basically all the other open issues).'},\n",
       " '44_3': {'created_at': '2018-11-30T22:30:11Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '44',\n",
       "  'text': 'Ok, this is now included in the new release VERSION (by ISSUE_REF ).'},\n",
       " '45': {'created_at': '2018-11-28T22:38:57Z',\n",
       "  'author': 'siddsach',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Thank you so much for this well-documented and easy-to-understand implementation! I remember meeting you at WeCNLP and am so happy to see you push out usable implementations of the SOA in pytorch for the community!!!!!\\nI have a question: The convert_tokens_to_ids method in the BertTokenizer that provides input to the BertEncoder uses an OrderedDict for the vocab attribute, which throws an error (e.g. KeyError: 'ketorolac') for any words not in the vocab. Can I create another vocab object that adds unseen words and use that in the tokenizer? Does the pretrained BertEncoder depend on the default id mapping?\\nIt seems to me that ideally in the long-term, this repo would incorporate character level embeddings to deal with unseen words, but idk if that is necessary for this use-case.\"},\n",
       " '45_0': {'created_at': '2018-11-29T14:44:43Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '45',\n",
       "  'text': \"If you tokenize properly the input (tokenize before convert_tokens), it automatically 'fallbacks' FILEPATH ( FLAG ) embedding.\\nYou can add new words in the vocabulary but you'll have to train the corresponding embeddings.\"},\n",
       " '45_1': {'created_at': '2018-11-30T22:31:36Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '45',\n",
       "  'text': 'Hi USER ,\\nThanks for your kind words!\\n USER is right, BPE progressively falls-back on character level embeddings for unseen words.'},\n",
       " '45_2': {'created_at': '2019-10-21T01:35:41Z',\n",
       "  'author': 'ilham-bintang',\n",
       "  'author_location': 'ID',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '45',\n",
       "  'text': \"If you tokenize properly the input (tokenize before convert_tokens), it automatically 'fallbacks' FILEPATH ( FLAG ) embedding.\\nYou can add new words in the vocabulary but you'll have to train the corresponding embeddings.\\n\\nHi, what do you mean tokenize properly the input (tokenize before convert_tokens) ?\\nCan you refer a tokenization sample (before and after) or a sample code if any? thank you\"},\n",
       " '46': {'created_at': '2018-11-29T03:33:09Z',\n",
       "  'author': 'zhaoxy92',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi, I have a question in terms of using BERT for sequential labeling task.\\nPlease correct me if I'm wrong.\\nMy understanding is:\\n\\nUse BertModel loaded with pretrained weights instead of MaskedBertModel.\\nIn such case, take a sequence of tokens as input, BertModel would output a list of hidden states, I only use the top layer hidden states as the embedding for that sequence.\\nThen to fine tune the model, add a linear fully connected layer and softmax to make final decision.\\n\\nIs this entire process correct? I followed this procedure but could not have any results.\\nThank you!\"},\n",
       " '46_0': {'created_at': '2018-11-30T12:26:33Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'Well that seems like a good approach. Maybe you can find some inspiration in the code of the BertForQuestionAnswering model? It is not exactly what you are doing but maybe it can help.'},\n",
       " '46_1': {'created_at': '2018-11-30T13:19:04Z',\n",
       "  'author': 'zhaoxy92',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"Thanks. It worked. However, a interesting issue about BERT is that it's highly sensitive to learning rate, which makes it very difficult to combine with other models\"},\n",
       " '46_2': {'created_at': '2018-11-30T14:05:09Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"USER what sequence labeling task are you doing? I've got CoNLL'03 NER running with the bert-base-cased model, and also found the same sensitivity to hyper-parameters.\\nThe best dev F1 score i've gotten after half a day a day of trying some parameters is VERSION VERSION , which is a bit lower than the VERSION dev score for BERT_base reported in the paper. I guess more tuning will increase the score some more.\\nThe best configuration for me so far is:\\n\\nBatch size: 160 (on four P40 GPUs with 24GB RAM each). Smaller batch sizes that fit on one or two GPUs give bad results.\\nOptimizer: Adam with learning rate 1e-4. Tried BertAdam with learning rate 1e-5, but it didn't seem to converge.\\n FILEPATH : Only fp32 works. Tried fp16 (half precision) to allow larger batch sizes, but this gave really low scores, with and without loss scaling.\\n\\nAlso, properly averaging the loss is important: Not just loss /= batch_size. You need to take into account padding and word pieces without predictions ( FILEPATH #33 (comment)). If you have a mask tensor that indicates which bert inputs correspond to tagged tokens, then the proper averaging is loss /= mask.float().sum\\nAnother tip, truncating the input (#66) enables much larger batch sizes. Without it the largest possible batch size was 56, but with truncating 160 is possible.\"},\n",
       " '46_3': {'created_at': '2018-11-30T17:38:41Z',\n",
       "  'author': 'zhaoxy92',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'I am also working on CoNLL03. Similar results as you got.'},\n",
       " '46_4': {'created_at': '2018-12-03T11:50:40Z',\n",
       "  'author': 'srslynow',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"USER with the risk of going off topic here, would you mind sharing your code? I'd love to read and adapt it for a similar sequential classification task.\"},\n",
       " '46_6': {'created_at': '2019-01-12T12:47:16Z',\n",
       "  'author': 'rremani',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'USER Thanks a lot for the starter, got awesome results!'},\n",
       " '46_7': {'created_at': '2019-01-15T03:32:08Z',\n",
       "  'author': 'nijianmo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'Thanks for sharing these tips here! It helps a lot.\\nI tried to finetune BERT on multiple imbalanced datasets and found the result quite unstable... For an imbalanced dataset, I mean there are much more O labels than the others under the {B,I,O} tagging scheme. Tried weighted cross-entropy loss but the performance is still not as expected. Has anyone met the same issue?\\nThanks!'},\n",
       " '46_8': {'created_at': '2019-01-27T15:59:42Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'Hi~ USER \\nI uesd batch size=16, and lr=2e-5, get the dev F1= VERSION and test F1= VERSION which lower than ELMO. What about your result now?'},\n",
       " '46_9': {'created_at': '2019-01-28T09:40:37Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"USER I didn't do any more CoNLL'03 runs since the numbers reported in the BERT paper were apparently achieved by using document context, which is different from the standard sentence-based evaluation. You can find more details here: ISSUE_REF (comment)\"},\n",
       " '46_10': {'created_at': '2019-01-28T10:11:24Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'Hmmm...I think they should tell that in the paper...And do you know where to find that they used document context?'},\n",
       " '46_11': {'created_at': '2019-01-28T10:27:20Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"That's what the folks over at allennlp said. I don't know where they got this information, maybe personal communication with one of the BERT authors?\"},\n",
       " '46_12': {'created_at': '2019-01-28T10:36:20Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'Anyway, thank you very much for tell me that.'},\n",
       " '46_13': {'created_at': '2019-03-19T07:25:35Z',\n",
       "  'author': 'kamalkraj',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'URL \\nReplicated results from BERT paper'},\n",
       " '46_14': {'created_at': '2019-05-04T01:43:02Z',\n",
       "  'author': 'JianLiu91',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'URL gives a solution that is very easy to understand.\\nHowever, I still wonder whether is the best practice.'},\n",
       " '46_15': {'created_at': '2019-05-14T11:57:39Z',\n",
       "  'author': 'dangal95',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"Hi all,\\nI am trying to train the BERT model on some data that I have. However, I am having trouble understanding how to adjust the labels following tokenization. I am trying to perform word level classification (similar to NER)\\nIf I have the following tokenized sentence and its' labels:\\noriginal_tokens = ['The', ', 'eng-30- COMMIT FLAG ', ' ', 'frailty']\\noriginal_labels = [0, 2, 3, 4, 1]\\n\\nThen after using the BERT tokenizer I get the following:\\nbert_tokens = ['[CLS]', 'the', ' ', 'eng-30- COMMIT FLAG ', ' ', 'frail', '##ty', '[SEP]']\\nAlso, I adjust my label array as follows:\\nbert_labels = [0, 2, 3, 4, 1, 1]\\nN.B. Tokens such as eng-30- COMMIT FLAG are not tokenized further as I included an ignore list which contains words and tokens that I do not want tokenized and I swapped them with the [unusedXXX] tokens found in the vocab.txt file.\\nNotice how the last word 'frailty' is transformed into ['frail', '##ty'] and the label '1' which was used for the whole word is now placed under each word piece. Is this the correct way of doing it? If you would like a more in-depth explanation of what I am trying to achieve you can read the following: URL \\nAny help would be greatly appreciated! Thanks in advance\"},\n",
       " '46_16': {'created_at': '2019-05-15T02:18:00Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'USER , adjusting the original labels is probably not the best way. A simpler method that works well is described in this issue, here ISSUE_REF (comment)'},\n",
       " '46_17': {'created_at': '2019-07-18T00:19:05Z',\n",
       "  'author': 'g-jing',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'USER Hi, I am recently considering using weighted loss in NER task. I wonder if you have tried weighted crf or weighted softmax in pytorch implementation. If so, did you get a good performance ? Thanks in advance.'},\n",
       " '46_18': {'created_at': '2019-07-25T02:50:18Z',\n",
       "  'author': 'weizhepei',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"Many thanks to USER ! For those who may concern , I've implemented a NER model based on pytorch-transformers and USER 's idea, which might help you get a quick start on it. Welcome to check this out.\"},\n",
       " '46_20': {'created_at': '2019-07-29T17:49:36Z',\n",
       "  'author': 'ramithp',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"That's what the folks over at allennlp said. I don't know where they got this information, maybe personal communication with one of the BERT authors?\\n\\nJust adding a bit of clarification since I revisited the paper after reading that comment.\\nFrom the BERT Paper Section VERSION ( URL )\\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data.\"},\n",
       " '46_21': {'created_at': '2019-07-29T21:28:36Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"USER that was added in v2 of the paper, but wasn't present in v1, which is the version the discussion here refers to\"},\n",
       " '46_22': {'created_at': '2019-07-29T22:14:18Z',\n",
       "  'author': 'ramithp',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': \"USER Yeah, I just realized that. No wonder I couldn't remember seeing it earlier. Thanks for confirming it. Just wanted to add that bit to the thread in case there were others that haven't read the revision.\"},\n",
       " '46_23': {'created_at': '2019-09-07T00:21:16Z',\n",
       "  'author': 'g-jing',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '46',\n",
       "  'text': 'USER USER USER USER USER \\nSorry about tag all of you. I wonder how to set the weight decay other than the BERT structure, for example the crf parameter after BERT output. Should I set it to be VERSION or 0? Sorry again for tagging all of you because it is kind of urgent.'},\n",
       " '47': {'created_at': '2018-11-29T09:18:21Z',\n",
       "  'author': 'mikelkl',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi there,\\nThanks for releasing this awesome repo, it does lots people like me a great favor.\\nSo far I've tried sentence-pair BertForSequenceClassification task, and it indeed work. I'd like to know if it is possible to use BertForSequenceClassification to model triple sentences classification problem and its input can be described as below:\\n**[CLS]A[SEP]B[SEP]C[SEP]**\\n\\nExpecting for your reply!\\nThanks & Regards\"},\n",
       " '47_0': {'created_at': '2018-11-29T15:04:21Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': \"Technically it is possible but BERT was not pretrained to handle multiple SEP tokens between sentences and does not have a third token_type, so I think it won't be easy to make it work. You may also want to use a new token for the second separation.\"},\n",
       " '47_1': {'created_at': '2018-11-30T01:54:35Z',\n",
       "  'author': 'mikelkl',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': 'Technically it is possible but BERT was not pretrained to handle multiple SEP tokens between sentences and does not have a third token_type, so I think it won\\'t be easy to make it work. You may also want to use a new token for the second separation.\\n\\nHi artemisart,\\nThanks for your reply.\\nSo, if someone wanna take multiple sentences as input of BertForSequenceClassification, let\\'s say a whole passage, an alternative way is to concatenate them into a single \"sentence\" and then fit it in, right?'},\n",
       " '47_2': {'created_at': '2018-11-30T10:19:31Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': \"I you don't have a separation ( FILEPATH ) then yes you can just concatenate them (but you are still limited to 512 tokens).\"},\n",
       " '47_3': {'created_at': '2018-11-30T22:58:16Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': 'USER I would also go with the solution and answer of USER .'},\n",
       " '47_4': {'created_at': '2019-03-13T01:43:09Z',\n",
       "  'author': 'alphanlp',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': 'USER hi, if i have a single sentence classification task, should the max length of sentence limited to half of 512, that is to say 256?'},\n",
       " '47_5': {'created_at': '2019-03-13T08:31:20Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': 'No, it will be better if you use the full 512 tokens.'},\n",
       " '47_6': {'created_at': '2019-08-08T04:55:31Z',\n",
       "  'author': 'thedrowsywinger',\n",
       "  'author_location': 'BD',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': \"wouldn't concatenating the whole passage into a single sentence mean losing context of each sentence? USER\"},\n",
       " '47_7': {'created_at': '2019-08-08T06:20:05Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': \"No it shouldn't\"},\n",
       " '47_8': {'created_at': '2019-08-12T06:29:59Z',\n",
       "  'author': 'thedrowsywinger',\n",
       "  'author_location': 'BD',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': 'What if I want to check on a huge corpus, that even concatenating into one sentence exceeds the 512 token limit? USER'},\n",
       " '47_9': {'created_at': '2019-08-28T01:14:07Z',\n",
       "  'author': 'mikelkl',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': 'USER maybe u should try Transformer-XL'},\n",
       " '47_10': {'created_at': '2021-09-15T11:00:52Z',\n",
       "  'author': 'sid8491',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '47',\n",
       "  'text': \"I you don't have a separation ( FILEPATH ) then yes you can just concatenate them (but you are still limited to 512 tokens).\\n\\nI have 3 inputs, 1 of the input contains conversation (QUERY, ANSWER).\\nQUERY: I want to ask a question.\\n\\nANSWER: Sure, ask away.\\nQUERY: How is the weather today?\\nANSWER: It is nice and sunny.\\nQUERY: Okay, nice to know.\\nANSWER: Would you like to know anything else?\\n\\nHow can I tell the model to separate the turns of conversation? Model is classification model.\\nI was thinking to add a new special token between the turns but could not get it work.\"},\n",
       " '48': {'created_at': '2018-11-30T05:48:04Z',\n",
       "  'author': 'danyaljj',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'When running the following command for tuning on squad, I am getting a petty error inside logger TypeError: object of type \\'NoneType\\' has no len(). Any thoughts what could be the main cause of the problem?\\nFull log:\\n FILEPATH \\\\\\n\\n.\\n.\\n.\\n\\n FILEPATH :10:14 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :10:14 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :10:14 - INFO - __main__ - start_position: 47\\n FILEPATH :10:14 - INFO - __main__ - end_position: 48\\n FILEPATH :10:14 - INFO - __main__ - answer: the 1870s\\n FILEPATH :14: FILEPATH \\n FILEPATH :14:51 - INFO - __main__ - ***** Running training *****\\n FILEPATH :14:51 - INFO - __main__ - Num orig examples = 87599\\nTraceback (most recent call last):\\n File \" FILEPATH \", line 989, in \\n main()\\n File \" FILEPATH \", line 884, in main\\n logger.info(\" Num split examples = %d\", len(train_features))\\nTypeError: object of type \\'NoneType\\' has no len()'},\n",
       " '48_0': {'created_at': '2018-11-30T13:24:02Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '48',\n",
       "  'text': 'Oh I see, this should be fixed in master by COMMIT just update your repo please.'},\n",
       " '49': {'created_at': '2018-11-30T06:30:56Z',\n",
       "  'author': 'ejld',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi, I am running the same task with the same hyper parameters as the official Google Tensorflow implementation of BERT, however, I am getting around VERSION % lower accuracy. Can you please give any hint about the possible cause?\\nThanks!'},\n",
       " '49_1': {'created_at': '2018-11-30T22:56:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '49',\n",
       "  'text': 'Hi USER , yes BERT has a large variance on many fine-tuning tasks (see also the discussion in ISSUE_REF ).\\nYou should try a bunch of different seeds (like 10 seeds for example) and compare the mean and standard deviation of the results.'},\n",
       " '50_0': {'created_at': '2018-11-30T14:02:55Z',\n",
       "  'author': 'timniven',\n",
       "  'author_location': 'TW',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '50',\n",
       "  'text': 'I have the same issue.\\n\\nOSError: HEAD request failed for url URL with status code 404\\n\\nIt would be nice to be able to cache the vocab files as well as the model weights out of the box.'},\n",
       " '50_1': {'created_at': '2018-11-30T14:13:48Z',\n",
       "  'author': 'zeze-zzz',\n",
       "  'author_location': 'KR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '50',\n",
       "  'text': \"I found temporary solution for this issue.\\nBertTokenizer.from_pretrained method accepts local file instead of model_name\\nex) BertTokenizer.from_pretrained('/ FILEPATH ')\\nvocab txt file can be downloaded from google bert repo.\"},\n",
       " '50_2': {'created_at': '2018-11-30T14:44:24Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '50',\n",
       "  'text': 'The files are back. Sorry, wrong manipulation while adding the new models.'},\n",
       " '50_3': {'created_at': '2020-05-24T08:43:29Z',\n",
       "  'author': 'rlpatrao',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '50',\n",
       "  'text': \"I found temporary solution for this issue.\\nBertTokenizer.from_pretrained method accepts local file instead of model_name\\nex) BertTokenizer.from_pretrained('/ FILEPATH ')\\n\\nWell, this solution doesn't seem to be working now, I get\\nOSError: Model name ' FILEPATH ' was not found in tokenizers model name list ( FILEPATH , bart-large-mnli, bart-large-cnn, bart-large-xsum). We assumed ' FILEPATH ' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.\"},\n",
       " '50_4': {'created_at': '2021-05-05T07:14:47Z',\n",
       "  'author': 'MitraTj',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '50',\n",
       "  'text': \"I found temporary solution for this issue.\\nBertTokenizer.from_pretrained method accepts local file instead of model_name\\nex) BertTokenizer.from_pretrained('/ FILEPATH ')\\nvocab txt file can be downloaded from google bert repo.\\n\\nHi, I add this file, however I got another error:\\n*** json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1)\\nAny help please?\"},\n",
       " '51': {'created_at': '2018-11-30T18:39:54Z',\n",
       "  'author': 'samyam',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hello,\\nI am trying to run the squad fine tuning script, but it hangs after printing out a few predictions. I am attaching the log. Can you help take a look?\\nI am running the script on a machine with 8 M40s.\\nbert_squad.log\\nBest,\\nSamyam'},\n",
       " '51_0': {'created_at': '2018-11-30T19:47:07Z',\n",
       "  'author': 'samyam',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '51',\n",
       "  'text': 'Never mind, it just needed time to process the examples. It might be good to have the progress bar inside convert_examples_to_features.'},\n",
       " '51_1': {'created_at': '2018-11-30T19:57:43Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '51',\n",
       "  'text': \"Maybe try distributed training? I don't think PyTorch DataParallel will be very efficient on 8 GPUs due to the python GIL.\"},\n",
       " '51_2': {'created_at': '2018-11-30T20:23:57Z',\n",
       "  'author': 'samyam',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '51',\n",
       "  'text': 'Thanks for the suggestion. I will try that. Currently, its showing me about 9 hours to fine tune bert-large on squad with batch size of 32 using DataParallel.\\nThe performance improves quite a bit if a if I use a batch size of 256 with gradient accumulate, which makes sense as this reduces the frequency of communication of the gradients. A question I have is, does the learning rate adapt automatically to the batch size being used? Have you tried larger batch sizes?'},\n",
       " '52': {'created_at': '2018-12-01T19:34:40Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'issue',\n",
       "  'text': \"I think that\\n URL \\nmay well have a problem, as it's not consistent with\\n URL \\nnor with\\n URL \\nand this currently breaks the example.\\nOne quick patch would be to replace that line with\\ntmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\\nlogits = model(input_ids, segment_ids, input_mask)\\n\\nBut I am not so sure, there are likely better ways.\"},\n",
       " '52_0': {'created_at': '2018-12-02T08:17:39Z',\n",
       "  'author': 'yanshanjing',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '52',\n",
       "  'text': 'You are right, I also encountered this small error.'},\n",
       " '52_1': {'created_at': '2018-12-02T12:02:34Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '52',\n",
       "  'text': 'Thanks for noticing, fixed in ISSUE_REF .'},\n",
       " '53_0': {'created_at': '2018-12-02T12:07:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '53',\n",
       "  'text': 'Can you post a more detailed log?'},\n",
       " '53_2': {'created_at': '2018-12-02T12:26:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '53',\n",
       "  'text': \"Strange error. I am only using standard library here. Maybe it has something to do with your installation of Conda. You can try to manually specify a cache directory for the package by either:\\n\\nsetting the environment variable PYTORCH_PRETRAINED_BERT_CACHE=XXX to a directory XXX you created to store the downloaded models.\\nsending the path to this directory to the tokenizer and model using the cache_dir=XXX arguments, for example: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir=XXX)\"},\n",
       " '54': {'created_at': '2018-12-03T07:56:56Z',\n",
       "  'author': 'A-Rain',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': \"hello, when I am running run_classifier.py with MRPC dataset, there seems to be an mistake. the mistake is as following:\\n\\nthe mistake is happening when training is over and the model is for evaluating\\nwith torch.no_grad():\\n tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\\n\\nhere I found the size of logits is []\\nI'm using python3.5 and torch= VERSION , I don't know how to fix it.\"},\n",
       " '54_0': {'created_at': '2018-12-03T08:37:08Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '54',\n",
       "  'text': \"Hi, just update the repo to the current master, this should have been fixed this weekend (re-open the issue of it's not).\"},\n",
       " '55': {'created_at': '2018-12-03T10:58:43Z',\n",
       "  'author': 'Deep1994',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi, I have a dataset like :\\nFrom Monday to Friday most people are busy working or studying, but in the evenings and weekends they are free and _ themselves.\\nAnd there are four candidates for the missing blank area:\\n[\"love\", \"work\", \"enjoy\", \"play\"], here \"enjoy\" is the correct answer, it is a cloze-style task, and it looks like the maskLM in the BERT, the difference is that I don\\'t want to search the candidate from all the tokens but the four given candidates, how can I do this? It looks like negtive sampling method. Do you have any idea? Thank you!'},\n",
       " '55_1': {'created_at': '2018-12-09T20:57:24Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '55',\n",
       "  'text': \"The solution of USER looks good. Don't hesitate to re-open the issue if you have other questions.\"},\n",
       " '55_2': {'created_at': '2020-03-03T14:41:33Z',\n",
       "  'author': 'tarskiandhutch',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '55',\n",
       "  'text': \"Just a note that this solution does not help you if any of your candidates are out of your model's whole-word vocabulary. (A work-around is required to deal with BERT's reliance on word-piece tokens.)\"},\n",
       " '56': {'created_at': '2018-12-03T12:00:09Z',\n",
       "  'author': 'ZacharyWaseda',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'I change the run_classfifier.py in order to support continuously training. i save the model.state_dict() and the BertAdam optimizer.state_dict(), and I load them when start continuously training. However, After some epochs, the loss will increase little by little and finally end with a large loss value. I do not know the reason. Please help me.'},\n",
       " '57_0': {'created_at': '2018-12-04T02:06:25Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '57',\n",
       "  'text': \"Looks like there was a code change which changed the forward method of the model involved here from returning a tensor to returning a tuple of tensors and the example hasn't been updated yet to reflect that change. There's probably a line in run_classifier.py like\\nloss = model(input...)\\nwhich now needs to be\\nloss, something_else = model(input...)\"},\n",
       " '57_1': {'created_at': '2018-12-04T07:27:06Z',\n",
       "  'author': 'Qzsl123',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '57',\n",
       "  'text': \"Looks like there was a code change which changed the forward method of the model involved here from returning a tensor to returning a tuple of tensors and the example hasn't been updated yet to reflect that change. There's probably a line in run_classifier.py like\\nloss = model(input...)\\nwhich now needs to be\\nloss, something_else = model(input...)\\n\\nYou are right! Thx!\"},\n",
       " '58_1': {'created_at': '2018-12-05T00:12:05Z',\n",
       "  'author': 'chledowski',\n",
       "  'author_location': 'PL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '58',\n",
       "  'text': 'Thanks, it worked. I think it could be great if 3 classes was default when choosing MNLI :)'},\n",
       " '59': {'created_at': '2018-12-04T03:13:30Z',\n",
       "  'author': 'danyaljj',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"After training squad, I have a model file in a local folder:\\n FLAG 1 khashab2 cs_danr 4.7M Nov 21 19:20 dev- VERSION .json\\n FLAG 1 khashab2 cs_danr 3.4K Nov 29 22:52 evaluate- VERSION .py\\ndrwxrwsr-x 2 khashab2 cs_danr 10 Nov 30 14:57 out2\\n FLAG 1 khashab2 cs_danr 29M Nov 21 19:20 train- VERSION .json\\n FLAG 1 khashab2 cs_danr 490M Nov 29 23:14 train- VERSION .json_bert-base-uncased_384_128_64\\n FLAG 1 khashab2 cs_danr 490M Nov 30 15:05 train- VERSION .json_bert-large-uncased_384_128_64\\n\\nI want to use this pre-trained model to make predictions. Is there any example that I can follow this? (if not any pointers?) I looked into the instructions and didn't find anything relevant on this.\"},\n",
       " '59_0': {'created_at': '2018-12-14T14:42:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '59',\n",
       "  'text': 'Hi there are now examples on how you can save and reload the models in the examples (run_classifier, run_squad and run_swag)'},\n",
       " '60': {'created_at': '2018-12-04T11:08:09Z',\n",
       "  'author': 'DeeepSeeek',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Zero-pad up to the sequence length.\\nwhile len(input_ids) < max_seq_length:\\ninput_ids.append(0)\\ninput_mask.append(0)\\nsegment_ids.append(0)\\nin segment_ids array,1 indicates token from passage and 0 indicate token form query.\\nwhen padding,why segment_ids filled with 0,which represents query'},\n",
       " '60_1': {'created_at': '2018-12-05T15:37:03Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '60',\n",
       "  'text': 'Hi, what is your question?'},\n",
       " '60_2': {'created_at': '2018-12-05T15:54:10Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '60',\n",
       "  'text': 'Strictly speaking, the zero-padding in segment_ids leads to ambiguous tensor entries, because 0 can mean both \"first sentence\" (or query in another task?) and \"padding\".\\nBut in practice this isn\\'t a problem because anything related to padding gets masked out later.'},\n",
       " '61_0': {'created_at': '2018-12-04T13:46:59Z',\n",
       "  'author': 'zhongpeixiang',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '61',\n",
       "  'text': 'I probably know the bug. The final output layer is for binary classification but I use it for 4-class classification. I thought BERT can automatically decide between sigmoid and soft max. I will replace it with my own classifier tomorrow and see how it goes.'},\n",
       " '61_1': {'created_at': '2018-12-05T03:41:32Z',\n",
       "  'author': 'zhongpeixiang',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '61',\n",
       "  'text': 'The mismatched output size between BERT and our dataset is the bug. Also, remember to set the num_labels to your output size:\\noutput_size = 4\\nmodel.classifier = nn.Linear(768, output_size)\\nmodel.num_labels = output_size'},\n",
       " '62': {'created_at': '2018-12-05T10:11:21Z',\n",
       "  'author': 'agemagician',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hello,\\nI am trying to extract features from German text using bert-base-multilingual-cased. However, my text is bigger than 512 words.\\nIs there any way to use the pertained Bert for text greater than 512 words'},\n",
       " '62_0': {'created_at': '2018-12-05T10:41:25Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '62',\n",
       "  'text': \"Hello,\\nI do not think that it is possible out of the box. The article states the following:\\n\\nWe use learned positional embeddings with supported sequence lengths up to 512 tokens.\\n\\nThe positional embeddings are therefore limited to 512 tokens. You may be able to add positional embeddings for position greater than 512 and learn them on your specific dataset but I don't know how efficient that would be.\"},\n",
       " '62_1': {'created_at': '2018-12-09T21:04:52Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '62',\n",
       "  'text': 'Hi USER , you cannot really use pretrained bert for text longer than 512 tokens per se but you can use the sliding window approach.\\nCheck this issue of the original bert repo for more details: ISSUE_REF'},\n",
       " '63': {'created_at': '2018-12-05T14:01:41Z',\n",
       "  'author': 'Qzsl123',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Is it posible to fine tuned to the multi choices problems , which usually has one passage, question and ABCD four options?'},\n",
       " '63_0': {'created_at': '2018-12-05T14:27:37Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': \"Yes it is, the code is not written yet but I'm planning to work on it. The idea is to format the input data the same way the authors of Improving Language Understanding with Unsupervised Learning\\n\\nYou run an inference (context, choice) for each choice, you compute the image of the [CLS] token by a linear layer with 1 output and then compute a softmax over the output of all choices.\\nI will try to create a PR with this code very soon.\"},\n",
       " '63_1': {'created_at': '2018-12-05T14:57:06Z',\n",
       "  'author': 'Qzsl123',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': 'Thx for the reply.\\nActually, I have the same plan. But I am not sure whether it will work. Anyway, I will have a try.'},\n",
       " '63_2': {'created_at': '2018-12-05T14:59:06Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': \"If it worked in the OpenAI paper, I don't really see why it wouldn't work with this architecture.\"},\n",
       " '63_3': {'created_at': '2018-12-07T08:25:47Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': 'USER The code for multiple choice task is available in PR ISSUE_REF if you want to test it.'},\n",
       " '63_4': {'created_at': '2018-12-07T08:31:37Z',\n",
       "  'author': 'Qzsl123',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': 'USER yeah, I am trying to run it. Thanks for the wonderful job!'},\n",
       " '63_5': {'created_at': '2018-12-25T06:47:22Z',\n",
       "  'author': 'DukeZhu',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': \"Yes it is, the code is not written yet but I'm planning to work on it. The idea is to format the input data the same way the authors of Improving Language Understanding with Unsupervised Learning\\n\\nYou run an inference (context, choice) for each choice, you compute the image of the [CLS] token by a linear layer with 1 output and then compute a softmax over the output of all choices.\\nI will try to create a PR with this code very soon.\\n\\nhi,The multi choices problem usually has one passage, question and ABCD four options In your model, dose context means passage&question ?\"},\n",
       " '63_6': {'created_at': '2020-10-13T10:00:46Z',\n",
       "  'author': 'monk1337',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '63',\n",
       "  'text': 'Any update on this issue?'},\n",
       " '64': {'created_at': '2018-12-05T19:13:24Z',\n",
       "  'author': 'kh522',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Is there a reason why the Bert uncased model and the Bert large model give lower results that the cased model on downstream tasks?'},\n",
       " '64_0': {'created_at': '2018-12-05T20:54:13Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': 'Any specific example that we could investigate?'},\n",
       " '64_1': {'created_at': '2018-12-05T21:17:50Z',\n",
       "  'author': 'kh522',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': \"I've implemented a version of SQuAD VERSION on top of the current SQuAD that is similar to the way Google implemented their's on the official Bert repo. The base cased model works fine, but I noticed that uncased models tend to give worse results, even the large model.\"},\n",
       " '64_2': {'created_at': '2018-12-05T22:02:58Z',\n",
       "  'author': 'elyase',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': 'USER would love to try it out. Are you planing to share your code?'},\n",
       " '64_3': {'created_at': '2018-12-05T22:12:46Z',\n",
       "  'author': 'kh522',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': \"Sorry, but not quite yet. I was wondering if anyone had an intuition behind the error. If I recall correctly the SQuAD file lowercases the inputs for the tokenizer as a default. Shouldn't this mean that the pretrained uncased actually does better than the cased version?\"},\n",
       " '64_4': {'created_at': '2018-12-05T22:35:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': 'Hi USER , were you carefull no to lower case the input in the case of the uncased models? By default the tokenizer will lower the input see here in the readme'},\n",
       " '64_5': {'created_at': '2018-12-05T22:57:38Z',\n",
       "  'author': 'kh522',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': 'Ah, I see. That would be a problem. I assume that the difference in accent markers will lead to a lower result. That being said, FILEPATH ?'},\n",
       " '64_6': {'created_at': '2018-12-06T18:10:00Z',\n",
       "  'author': 'kh522',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': \"I've tried running it with the FLAG flag set to False, and the results are still not good yet. Is there another possible idea?\"},\n",
       " '64_7': {'created_at': '2018-12-09T21:07:49Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': 'Try 10 different seeds maybe? A bigger batch-size can help too. More generally, you should try to explore the space of hyper-parameters for fine-tuning, FILEPATH .'},\n",
       " '64_8': {'created_at': '2019-03-27T19:11:38Z',\n",
       "  'author': 'maxsonate',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': 'In the run_squad.py, the seed is set to 42, therefore the results reported in the repo should be reproducible, as there would not be any other randomness.'},\n",
       " '64_9': {'created_at': '2020-06-19T08:34:57Z',\n",
       "  'author': 'abmitra84',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '64',\n",
       "  'text': \"Hi USER , were you carefull no to lower case the input in the case of the uncased models? By default the tokenizer will lower the input see here in the readme\\n\\n USER \\nJust want to be sure (as the link does not land me in anything specific). FILEPATH 't need to lower case input text as tokenizer takes care of it; is this correct understanding?\\nIn case, someone lowercases the text, what problem it can cause?\"},\n",
       " '65': {'created_at': '2018-12-06T14:40:33Z',\n",
       "  'author': 'avisil',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"Is there any way of not updating the BERT embeddings during the fine tuning process? For example while running on SQUAD, I want to see the effect of not updating the parameters associated with the BERT embeddings. I saw that required_grad is set to True for cpu and fp16. Which makes me think that it's assuming do_grad for all the parameters.\\nI'm asking if there's any quick way to disable the update to those embeddings but let the model update other parameters.\"},\n",
       " '65_0': {'created_at': '2018-12-06T15:11:23Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': \"You can do it by setting the requires_grad attribute of the embedding layer in BertModel. That will look something like this:\\n model = BertForQuestionAnswering.from_pretrained(args.bert_model,\\n cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(args.local_rank))\\n model.bert.embeddings.requires_grad = False\\n\\nI haven't tested this code but it should do what you are asking.\\nMore explanation are available on the PyTorch forums\"},\n",
       " '65_1': {'created_at': '2018-12-06T16:30:42Z',\n",
       "  'author': 'avisil',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': 'Thanks let me try this..I was thinking of going through the BERTAdam optimizer.'},\n",
       " '65_3': {'created_at': '2018-12-07T08:18:51Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': 'Thanks for your feedback.\\nHave you checked that the values of the embedding matrix are indeed unchanged by the finetuning?'},\n",
       " '65_4': {'created_at': '2018-12-07T15:19:50Z',\n",
       "  'author': 'avisil',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': \"Nope I haven't.. I plan to do that today. USER do you think we're on the right track? Just need your 2 cents :)\"},\n",
       " '65_5': {'created_at': '2018-12-09T21:13:11Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': 'Yes you can do\\nfor p in model.bert.embeddings.parameters():\\n p.requires_grad = False\\nYou can also just not send these parameters to the optimizer (when you create the optimizer) as detailed on the PyTorch forums. Both methods will work. Combining the two will gives the lowest overhead (no un-necessary computation of gradient and no un-necessary check of update during the optimizer step). The PyTorch forum is the best reference for this kind of general question.'},\n",
       " '65_7': {'created_at': '2019-04-09T10:32:23Z',\n",
       "  'author': 'ConanCui',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': \"Nope I haven't. I plan to do that today. USER do you think we're on the right track? Just need your 2 cents :)\\n\\nHow to check the values of the embedding matrix change or not?\"},\n",
       " '65_9': {'created_at': '2022-01-18T22:29:18Z',\n",
       "  'author': 'cgr71ii',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '65',\n",
       "  'text': 'Nope I haven\\'t. I plan to do that today. USER do you think we\\'re on the right track? Just need your 2 cents :)\\n\\nHow to check the values of the embedding matrix change or not?\\n\\nHi! You can use the following code in order to check if any layer has been modified (it should work for any pytorch code if I am not wrong, not just BERT):\\nimport copy\\nfrom transformers import BertModel\\n\\nbert = BertModel.from_pretrained(\\'bert-base-uncased\\')\\nlayer = bert.embeddings\\nfrozen_parameters = {}\\n\\n# Copy tensors\\nfor name, p in layer.named_parameters():\\n frozen_parameters[name] = copy.deepcopy(p.data) # Freeze in order to be able to compare later\\n\\n# Do stuff ...\\n\\n# Check if the value of the tensors have been updated\\nfor name, p in layer.named_parameters():\\n updated = (frozen_parameters[name] != p.data).any().cpu().detach().numpy()\\n\\n print(f\"Layer \\'{name}\\' has been updated? {\\'yes\\' if updated else \\'no\\'}\")\\nIt is very similar to the code I use in order to check if a layer has been updated (remember that it won\\'t be updated if grad_fn is None), but I have not tested this exactly code.\\nI hope it helps!'},\n",
       " '66_2': {'created_at': '2018-12-29T01:43:55Z',\n",
       "  'author': 'liu946',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '66',\n",
       "  'text': \"USER I got the problem because I didn't limit the max length of the sentence so that the position embedder get the position token id lager than its length.\"},\n",
       " '66_3': {'created_at': '2019-06-03T07:52:21Z',\n",
       "  'author': 'yyHaker',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '66',\n",
       "  'text': \"USER I got the problem because I didn't limit the max length of the sentence so that the position embedder get the position token id lager than its length.\\n\\n3q,I meet the same problem too, and I solve the problem after I set the max length of input sequence, but here how the position embedder get the position token id?\"},\n",
       " '66_5': {'created_at': '2019-09-18T08:10:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '66',\n",
       "  'text': 'USER these are old issues related to pytorch_pretrained_bert, you should rather open a new issue with a clear description of the model you are using, the version of the library and the error message you have.'},\n",
       " '66_6': {'created_at': '2021-05-20T08:11:30Z',\n",
       "  'author': 'machouz',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '66',\n",
       "  'text': 'If you are using a tokenizer try:\\ntokenizer(input, truncation=True)\\nThis will truncate the input to the max_length'},\n",
       " '66_7': {'created_at': '2022-03-16T22:28:56Z',\n",
       "  'author': 'IreneZihuiLi',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '66',\n",
       "  'text': 'This actually solved my issue...'},\n",
       " '67': {'created_at': '2018-12-07T13:42:59Z',\n",
       "  'author': 'zhezhaoa',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'First of all, Thank you for this great job. I use the official tensorflow implementation to pretrain on my corpus and then save the model. I want to convert this model to pytorch format and use it, but I got the error:\\nTraceback (most recent call last):\\nFile \"convert_tf_checkpoint_to_pytorch.py\", line 105, in \\nconvert()\\nFile \"convert_tf_checkpoint_to_pytorch.py\", line 86, in convert\\npointer = getattr(pointer, l[0])\\nAttributeError: \\'Parameter\\' object has no attribute \\'adam_m\\'\\nCould you give me some advice? Thank you very much.\\nIt is great if you can release the pretrain code. I think it is useful even we cannot use TPU. Because we can fine-tune above google\\'s pertained model.'},\n",
       " '67_1': {'created_at': '2018-12-08T11:32:29Z',\n",
       "  'author': 'zhezhaoa',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '67',\n",
       "  'text': 'Thank you very much! It could be great if you can provide pertaining code like the official TF implementation.'},\n",
       " '67_2': {'created_at': '2018-12-14T14:42:40Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '67',\n",
       "  'text': 'Ok this loading issue is now fixed in master and the new VERSION release.'},\n",
       " '68': {'created_at': '2018-12-07T13:44:09Z',\n",
       "  'author': 'wcgan',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Thanks a lot for the code! I need help figuring out why the script is not working so long the batch_size is set to be above 1. Specifically, it seems to be stuck at Line 908: loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions). I am using 4 k80. Thanks!'},\n",
       " '68_0': {'created_at': '2018-12-07T14:11:41Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '68',\n",
       "  'text': 'Please copy paste the command you are using to run this example.'},\n",
       " '68_2': {'created_at': '2018-12-09T21:21:54Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '68',\n",
       "  'text': \"I don't see why this wouldn't work.\\nMaybe update the repo & module to the latest version?\\nYou should also add FLAG to the arguments if you are using an uncased model.\\nMaybe post a full log of your output?\"},\n",
       " '68_3': {'created_at': '2018-12-11T03:35:24Z',\n",
       "  'author': 'wcgan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '68',\n",
       "  'text': 'Updated to the latest version but it still does not work. When I terminate the script after it is stuck for some time, I get the message \\'../ FILEPATH \", line 1072, in _wait_for_tstate_lock elif lock.acquire(block, timeout)\\'. Perhaps it is running into some deadlock condition?\\nI\\'m not sure how to obtain a full log, would you be able to explain how can I do so? Thanks!'},\n",
       " '68_4': {'created_at': '2018-12-14T14:43:02Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '68',\n",
       "  'text': 'Hi, you can try with the new release VERSION .'},\n",
       " '68_5': {'created_at': '2019-01-01T21:55:15Z',\n",
       "  'author': 'johnsonice',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '68',\n",
       "  'text': 'Is the problem resolved? I am having the same issue, using 2 gtx1080ti.\\nit stuck when running on multiple gpus. I have to comment out torch.nn.DataParallel(model), to make it work.'},\n",
       " '68_6': {'created_at': '2019-02-21T03:01:24Z',\n",
       "  'author': 'ankit-ai',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '68',\n",
       "  'text': 'If you are using a multi-GPU setting, pytorch splits the batch dynamically between the 2 GPUs.\\nExample - batch_size =5\\nGPU 0 may get 3,max_sequence_len\\nGPU 1 may get 2,max_sequence_len\\nThis could be a cuda splitting issue, I recommend you try a single GPU setting to debug this.\\nThanks,\\nAnkit'},\n",
       " '69': {'created_at': '2018-12-07T16:02:00Z',\n",
       "  'author': 'nischalhp',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'issue',\n",
       "  'text': 'URL \\nThe confusing part here is that in line 146, only the first answer is considered, so I am wondering why is there a check for multiple answers before.\\nAlso, SQuad dataset has multiple answers for the same question. Is this by design or am I fundamentally missing something?'},\n",
       " '69_0': {'created_at': '2018-12-08T02:07:36Z',\n",
       "  'author': 'ymcui',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '69',\n",
       "  'text': 'Hi,\\nIn train- VERSION .json, there is only one answer for the question.\\nIn dev- VERSION .json and hidden test- VERSION .json, there are several answers for a given question.\\nI think the code that you mentioned is designed for not mistakenly using dev- VERSION .json for training. If you are going to use your own data or other types of data that has multiple answers, you can simply comment out this part.\\nBest'},\n",
       " '69_1': {'created_at': '2018-12-08T11:53:50Z',\n",
       "  'author': 'nischalhp',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '69',\n",
       "  'text': 'Hello USER ,\\nI did exactly that, thank you for confirming. Just wanted to be sure that there are no other implications. You are right, I have converted our dataset into SQuAD form and using that with the model.\\nRegards,\\nNischal'},\n",
       " '70': {'created_at': '2018-12-08T08:38:37Z',\n",
       "  'author': 'Arjunsankarlal',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Well I am trying to generate embedding for a large sentence. I get this error\\n\\nTraceback (most recent call last):\\nall_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\\nFile \"/ FILEPATH \", line 477, in call\\nresult = self.forward(*input, **kwargs)\\nFile \"/ FILEPATH \", line 611, in forward\\nembedding_output = self.embeddings(input_ids, token_type_ids)\\nFile \"/ FILEPATH \", line 477, in call\\nresult = self.forward(*input, **kwargs)\\nFile \"/ FILEPATH \", line 196, in forward\\nposition_embeddings = self.position_embeddings(position_ids)\\nFile \"/ FILEPATH \", line 477, in call\\nresult = self.forward(*input, **kwargs)\\nFile \"/ FILEPATH \", line 110, in forward\\nself.norm_type, self.scale_grad_by_freq, self.sparse)\\nFile \"/ FILEPATH \", line 1110, in embedding\\nreturn torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\\nRuntimeError: FILEPATH :352\\n\\nI find that max_position_embeddings (default size 512) is getting exceeded. Which is taken from the config that is downloaded as part of the initial step. Initially the download was done to the default location PYTORCH_PRETRAINED_BERT_CACHE where I was not able to find the config.json other than the model file and vocab.txt (named with random characters). I did it to a specific location in local with the cache_dir param, here also I was facing the same problem of finding the bert_config.json.\\nAlso I found a file in both the default cache and local cache, named with junk characters of JSON type. When I tried opening it, I could just see this\\n{\"url\": \" URL \"etag\": \"\" COMMIT -49\"\"}\\nAny help to modify the config.json would be appreciated.\\nOr if this is been caused for a different reason, Please let me know.'},\n",
       " '70_0': {'created_at': '2018-12-08T11:15:09Z',\n",
       "  'author': 'Arjunsankarlal',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '70',\n",
       "  'text': 'The problem is because of the max_position_embeddings default size is 512 and it is exceeding in the case of my input as I mentioned. For now I have just made hack by hard coding it directly in the modelling.py file directly . Yet need to know, where to find the bert_config.json file and changing it there would be the correct way of doing it.'},\n",
       " '70_1': {'created_at': '2018-12-08T11:55:18Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '70',\n",
       "  'text': \"The config file is located in the .tar.gz archive that is getting downloaded, cached, and then extracted on the fly as you create a BertModel instance with the static from_pretrained() constructor.\\nYou'll see a log message like\\n FILEPATH \\n\\nIf you extract that archive yourself, you'll find the bert_config.json file. The thing, though, is that it doesn't make sense to modify this file, as it is tied to the pretrained models. If you increase max_position_embeddings in the config, you won't be able to use the pretrained models.\\nInstead, you will have to train a model from scratch, which may or -- more likely -- may not be feasible depending on the hardware you have access to.\"},\n",
       " '70_2': {'created_at': '2018-12-09T07:32:03Z',\n",
       "  'author': 'Arjunsankarlal',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '70',\n",
       "  'text': \"Yeah as you said, while debugging I noticed that every time the .tar.gz file was extracted to a new temp cache location and from there models are fetched. Even in that case we are not able to find the json file where it was extracted. Also I think max_position_embeddings does not relate with the model training because, when I changed its value(before loading the model with torch.load) like this\\nconfig.__dict__['max_position_embeddings'] = 2048\\nfrom 512 to 2048 (hard coded way) the code ran properly without any error.\\nAnd the lines in modelling.py tells that it can be customised if required. But I don't see a way parameterising it so that it will be changed while fetching the config, because it is loaded like this.\\nIt would be great if customisations are supported for the applicable options.\"},\n",
       " '70_3': {'created_at': '2018-12-09T16:04:14Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '70',\n",
       "  'text': \"It does not make sense to customize options when using pretrained models, it only makes sense when training your own model from scratch.\\nYou cannot use the pretrained models with another max_position_embeddings than 512, because the pretrained models contain pretrained embeddings for 512 positions.\\nThe original transformer paper introduced a positional encoding which allows extrapolation to arbitrary input lengths, but this was not used in BERT.\\nYou can override max_position_embeddings, but this won't have any effect. The model will probably run fine for shorter inputs, but you will get a RuntimeError: cuda runtime error (59) for an input longer than 512 word pieces, because the embedding lookup here will attempt to use an index that is too large.\"},\n",
       " '70_4': {'created_at': '2018-12-09T21:19:31Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '70',\n",
       "  'text': \"Indeed, it doesn't make sense to go over 512 tokens for a pre-trained model.\\nIf you have longer text, you should try the sliding window approach detailed on the original Bert repo: ISSUE_REF\"},\n",
       " '70_5': {'created_at': '2021-08-03T13:27:47Z',\n",
       "  'author': 'pratikchhapolika',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '70',\n",
       "  'text': 'What if my sentences are well within 100 token in length. In that case does it make sense to change max_position_embeddings?\\nAdding 1 more similar question to, during model evaluation if I pass sentence to model and generate embeddings will it take sentence length as total tokens or 512 default? In that scenario if my sentence has 10 unique tokens then what does 512 stands for in hidden layers?'},\n",
       " '71': {'created_at': '2018-12-08T11:56:57Z',\n",
       "  'author': 'nischalhp',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hello,\\nWhen training the bert-base-multilingual-cased model for Question and Answering, I see that the tokens look like this :\\ntokens: [CLS] what is the ins ##ured _ name ? [SEP] versi ##cherung ##ss ##che ##in erg ##o hau ##srat ##versi ##cherung hr - sv 927 ##26 ##49 ##2 \\nAny idea why words are getting replaced with #?\\nHere is the command I am using :\\npython run_squad.py FLAG bert-base-multilingual-cased FLAG FLAG FLAG dataset_train.json FLAG dataset_predict.json FLAG 12 FLAG 3e-5 FLAG VERSION FLAG 400 FLAG 20 FLAG output_dir'},\n",
       " '71_0': {'created_at': '2018-12-08T12:51:01Z',\n",
       "  'author': 'ymcui',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '71',\n",
       "  'text': 'Because it uses WordPiece tokenization, and will introduce the # token.\\nCheck: URL'},\n",
       " '71_1': {'created_at': '2018-12-09T10:29:53Z',\n",
       "  'author': 'nischalhp',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '71',\n",
       "  'text': 'USER okay sweet, thank you. Will use the relevant one.'},\n",
       " '71_2': {'created_at': '2018-12-10T16:47:59Z',\n",
       "  'author': 'nischalhp',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '71',\n",
       "  'text': 'USER How do I change this ? or is not possible to do so?'},\n",
       " '71_3': {'created_at': '2018-12-11T00:16:03Z',\n",
       "  'author': 'ymcui',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '71',\n",
       "  'text': 'If you are training completely from scratch, then it will be possible to use your own tokenizer.\\nHowever, if you are fine-tuning on the existing pre-trained BERT models, I think it will not be possible to change the tokenizer, as the pre-trained BERT models are trained using WordPiece tokenizer.'},\n",
       " '71_4': {'created_at': '2018-12-11T10:33:23Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '71',\n",
       "  'text': 'USER is right.\\nSince the purpose of the present repo is to supply pre-trained model basically you are stuck with WordPiece tokenization.\\nIf you build a new model and train it from scratch, you can selected whatever tokenization you want :-)'},\n",
       " '71_5': {'created_at': '2018-12-11T13:32:37Z',\n",
       "  'author': 'nischalhp',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '71',\n",
       "  'text': 'USER USER - Yes, that is quite a problem and thanks for getting back. Evaluating building something on our own now'},\n",
       " '72': {'created_at': '2018-12-08T15:16:50Z',\n",
       "  'author': 'artemlos',\n",
       "  'author_location': 'SE',\n",
       "  'type': 'issue',\n",
       "  'text': 'Are there any example training files for run_classifier.py?'},\n",
       " '72_0': {'created_at': '2018-12-08T15:19:17Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '72',\n",
       "  'text': 'Please read the example section in the readme'},\n",
       " '73': {'created_at': '2018-12-09T07:06:52Z',\n",
       "  'author': 'friskit-china',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI found that you initilized all weights twice:\\nThe first one is in BertModel class:\\n URL \\nAnd the second one is in classes of each tasks such as in BertForSequenceClassification class:\\n URL \\nI think maybe you only need the second one?'},\n",
       " '73_0': {'created_at': '2018-12-09T07:49:11Z',\n",
       "  'author': 'Arjunsankarlal',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '73',\n",
       "  'text': 'I think it required for both the places. Because both of them can be used individually. As it is mentioned in the README.md file, the model can be loaded with 7 classes. In fact if you check BertForMaskedLM and BertForNextSentencePrediction classes it also has the weights initialised.\\nPlease correct me if I am wrong :)'},\n",
       " '74_0': {'created_at': '2018-12-10T09:32:58Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '74',\n",
       "  'text': 'As mentioned in ISSUE_REF , the maximum value of max_sequence_length is 512.'},\n",
       " '74_1': {'created_at': '2018-12-10T15:14:47Z',\n",
       "  'author': 'artemlos',\n",
       "  'author_location': 'SE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '74',\n",
       "  'text': 'USER thanks!'},\n",
       " '75': {'created_at': '2018-12-10T15:14:22Z',\n",
       "  'author': 'artemlos',\n",
       "  'author_location': 'SE',\n",
       "  'type': 'issue',\n",
       "  'text': \"I'm trying to figure out how the FLAG parameter works in run_classifier. Based on the source, it seems like it represents the number of words? Is that correct?\"},\n",
       " '75_0': {'created_at': '2018-12-10T16:53:46Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'max_seq_length specifies the maximum number of tokens of the input. The number of token is superior or equal to the number of words of an input.\\nFor example, the following sentence:\\nThe man hits the saxophone and demonstrates how to properly use the racquet.\\n\\nis tokenized as follows:\\nthe man hits the saxophone and demonstrates how to properly use the ra ##c ##quet .\\n\\nAnd depending on the task 2 to 3 additional special tokens ([CLS] and [SEP]) are added to the input to format it.'},\n",
       " '75_1': {'created_at': '2018-12-11T12:11:10Z',\n",
       "  'author': 'artemlos',\n",
       "  'author_location': 'SE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'USER thanks!'},\n",
       " '75_2': {'created_at': '2019-02-13T04:04:17Z',\n",
       "  'author': 'OswaldoBornemann',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'could we make it smaller?'},\n",
       " '75_3': {'created_at': '2019-04-23T04:48:45Z',\n",
       "  'author': 'echan00',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'So what if there are sentences where the maximum number of tokens is greater than max_seq_length?\\nDoes that mean extra tokens beyond max_seq_length will get cut off?'},\n",
       " '75_4': {'created_at': '2019-04-23T08:50:08Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'USER yes, just use smaller sentences\\n USER no automatic cut off but there is a warning from the tokenizer that your inputs are too long and the model will throw an error. You have to limit the size manually.'},\n",
       " '75_5': {'created_at': '2020-07-25T05:27:36Z',\n",
       "  'author': 'SaurabhBhatia0211',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'Hi All,\\nDoes that mean we cannot use BERT for classifying long documents. The documents having 5-6 Paragraphs and each paragraph having 10-15 mins with about 10-12 words in each line ?'},\n",
       " '75_6': {'created_at': '2020-09-15T16:56:33Z',\n",
       "  'author': 'bavaria95',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': 'USER \\nYou can try splitting a document to smaller chunks (e.g. paragraphs or even lines), computing embedding for each of those chunks, and average those vectors to get the document representation.'},\n",
       " '75_7': {'created_at': '2023-07-12T18:42:30Z',\n",
       "  'author': 'brando90',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '75',\n",
       "  'text': \"USER is this true?\\n\\nHuggingFace's Trainer API, including the SFTrainer, by default pads all sequences to the maximum length within the batch, not to the max_seq_length argument. The max_seq_length argument serves as a hard limit to the sequence length, truncating any examples that are longer than that. The API was designed this way because padding to the maximum sequence length in the batch improves computational efficiency.\\n\\n?\"},\n",
       " '76_0': {'created_at': '2019-08-15T14:25:04Z',\n",
       "  'author': 'kakeith',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '76',\n",
       "  'text': 'FILEPATH ! Thanks for clarifying it should be the path to the directory and not the filename itself.'},\n",
       " '76_2': {'created_at': '2020-06-22T12:47:16Z',\n",
       "  'author': 'gongel',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '76',\n",
       "  'text': 'Thanks'},\n",
       " '77': {'created_at': '2018-12-11T00:48:11Z',\n",
       "  'author': 'decodyng',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'I was trying to work through the toy tokenization example from the main README, and I hit an error on the step of loading in a pre-trained BERT tokenizer.\\n~/bert_transfer$ python3 test_tokenizer.py \\nTraceback (most recent call last):\\n File \"test_tokenizer.py\", line 10, in \\n tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\n File \"/ FILEPATH \", line 117, in from_pretrained\\n resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\\n File \"/ FILEPATH \", line 88, in cached_path\\n return get_from_cache(url_or_filename, cache_dir)\\n File \"/ FILEPATH \", line 169, in get_from_cache\\n os.makedirs(cache_dir, exist_ok=True)\\n File \"/ FILEPATH \", line 226, in makedirs\\n head, tail = path.split(name)\\n File \"/ FILEPATH \", line 103, in split\\n i = p.rfind(sep) + 1\\nAttributeError: \\'PosixPath\\' object has no attribute \\'rfind\\'\\n\\n~/bert_transfer$ python3 FLAG \\nPython VERSION \\n\\nExact usage in script:\\nfrom pytorch_pretrained_bert import BertTokenizer\\n\\ntest_sentence = \"When PyTorch first launched in early 2017, it quickly became a popular choice among AI researchers, who found it ideal for rapid experimentation due to its flexible, dynamic programming environment and user-friendly interface\"\\n\\nif __name__ == \"__main__\":\\n tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\n\\nI am curious if you\\'re able to replicate this error on python VERSION , since the repo states support for VERSION +.'},\n",
       " '77_0': {'created_at': '2018-12-11T10:28:47Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '77',\n",
       "  'text': \"Oh you are right, the file caching utilities requires python VERSION .\\nI don't intend to maintain a lot of backward compatibility in terms of Python versions (I already surrendered maintaining a Python 2 version) so I will bump up the requirements to python VERSION .\\nIf you are limited to python VERSION and find a way around this, don't hesitate to share your solution with a PR though.\"},\n",
       " '77_1': {'created_at': '2018-12-13T11:16:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '77',\n",
       "  'text': 'Ok USER fixed this issue with ISSUE_REF and we will be compatible with Python VERSION + again in the coming release (today probably). Thanks USER !'},\n",
       " '78': {'created_at': '2018-12-11T16:28:00Z',\n",
       "  'author': 'wahlforss',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'First I want to say thanks for setting up all this!\\nI am using BertForSequenceClassification and am wondering what the optimal way is to structure my sequences.\\nRight now my sequences are blog post which could be upwards to 400 words long.\\nWould it be better to split my blog posts in sentences and use the sentences as my sequences instead?\\nThanks!'},\n",
       " '79': {'created_at': '2018-12-11T20:58:38Z',\n",
       "  'author': 'wahlforss',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'How can you run the model without training the model? If we already trained a model with run_classifer?'},\n",
       " '79_0': {'created_at': '2018-12-13T00:17:07Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '79',\n",
       "  'text': 'It looks like USER is planning to illustrate this in the examples soon.\\nYou find some useful code to do what you want to do in ISSUE_REF'},\n",
       " '79_1': {'created_at': '2018-12-14T14:43:43Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '79',\n",
       "  'text': 'Hi this is now included in the new release VERSION and there are examples on how you can save and reload the models in the updated run_classifier, run_squad and run_swag.'},\n",
       " '80': {'created_at': '2018-12-13T17:58:02Z',\n",
       "  'author': 'asafamr',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I think logging.basicConfig should not be called inside library code\\ncheck out this SO thread\\n URL'},\n",
       " '81': {'created_at': '2018-12-14T03:24:58Z',\n",
       "  'author': 'SummmerSnow',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI downloaded pretrained model and vocabulary file, and wanted to test BertModel to get hidden states.\\nwhen this\\nencoded_layers, _ = model(tokens_tensor, segments_tensors) lines run, I got this error: Segmentation fault (core dumped).\\nI wonder what caused this error'},\n",
       " '81_0': {'created_at': '2018-12-14T14:44:28Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Hi, you need to give me more information (a screen copy of a full log of the error).'},\n",
       " '81_2': {'created_at': '2018-12-17T08:32:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': \"There is no special c function in our package, it's all python code.\\nMaybe you just don't have enough memory to load BERT?\\nOr some dependency is not well installed like pytorch (or apex if you are using it).\"},\n",
       " '81_3': {'created_at': '2018-12-22T14:26:31Z',\n",
       "  'author': 'SummmerSnow',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Thanks for your advice.\\nMaybe because of my pytorch version( VERSION ) I not not sure.\\nI download the source code instead of pip install and using VERSION version and run successfully.\\nThanks for your code and advice again~'},\n",
       " '81_4': {'created_at': '2019-05-07T02:35:02Z',\n",
       "  'author': 'wyx518',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'I also have this problem, and my torch version is VERSION . I have tried to download the source code instead of pip install but also failed.'},\n",
       " '81_5': {'created_at': '2020-06-10T09:27:25Z',\n",
       "  'author': 'zhaohongjie',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'I have the same question USER'},\n",
       " '81_6': {'created_at': '2020-06-16T03:16:22Z',\n",
       "  'author': 'FredHuang16',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Did you solve this ? USER'},\n",
       " '81_7': {'created_at': '2020-06-22T02:47:16Z',\n",
       "  'author': 'nv-quan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'USER did you solve this? I have the same question too.'},\n",
       " '81_8': {'created_at': '2020-06-24T09:35:12Z',\n",
       "  'author': 'naveenjafer',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Has someone solved this issue by any chance?'},\n",
       " '81_9': {'created_at': '2020-06-24T09:55:30Z',\n",
       "  'author': 'nv-quan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Has someone solved this issue by any chance?\\n\\nDo you have the same problem? You could try to debug it by import transformers and torch only, then call torch.nn.CrossEntropyLoss() to see if it results in Segmentation fault. I accidentally fixed this error by install more packages'},\n",
       " '81_10': {'created_at': '2020-06-24T13:21:54Z',\n",
       "  'author': 'End2EndAI',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Hello,\\nHad this error with CamembertForSequenceClassification.from_pretrained(), needed to update torch== VERSION and torchvision== VERSION'},\n",
       " '81_11': {'created_at': '2020-06-25T08:53:02Z',\n",
       "  'author': 'gabrer',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'I had the same issue while loading pretrained models.\\nUpdated to the last version of Pytorch ( VERSION ) and worked fine.'},\n",
       " '81_12': {'created_at': '2020-06-26T21:31:20Z',\n",
       "  'author': 'naveenjafer',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Yup, that worked guys! Thank you USER and USER'},\n",
       " '81_13': {'created_at': '2020-09-17T03:47:16Z',\n",
       "  'author': 'Apollo2Mars',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '81',\n",
       "  'text': 'Update by pip install torch== VERSION and the problem solved'},\n",
       " '82_0': {'created_at': '2018-12-15T20:45:34Z',\n",
       "  'author': 'danyaljj',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '82',\n",
       "  'text': 'The issue was, not properly loading the model file and moving it to GPU.'},\n",
       " '83_0': {'created_at': '2018-12-17T01:21:01Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '83',\n",
       "  'text': 'The metric used for evaluation of CoLA in the GLUE benchmark is not accuracy but the URL (see URL ).\\nIndeed authors report in URL VERSION for Matthews correlation with BERT-base.'},\n",
       " '83_1': {'created_at': '2018-12-17T06:41:06Z',\n",
       "  'author': 'pfecht',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '83',\n",
       "  'text': 'Makes sense, looks like I missed that point. Thank you.'},\n",
       " '84_0': {'created_at': '2018-12-17T08:29:55Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '84',\n",
       "  'text': 'Full log of the error?'},\n",
       " '84_1': {'created_at': '2018-12-22T14:31:24Z',\n",
       "  'author': 'SummmerSnow',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '84',\n",
       "  'text': 'This is caused by pytorch version.\\nI found , In VERSION version, _load_from_state_dict() only take 7 arguments, but In VERSION and this code, we need feed 8 arguments.\\nmodule._load_from_state_dict(\\n state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\\n\\nlocal_metadata should be removed in pytorch VERSION'},\n",
       " '84_2': {'created_at': '2019-01-07T11:46:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '84',\n",
       "  'text': 'Ok thanks USER !'},\n",
       " '85': {'created_at': '2018-12-18T03:13:11Z',\n",
       "  'author': 'AIRobotZhang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': 'When I run the examples for MRPC, my program was always killed becaused of big memory occupied. Anyone encounter with this issue?'},\n",
       " '85_0': {'created_at': '2018-12-18T08:04:33Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '85',\n",
       "  'text': 'You should lower the batch size probably'},\n",
       " '86': {'created_at': '2018-12-18T10:36:23Z',\n",
       "  'author': 'patrick-s-h-lewis',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi team, love the work.\\nJust a feature suggestion: when running on GPU (presumably the CPU too), BERT will break when you try to run on sentences longer than 512 tokens (on bert-base).\\nThis is because the position embedding matrix size is only 512 (or whatever else it is for the other bert models)\\n FILEPATH 't allow you tokenize a sentence longer than the number of positional embeddings, so that you get a better error message than a bit scary (uncatchable) cuda error.\"},\n",
       " '86_0': {'created_at': '2018-12-18T12:10:50Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '86',\n",
       "  'text': 'Could do that indeed Patrick.\\nIn particular when the tokenizer is loaded from one of Google pre-trained model.\\nIf you have a working implementation feel free to do a PR.\\nOtherwise I will have a look at that when I start working on the next release.'},\n",
       " '86_1': {'created_at': '2018-12-18T13:47:48Z',\n",
       "  'author': 'patrick-s-h-lewis',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '86',\n",
       "  'text': 'Happy to do a PR :) will do today or tomorrow'},\n",
       " '87': {'created_at': '2018-12-18T13:21:51Z',\n",
       "  'author': 'jaderabbit',\n",
       "  'author_location': 'ZA',\n",
       "  'type': 'issue',\n",
       "  'text': \"In reference to following tweet:\\nWould it be possible to do a benchmark on the speed of prediction? I was working with the tensorflow version of BERT, but it uses the new Estimators and I'm struggling to find a straight-forward way to benchmark it since it all gets hidden in layers of computation graph. I'd imagine pytorch being more forgiving in this regard.\"},\n",
       " '87_0': {'created_at': '2018-12-18T13:28:09Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'Do you have a dataset in mind for the benchmark?\\nWe can do a simple benchmark by timing the duration of evaluation on the SQuAD dev set for example.'},\n",
       " '87_1': {'created_at': '2018-12-18T15:05:44Z',\n",
       "  'author': 'jaderabbit',\n",
       "  'author_location': 'ZA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'Yes, that would be perfect! Ideally, it would exclude loading and setting up the model (something that the tf implementation literally does not allow for :P)'},\n",
       " '87_2': {'created_at': '2018-12-19T12:11:03Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'Hi Jade,\\nI did some benchmarking on a V100 GPU. You can check the script I used on the benchmark branch (mostly added timing to run_squad).\\nHere are the results:\\n\\nmax_seq_length\\nfp32\\nfp16\\n\\n384\\n140\\n352\\n\\n256\\n230\\n751\\n\\n128\\n488\\n1600\\n\\n64\\n1030\\n3663\\n\\nI will give a look on an older K80 (without fp16 support) when I have time.'},\n",
       " '87_3': {'created_at': '2018-12-19T14:30:40Z',\n",
       "  'author': 'jaderabbit',\n",
       "  'author_location': 'ZA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"This is fantastic! Thank you so so so so much!\\nIf you get a chance to do the K80, that would be brilliant. I'll try run it when I get time. Currently doing a cost versus speed comparison just to get a feel.\"},\n",
       " '87_4': {'created_at': '2018-12-19T14:35:13Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"You can run it like this for fp32 (just remove FLAG ):\\npython run_squad.py \\\\\\n FLAG bert-base-uncased \\\\\\n FLAG \\\\\\n FLAG \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG 128 \\\\\\n FLAG 3e-5 \\\\\\n FLAG VERSION \\\\\\n FLAG 384 \\\\\\n FLAG 128 \\\\\\n -- FILEPATH /\\nAnd like this for fp16 (add FLAG ):\\npython run_squad.py \\\\\\n FLAG bert-base-uncased \\\\\\n FLAG \\\\\\n FLAG \\\\\\n FLAG \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG 128 \\\\\\n FLAG 3e-5 \\\\\\n FLAG VERSION \\\\\\n FLAG 384 \\\\\\n FLAG 128 \\\\\\n -- FILEPATH /\\nAdjust predict_batch_size 128 to fill your GPU around 50% at least and adjust FLAG 384 to test with various sequence lengths. For small sequences (under 64 tokens) we should desactivate the windowing (related to doc_stride). I didn't take time to do that so the dataset reading didn't work (hence the absence of datapoint).\"},\n",
       " '87_5': {'created_at': '2018-12-19T14:53:31Z',\n",
       "  'author': 'jaderabbit',\n",
       "  'author_location': 'ZA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"Fantastic. Tomorrow I'm going to run it for some smaller max sequence lengths (useful for my use case) and on some other GPUS: The Tesla M60 and then the K80\"},\n",
       " '87_6': {'created_at': '2019-01-02T12:41:02Z',\n",
       "  'author': 'jaderabbit',\n",
       "  'author_location': 'ZA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"Managed to replicate your results on the V100. :)\\nAlso, I've done the experiments below for sequences of length 64 on different GPUS. Will do the other sequence lengths when I get a chance.\\n\\nGPU\\nmax_seq_length\\nfp32\\nfp16\\n\\nTesla M60\\n64\\n210\\n FILEPATH \\n\\nTesla K80\\n64\\n143\\n FILEPATH\"},\n",
       " '87_7': {'created_at': '2019-01-07T11:19:41Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'USER USER Thank you for the experiments.\\nI think these results deserves more visibility, maybe a dedicated markdown page or a section in the README.md?'},\n",
       " '87_8': {'created_at': '2019-01-07T11:48:34Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'Your are right Gregory.\\nThe readme is starting to be too big in my opinion.\\n FILEPATH (feel free to start a PR if you have experience in these kind of stuff).'},\n",
       " '87_9': {'created_at': '2019-01-07T15:24:13Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"I'm more or less new to sphinx but I would be happy to work on it with you.\"},\n",
       " '87_10': {'created_at': '2019-01-07T21:28:33Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"Sure, if you want help that could definitely speed up the process.\\nThe first step would be to create a new branch to work on with a docfolder and then generate the doc in the folder using sphinx.\\nGood introductions to sphinx and readthedoc are here: URL \\nand here: URL \\nWe will need to add some dependencies for the but we should strive to keep it as light as possible.\\nHere is an example of repo I've worked on recently (still a draft but the doc is functional) URL\"},\n",
       " '87_11': {'created_at': '2019-01-09T10:55:06Z',\n",
       "  'author': 'apurvaasf',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'Hi USER ,\\nI am looking to deploy a pre-trained squad-bert model to make predictions in real-time.\\nRight now when I run:\\npython run_squad.py \\\\ FLAG bert-base-uncased \\\\ FLAG \\\\ FLAG \\\\ FLAG $ FILEPATH \\\\ FLAG $ FILEPATH \\\\ FLAG 128 \\\\ FLAG 3e-5 \\\\ FLAG VERSION \\\\ FLAG 384 \\\\ FLAG 128 \\\\ -- FILEPATH /\\nit takes 22 seconds to generate the prediction. Is there a way to reduce the amount off time taken to less than a second?\\nThe \"test.json\" has one context and 1 question on the same. It looks like this:\\n{ \"data\": [ { \"title\": \"Arjun\", \"paragraphs\": [ { \"context\": \"Arjun died in 1920. The American Football Club (AFC) celebrated this death. Arjun now haunts NFC. He used to love playing football. But nobody liked him.\", \"qas\": [ { \"question\": \"When did Arjun die?\", \"id\": \" COMMIT \" } ] } ] } ] }\\nPlease help me with this. I switched to using the PyTorch implementation hoping that getting a saved model and making predictions using the saved model will be easier in PyTorch.'},\n",
       " '87_12': {'created_at': '2019-01-11T06:52:55Z',\n",
       "  'author': 'jaderabbit',\n",
       "  'author_location': 'ZA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"USER Might be worth opening another ticket since that's slightly different to this. It shouldn't be too hard to write your own code for deployment. The trick is to make sure it does all the loading once, and just calls predict each time you need a prediction.\"},\n",
       " '87_13': {'created_at': '2019-06-25T20:04:14Z',\n",
       "  'author': 'hamediramin',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': 'Hi USER and thanks for the amazing implementation. I wonder what is the inference speed with a 512 batch size. It seems to take a lot of time to convert to GPU (1000msec for a batch size of 32) FILEPATH .'},\n",
       " '87_14': {'created_at': '2019-08-06T01:39:01Z',\n",
       "  'author': 'mitsuix',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"Hi USER and thanks for the amazing implementation. I wonder what is the inference speed with a 512 batch size. It seems to take a lot of time to convert to GPU (1000msec for a batch size of 32) FILEPATH .\\n\\nHave you found any solutions? I've met the same problem.\\nThe inference time is fast, but takes a lot of time to convert to GPU and convert the result to CPU for post-processing.\"},\n",
       " '87_15': {'created_at': '2020-06-03T09:05:54Z',\n",
       "  'author': 'CaesarWWK',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '87',\n",
       "  'text': \"Hi USER and thanks for the amazing implementation. I wonder what is the inference speed with a 512 batch size. It seems to take a lot of time to convert to GPU (1000msec for a batch size of 32) FILEPATH .\\n\\nHave you found any solutions? I've met the same problem.\\nThe inference time is fast, but takes a lot of time to convert to GPU and convert the result to CPU for post-processing.\\n\\nalbanD commented on 25 Mar\\nHi,\\nWe use github issues only for bugs or feature requests.\\nPlease use the forum to ask questions: URL as mentionned in the template you used.\\nNote that in your case, you are most likely missing torch.cuda.syncrhonize() when timing your GPU code which makes the copy look much slower than it is because it has to wait for the rest of the work to be done.\\n\\n#Pytorch#35292\"},\n",
       " '88': {'created_at': '2018-12-19T01:57:22Z',\n",
       "  'author': 'jwang-lp',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I used BERT in a very simple sentence classification task:\\nin __init__ I have\\nself.bert = BertModel(config)\\nself.cnn_classifier = CNNClassifier(self.config.hidden_size, intent_cls_num)\\nand in forward it's just\\nencoded_layers, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\\nconfidence_score = self.cnn_classifier(encoded_layers)\\nmasked_lm_loss = loss_fct(confidence_score, ground_truth_labels)\\nThis code works perfectly when I use VERSION version, but in VERSION , it:\\n\\nalways predicting the most common class when have a large training set\\ncannot even learn a dataset with only 4 samples (fed in as one batch); can learn a single sample though\\n\\nWhy are these problems happening in VERSION ? The only change in my code is that I changed weight_decay_rate to weight_decay...\"},\n",
       " '88_0': {'created_at': '2018-12-19T15:32:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '88',\n",
       "  'text': \"I don't know...\\nIf you can open-source a self contained example with data and code I can try to give it a deeper look.\\nAre you using apex? That's the main change in VERSION .\"},\n",
       " '88_1': {'created_at': '2018-12-20T00:20:48Z',\n",
       "  'author': 'jwang-lp',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '88',\n",
       "  'text': \"Hi Thomas! I've found the problem. I think It's because you modified your from_pretrained function and I'm still using a part of the from_pretrained function from version VERSION , which resulted in some compatibility issues. Thanks!\"},\n",
       " '89_0': {'created_at': '2019-01-07T12:08:47Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '89',\n",
       "  'text': 'Hi USER , thanks for the feedback. Indeed the run_squad example was not updated for cased models. I fixed that in commits COMMIT and COMMIT .\\nPlease re-open the issue if your problem is not fixed (and maybe summarize it in an updated version).'},\n",
       " '89_1': {'created_at': '2020-10-03T12:11:21Z',\n",
       "  'author': 'empty-id',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '89',\n",
       "  'text': 'It seems that default do_lower_case is still True.'},\n",
       " '91': {'created_at': '2018-12-20T07:27:11Z',\n",
       "  'author': 'AIRobotZhang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': \"I run the classification task with BERT pretrianed model, but while it's much lower than other methods on OMD dataset, which has 2 labels. The final accuracy result is only 62% on binary classification task!\"},\n",
       " '91_0': {'created_at': '2018-12-20T09:12:46Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '91',\n",
       "  'text': 'We need more informations on the parameters you use to run this training in order to understand what might be wrong.'},\n",
       " '91_1': {'created_at': '2018-12-21T03:11:52Z',\n",
       "  'author': 'AIRobotZhang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '91',\n",
       "  'text': 'We need more informations on the parameters you use to run this training in order to understand what might be wrong.\\n\\nTHANK YOU! Because of limited mermory, the batch_size is 8 and epoch is 6, and the content is short, so set the max_length is 50, other parameters are default.'},\n",
       " '91_2': {'created_at': '2019-01-07T12:11:22Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '91',\n",
       "  'text': 'Try various values for the hyper-parameters and at least 10 different seed values.\\nLimited memory should not be a limitation when you use gradient accumulation as indicated in the readme here (see also how it is used in all the examples like run_classifier, run_squad...)'},\n",
       " '92': {'created_at': '2018-12-20T13:52:44Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi!\\nThere is a problem with the way model are saved and loaded. The following code should crash and doesn't:\\nimport torch\\nfrom pytorch_pretrained_bert import BertForSequenceClassification\\n\\nmodel_fn = 'model.bin'\\nbert_model = 'bert-base-multilingual-cased'\\nmodel = BertForSequenceClassification.from_pretrained(bert_model, num_labels = 16)\\n\\nmodel_to_save = model.module if hasattr(model, 'module') else model\\ntorch.save(model_to_save.state_dict(), model_fn)\\nprint(model_to_save.num_labels)\\n\\nmodel_state_dict = torch.load(model_fn)\\nloaded_model = BertForSequenceClassification.from_pretrained(bert_model, state_dict = model_state_dict)\\nprint(loaded_model.num_labels)\\nThis code prints:\\n16\\n2\\n\\nThe code should raise an exception when trying to load the weights of the task specific linear layer. I'm guessing that the problem comes from PreTrainedBertModel.from_pretrained.\\nI would be happy to submit a PR fixing this problem but I'm not used to work with the PyTorch loading mechanisms. USER could you give me some guidance?\\nCheers!\"},\n",
       " '92_0': {'created_at': '2018-12-20T14:34:01Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '92',\n",
       "  'text': 'Ok I managed to find the problem. It comes from:\\n URL \\nWhen trying to load classifier.weight and classifier.bias, the following line gets added to error_msgs:\\nsize mismatch for classifier.weight: copying a param with shape torch.Size([16, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\\nsize mismatch for classifier.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([2]).\\n\\nFirst, I think that we should add a check of error_msgs to from_pretrained. I don\\'t really know if there is any other way than printing an error message and existing the program since the default behavior (keeping the classifier layer randomly initialized) can be frustrating for the user (I speak from experience ^^).\\nTo fix this, we should probably fetch the number of labels of the saved model and use it to instanciate the model being created before loading the saved weights. Unfortunately I don\\'t really know how to do that, any idea?\\nAnother possible \"fix\" would be to force the user to give a num_labels argument when loading a pretrained classification model with the following code in BertForSequenceClassification:\\n USER \\n def from_pretrained(cls, *args, **kwargs):\\n if \\'num_labels\\' not in kwargs:\\n raise ValueError(\\'num_labels should be given when loading a pre-trained classification model\\')\\n return super().from_pretrained(*args, **kwargs)\\nAnd even with this code, we are not able to check that the num_labels value is the same as the saved model. I don\\'t really like the idea of forcing the user to give an information that the checkpoint already contains.'},\n",
       " '92_2': {'created_at': '2019-01-03T07:51:40Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '92',\n",
       "  'text': 'As mentioned in my previous posts, I think that the library should either fetch the number of labels from the save file or force the user to provide a num_labels argument.\\nWhile what you are proposing fixes my problem I would like to prevent this problem for other users in the future by patching the library code.'},\n",
       " '92_3': {'created_at': '2019-01-07T11:20:43Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '92',\n",
       "  'text': \"I see thanks USER . Indeed not using the error_msg is bad practice, let's raise these errors.\\nRegarding fetching the number of labels, I understand your point but it will probably add too much custom logic in the library for the moment so let's go for your simple solution of setting the number of labels as mandatory for now (should have done that since the beginning).\"},\n",
       " '92_4': {'created_at': '2021-01-12T12:32:48Z',\n",
       "  'author': 'ugm2',\n",
       "  'author_location': 'ES',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '92',\n",
       "  'text': \"Hi everyone!\\nI had to come here to know that I had to include num_labels when loading the model because the error was misleading.\\nAlso, I didn't know how many labels there were so I had to guess.\\nThe model I was trying to load:\\nbiobert-base-cased- VERSION -mnli\"},\n",
       " '92_5': {'created_at': '2021-07-30T10:30:03Z',\n",
       "  'author': 'kaankork',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '92',\n",
       "  'text': \"I'm also facing a similar problem using the same model as USER - biobert-base-cased- VERSION -mnli\\nIn my example I know the exact num_labels and provide it as an argument while loading the model.\\nHow can I solve this?\\nRuntimeError: Error(s) in loading state_dict for BertForSequenceClassification:\\n size mismatch for classifier.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\\n size mismatch for classifier.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([10]).\"},\n",
       " '92_6': {'created_at': '2021-08-04T13:25:27Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '92',\n",
       "  'text': \"With the latest transformers versions, you can use the recently introduced (#12664) ignore_mismatched_sizes=True parameter for from_pretrained method in order to specify that you'd rather drop the layers that have incompatible shapes rather than raise a RuntimeError.\"},\n",
       " '93': {'created_at': '2018-12-20T14:00:03Z',\n",
       "  'author': 'rxy1212',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"When I run this code model = BertModel.from_pretrained('bert-base-uncased') , it would download a big file and sometimes that's very slow. Now I have download the model from URL So, It's possible to avoid download the pretrained model when I use pytorch-pretrained-BERT at the first time?\"},\n",
       " '93_0': {'created_at': '2018-12-20T14:08:10Z',\n",
       "  'author': 'rxy1212',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '93',\n",
       "  'text': 'I just find the way.'},\n",
       " '93_1': {'created_at': '2018-12-21T01:17:29Z',\n",
       "  'author': 'makkunda',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '93',\n",
       "  'text': 'USER could you explain the method used'},\n",
       " '94_0': {'created_at': '2019-07-12T23:30:00Z',\n",
       "  'author': 'peterflat',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '94',\n",
       "  'text': 'USER What was the conclusion of this issue?'},\n",
       " '95_0': {'created_at': '2018-12-20T19:59:40Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '95',\n",
       "  'text': 'Judging from the error message, I would say that the error is caused by the following line: URL \\nApparently, the proper way to save a model is the following one:\\n URL \\nIs this what you are doing?'},\n",
       " '95_1': {'created_at': '2018-12-20T21:44:01Z',\n",
       "  'author': 'ni40in',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '95',\n",
       "  'text': 'hi USER i see that model is being saved the same way in squad.py:\\n URL \\nso the problem must be elsewhere'},\n",
       " '95_3': {'created_at': '2019-01-07T12:14:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '95',\n",
       "  'text': 'Hi, here the problem is not with the saving of the model but the loading.\\nYou should just use\\nmodel.load_state_dict(torch.load(model_state_dict))\\n\\nand not\\nmodel.bert.load_state_dict(torch.load(model_state_dict))\\n\\nAlternatively, here is an example on how to save and then load a model using from_pretrained:\\n URL'},\n",
       " '96': {'created_at': '2018-12-20T18:46:14Z',\n",
       "  'author': 'Ashish-Gupta03',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"I'm not able to work with FP16 for pytorch BERT code. Particularly for BertForSequenceClassification, which I tried and got the issue\\nRuntime error: Expected scalar type object Half but got scalar type Float for argument ISSUE_REF target\\nwhen I enabled fp16.\\nAlso when using\\nlogits = logits.half() labels = labels.half()\\nthen the epoch time also increased.\\nOriginally posted by USER in URL\"},\n",
       " '97': {'created_at': '2018-12-20T18:46:30Z',\n",
       "  'author': 'Ashish-Gupta03',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"I'm not able to work with FP16 for pytorch BERT code. Particularly for BertForSequenceClassification, which I tried and got the issue\\nRuntime error: Expected scalar type object Half but got scalar type Float for argument ISSUE_REF target\\nwhen I enabled fp16.\\nAlso when using\\nlogits = logits.half() labels = labels.half()\\nthen the epoch time also increased.\\nThe training time without fp16 was VERSION hrs per epoch after doing logits.half() and labels.half() the runtime per epoch shot up to 8hrs.\"},\n",
       " '97_0': {'created_at': '2018-12-28T09:23:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '97',\n",
       "  'text': 'Which kind of GPU are you using? fp16 only works on recent GPU (better with Tesla and Volta series).'},\n",
       " '97_1': {'created_at': '2018-12-28T09:37:03Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '97',\n",
       "  'text': 'I experienced a similar issue with CUDA VERSION . Using VERSION solved this for me.'},\n",
       " '97_2': {'created_at': '2019-01-07T12:18:36Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '97',\n",
       "  'text': 'Yes, CUDA 10 is recommended for using fp16 with good performances.'},\n",
       " '98': {'created_at': '2018-12-21T08:29:40Z',\n",
       "  'author': 'mjc14',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'hi ,\\nthere is a bug in init_bert_weights().\\nthe BERTLayerNorm has twice init, the first init is in the BERTLayerNorm module init(). the second init in init_bert_weights().\\nif you want to get pre-training model that is not from google model, the second init will lead to bad convergence in my experiment gamma is variance , beta is mean, there are usually 1 and 0. the second init change it.\\nfirst:\\nself.gamma = nn.Parameter(torch.ones(config.hidden_size))\\nself.beta = nn.Parameter(torch.zeros(config.hidden_size))\\nsecond:\\nelif isinstance(module, BERTLayerNorm):\\nmodule.beta.data.normal_(mean= VERSION , std=config.initializer_range)\\nmodule.gamma.data.normal_(mean= VERSION , std=config.initializer_range)'},\n",
       " '98_0': {'created_at': '2019-01-07T12:18:49Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '98',\n",
       "  'text': 'Fixed, thanks'},\n",
       " '99': {'created_at': '2018-12-22T12:24:24Z',\n",
       "  'author': 'wlhgtc',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Use the same sentence in your Usage Section:\\n# Tokenized input\\ntext = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\\ntokenized_text = tokenizer.tokenize(text)\\n\\n# Mask a token that we will try to predict back with INLINECODE \\nmasked_index = 6\\ntokenized_text[masked_index] = \\'[MASK]\\'\\n\\nQ1.\\nWhen we use this sentence as training data,according to your code\\n if masked_lm_labels is not None:\\n loss_fct = CrossEntropyLoss(ignore_index=-1)\\n masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\\n return masked_lm_loss\\n\\nseem the loss is a sum of all word in this sentence, not the single word \"henson\", am I right? But in my opinion, we only need to calculate the masked word\\'s loss, not the whole sentence?\\nQ2.\\nIt\\'s also a question about masked, \"chooses 15% of tokens at random\" in the paper, I don\\'t know how to understand it... For each word, a probability of 15% to be masked or just 15% of the sentence is masked?\\nHope you could help me fix them.\\nBy the way, the notes in line 731 in: FILEPATH : if masked_lm_labels is not None, missed a word \"not\".'},\n",
       " '99_0': {'created_at': '2018-12-23T10:55:55Z',\n",
       "  'author': 'wlhgtc',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '99',\n",
       "  'text': 'USER Seem some conflict with the original BERT in tf.\\nThe code in tf is as follows:\\ndef gather_indexes(sequence_tensor, positions):\\n \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\\n...\\ninput_tensor = gather_indexes(input_tensor, positions)\\n\\nOr dose you mean that we could set all words that are not masked(random pick from the sentence) and the padding(add to reach max_length) to \"-1\"(in order to ignore)?'},\n",
       " '99_1': {'created_at': '2018-12-28T09:34:34Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '99',\n",
       "  'text': 'Q1:[...] But in my opinion, we only need to calculate the masked word\\'s loss, not the whole sentence?\\n\\nIt\\'s exactly what is done in the current implementation. The labels of not masked tokens are set to -1 and the loss function ignores those tokens by setting ignore_index=-1 (see documentation)\\n\\nQ2.\\nIt\\'s also a question about masked, \"chooses 15% of tokens at random\" in the paper, I don\\'t know how to understand it... For each word, a probability of 15% to be masked or just 15% of the sentence is masked?\\n\\nEach token has a probability of 15% of getting masked. You might wanna checkout this code to get a better understanding'},\n",
       " '99_2': {'created_at': '2018-12-28T13:13:54Z',\n",
       "  'author': 'wlhgtc',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '99',\n",
       "  'text': 'So nice to see your reply, it do fix my problem,4K.'},\n",
       " '99_3': {'created_at': '2018-12-30T02:36:46Z',\n",
       "  'author': 'wlhgtc',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '99',\n",
       "  'text': 'USER I want to rebuild BERT on a single GPU, still some problems. May I know your email address ?'},\n",
       " '99_4': {'created_at': '2018-12-30T12:40:56Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '99',\n",
       "  'text': 'malte.pietsch [at] deepset.ai\\nBut if you have issues that are of interest for others, please use github.'},\n",
       " '99_5': {'created_at': '2019-01-07T12:19:19Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '99',\n",
       "  'text': 'Thanks USER !'},\n",
       " '100': {'created_at': '2018-12-24T12:51:49Z',\n",
       "  'author': 'valsworthen',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hello,\\nI have a question regarding the BertForQuestionAnswering implementation. If I am not mistaken, for this model the sequence should be of the form Question tokens [SEP] Passage tokens. Therefore, the embedded representation computed by BertModel returns the states of both the question and the passage (a tensor of length passage + question + 1).\\nIf I am not mistaken, the span logits are then calculated for the whole sequence, i.e. they can be calculated for the question even if the answer is always in the passage (see the model code and the squad script). I wonder if this behavior is really desirable. Doesn't it confuse the model?\\nThank you for your work!\"},\n",
       " '100_0': {'created_at': '2018-12-28T09:20:49Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '100',\n",
       "  'text': 'This is the original behavior from the TF implementation.\\nThe predictions are filtered afterward (in write_predictions) so this is probably not a big issue.\\nMaybe try with another behavior and see if it improve upon the results?'},\n",
       " '101': {'created_at': '2018-12-26T02:05:34Z',\n",
       "  'author': 'SparkJiao',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"Recently I'm modifying the run_squad.py to run on CoQA. In the implementation of TensorFlow from Google, they use the probability on the first token of a context segment, where is the location of to as the that of the question is unanswerable. So I try to modified the run_squad.py in your implementation as this. But when I looked at the predictions, I have found that many questions answers are the first word of the context not the first token, , so I wanna know if your implementation have removed the hidden state of start token and end token? Or there may be other problems ? Thank you a lot!\"},\n",
       " '101_0': {'created_at': '2018-12-26T02:48:04Z',\n",
       "  'author': 'SparkJiao',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '101',\n",
       "  'text': \"I'm sorry that I have found a bug in my code. I have invalidly called a attribute of the InputFeature but it have run successfully. Now I have fixed it and re-run it. If I have more questions I will reopen this. Sorry to bother you!\"},\n",
       " '102': {'created_at': '2018-12-27T06:48:23Z',\n",
       "  'author': 'nihalnayak',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'I am trying out the extract_features.py example program. I noticed that a sentence gets split into tokens and the embeddings are generated. For example, if you had the sentence Definitely not, and the corresponding workpieces can be [Def, ##in, ##ite, ##ly, not]. It then generates the embeddings for these tokens.\\nMy question is how do I train an NER system on CoNLL dataset?\\nI want to extract embeddings for original tokens for training an NER with a neural architecture. If you have come across any resource that gives a clear explanation on how to carry this out, post it here.'},\n",
       " '102_0': {'created_at': '2018-12-28T09:17:16Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '102',\n",
       "  'text': \"Hi, you should read the discussion in ISSUE_REF . I left this issue open for reference on these questions.\\nDon't hesitate to participate there.\"},\n",
       " '103': {'created_at': '2018-12-27T23:17:42Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nAccording to PR ISSUE_REF , we should be able to achieve a 3-4 x speed up for both bert-base and bert-large. However, I can only achieve 2x speed up with bert-base. My docker image uses CUDA9.0 while the discussion in the PR ISSUE_REF is based on CUDA10.0... I am wondering if that makes the difference....\\nThanks'},\n",
       " '103_0': {'created_at': '2018-12-28T09:16:00Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '103',\n",
       "  'text': 'Maybe. You can try with pytorch docker image dockerhub VERSION -cuda10.0-cudnn7 to debug, as we did in the discussion in PR ISSUE_REF .'},\n",
       " '103_1': {'created_at': '2018-12-29T10:42:40Z',\n",
       "  'author': 'llidev',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '103',\n",
       "  'text': 'Just verified that CUDA10.0 makes 4x speedup. It should be better to include this in the main document.'},\n",
       " '103_2': {'created_at': '2019-07-22T23:10:34Z',\n",
       "  'author': 'Oxi84',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '103',\n",
       "  'text': 'What GPU do you run ... and how do you increase such a speedup? Is this possible with gtx 1080?\\nThanks'},\n",
       " '104_0': {'created_at': '2019-01-07T12:21:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': \"Hi USER , apex is a GPU specific extension.\\nWhat kind of use-case do you have in which you have apex installed but no GPU (also fp16 doesn't work on CPU, it's not supported on PyTorch currently)?\"},\n",
       " '104_1': {'created_at': '2019-01-07T13:04:52Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'The two cases I came across this:\\n\\ntesting if some code works for both GPU and CPU (on a GPU machine with apex installed)\\n FILEPATH \"toy GPU\" with only 2 GB RAM and therefore I am usually using the CPUs here.\\n\\nI agree that these are edge cases, but I thought the flag FLAG is intended for exactly such cases?'},\n",
       " '104_2': {'created_at': '2019-01-10T00:33:47Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': \"I see. It's a bit tricky because apex is loaded by default when it can be found and this loading is deep inside the library it-self, not the examples (here). I don't think it's worth it to add specific logic inside the loading of the library to handle such a case.\\nI guess the easiest solution in your case is to have two python environments (with conda or virtualenv) and switch to the one in which apex is not installed when don't want to use GPU.\\nFeel free to re-open the issue if this doesn't solve your problem.\"},\n",
       " '104_3': {'created_at': '2019-01-11T06:49:25Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': \"Sure, then it's not worth the effort.\"},\n",
       " '104_4': {'created_at': '2019-01-11T10:44:37Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'USER a solution would be to check torch.cuda.is_available() and then we can disable apex by using CUDA_VISIBLE_DEVICES=-1'},\n",
       " '104_6': {'created_at': '2019-03-29T18:17:01Z',\n",
       "  'author': 'LamDang',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'Hello USER ,\\nWhat do you mean by \"disable apex by CUDA_VISIBLE_DEVICES=-1\" ? I tried to do that but the import still work at this line'},\n",
       " '104_7': {'created_at': '2019-03-29T22:27:38Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'USER You can set the env CUDA_VISIBLE_DEVICES=-1 to disable cuda in pytorch (ex when you launch your script in bash CUDA_VISIBLE_DEVICES=-1 python script.py), and then wrap the import apex with a if torch.cuda.is_available() in the script.'},\n",
       " '104_8': {'created_at': '2019-04-01T03:18:59Z',\n",
       "  'author': 'vickyliin',\n",
       "  'author_location': 'TW',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'Hi all, I came across this issue when my GPU memory was fully loaded and had to make some inference at the same time. For this kind of temporary need, the simplest solution for me is just to touch apex.py before the run and remove it afterwards.'},\n",
       " '104_9': {'created_at': '2019-04-09T07:07:18Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'Re-opening this to remember to wrap the apex import with a if torch.cuda.is_available() in the next release as advocated by USER'},\n",
       " '104_10': {'created_at': '2019-04-10T20:12:01Z',\n",
       "  'author': 'LamDang',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'Hello, I pushed a pull request here to solve this issue upstream ISSUE_REF \\nUpdate: it is merged into apex'},\n",
       " '104_11': {'created_at': '2019-04-15T08:19:28Z',\n",
       "  'author': 'jingshu-liu',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': \"Re-opening this to remember to wrap the apex import with a if torch.cuda.is_available() in the next release as advocated by USER \\n\\nYes please, I also struggle with Apex in CPU mode, i have wrapped Bertmode in my object and when I tried to load the pretrained GPU model with torch.load(model, map_location='cpu') , it shows 'no module named apex' but if I install apex, I get no cuda error(I'm on a CPU machine in inference phase )\"},\n",
       " '104_12': {'created_at': '2019-04-15T08:21:31Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': \"Well it should be solved in apex now. What is the exact error message you have ?\\nBy the way, not using apex is also fine, don't worry about it if you don't need t.\"},\n",
       " '104_13': {'created_at': '2019-04-15T08:28:25Z',\n",
       "  'author': 'jingshu-liu',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': \"I got\\nmodel = torch.load(model_file, map_location='cpu')\\nresult = unpickler.load() ModuleNotFoundError: No module named 'apex'\\nmodel_file is a pretrained object with GPU with a bertmodel field , but I want to unpickle it in CPU mode\"},\n",
       " '104_14': {'created_at': '2019-04-15T08:34:42Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '104',\n",
       "  'text': 'Try to use pytorch recommended serialization practice ( FILEPATH ):\\n URL'},\n",
       " '105': {'created_at': '2018-12-28T16:32:05Z',\n",
       "  'author': 'hguan6',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I am using a server with Ubuntu VERSION and 4 TITAN X GPUs. The server runs the base model with no problems. But it cannot run the large model with 32-bit float point, so I enabled fp16, and the server went down.\\n(When I successfully ran the base model, it consumes 8G GPU memory for each of the 4 GPUS. )'},\n",
       " '105_0': {'created_at': '2019-01-07T11:03:37Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '105',\n",
       "  'text': 'Could you give more informations such as the command that you are using to run the model and the batch size that you are using? Have you tried reducing it?'},\n",
       " '105_1': {'created_at': '2019-01-07T12:24:34Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '105',\n",
       "  'text': 'Hi, USER try to adjust the batch size and use gradient accumulation (see this section in the readme and the run_squad and run_classifier examples) if needed.'},\n",
       " '106': {'created_at': '2018-12-30T11:25:55Z',\n",
       "  'author': 'zhaoguangxiang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': 'What is the command to reproduce the results of squad2.0 reported in the BERT.\\nThanks~'},\n",
       " '106_0': {'created_at': '2018-12-30T12:36:44Z',\n",
       "  'author': 'elyase',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '106',\n",
       "  'text': 'FILEPATH :\\n ISSUE_REF'},\n",
       " '106_1': {'created_at': '2019-01-14T09:03:50Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '106',\n",
       "  'text': 'This is now on master'},\n",
       " '107': {'created_at': '2018-12-30T11:33:29Z',\n",
       "  'author': 'zhaoguangxiang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': 'But some questions of train- VERSION .json are unanswerable.'},\n",
       " '108': {'created_at': '2018-12-30T13:08:53Z',\n",
       "  'author': 'l126t',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I prepare two sentences for mlm predict the mask part:\"Tom cant run fast. He [mask] his back a few years ago.\" The result of model (uncased base) is \\'got\\'. That is meaningless. Obviously ,\"hurt\" is better.\\nI wander how to make mlm to use the information of adjacent sentences.'},\n",
       " '108_0': {'created_at': '2019-01-07T11:01:20Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '108',\n",
       "  'text': 'The model is already using adjacent sentences to make its predictions, it just happens to be wrong in your case.\\nIf you would like to make it choose from a specific list of words, you could use the code that I mentionned in ISSUE_REF .'},\n",
       " '108_1': {'created_at': '2019-01-07T12:25:23Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '108',\n",
       "  'text': 'Thanks USER !'},\n",
       " '108_2': {'created_at': '2019-01-08T07:01:28Z',\n",
       "  'author': 'l126t',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '108',\n",
       "  'text': \"What 's the mlm accuracy of pretrained model? I find the scores of candidate in top 10 are very close,but most are not suitable. Is this the same prediction as Google's original project?\"},\n",
       " '109_0': {'created_at': '2019-01-07T10:54:56Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '109',\n",
       "  'text': \"Are you asking if it possible or do you want this change included to the code?\\nI don't see why this change would cause a problem, if we choose to implement it we should add a command line argument to specify this value.\"},\n",
       " '109_1': {'created_at': '2019-01-07T12:25:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '109',\n",
       "  'text': 'Yes, feel free to submit a PR if you have a working implementation.'},\n",
       " '109_2': {'created_at': '2020-06-11T19:12:04Z',\n",
       "  'author': 'diegoantognini',\n",
       "  'author_location': 'CH',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '109',\n",
       "  'text': 'USER USER Is it still the case with the new Trainer?'},\n",
       " '110_0': {'created_at': '2019-04-17T15:16:05Z',\n",
       "  'author': 'rpoli40',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '110',\n",
       "  'text': 'I have the same issue'},\n",
       " '110_1': {'created_at': '2019-07-02T17:10:34Z',\n",
       "  'author': 'PradyumnaGupta',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '110',\n",
       "  'text': 'Did you find any solution?'},\n",
       " '110_2': {'created_at': '2020-08-07T19:52:32Z',\n",
       "  'author': 'Iwazo8700',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '110',\n",
       "  'text': 'I have the same issue too'},\n",
       " '111_0': {'created_at': '2019-01-03T12:32:51Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': 'What kind of GPU are you using?'},\n",
       " '111_2': {'created_at': '2019-01-03T12:38:58Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': \"I don't think it is possible to use BERT on CPU (didn't work for me). The model is too big.\\nIf you find a way, feel free to re-open the issue.\"},\n",
       " '111_4': {'created_at': '2019-01-03T12:55:40Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': 'You are right.\\nAnd no, unfortunately, fine-tuning is not possible on CPU in my opinion.'},\n",
       " '111_6': {'created_at': '2019-05-04T13:42:14Z',\n",
       "  'author': 'phdowling',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': 'Sorry - why is the model \"too big\" to be trained on CPU? Shouldn\\'t the memory requirements of the GPU and CPU be basically the same? As far as I can tell, BERT should run on 12GB GPUs, plenty of CPU machines have more RAM than that. Or is there a difference in how the model is materialized in memory between CPU and GPU training?'},\n",
       " '111_7': {'created_at': '2019-05-06T07:30:15Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': 'My first comment was badly worded. You can run the model on CPU but training it on CPU is unrealistic.'},\n",
       " '111_8': {'created_at': '2019-05-06T09:47:18Z',\n",
       "  'author': 'phdowling',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': \"Can you elaborate? My machine has 30gb of ram but indeed I've found I am running out of memory. How come a 12gb gpu is enough, what makes the difference? Also I'm talking about fine-tuning a multiple choice model, just for context.\"},\n",
       " '111_9': {'created_at': '2019-07-15T06:39:59Z',\n",
       "  'author': 'khaerulumam42',\n",
       "  'author_location': 'ID',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '111',\n",
       "  'text': 'How big your data USER ? I have TPU for training, if you want I will give access to you'},\n",
       " '112_1': {'created_at': '2019-01-07T10:50:59Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '112',\n",
       "  'text': 'Those messages are correct, the pretrained weights that have been released by Google Brain are just the ones of the core network. They did not release task specific (such as SQuAD) weights. To get a model that solves this task, you would have to train one yourself or get it from someone else.\\nTo answer your second question, yes, predictions can run on CPU.'},\n",
       " '112_2': {'created_at': '2019-01-07T12:27:12Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '112',\n",
       "  'text': 'USER is right (even though I think prediction on CPU will be very slow, you should use a GPU)'},\n",
       " '112_4': {'created_at': '2019-01-31T16:19:03Z',\n",
       "  'author': 'girishponkiya',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '112',\n",
       "  'text': 'Those messages are correct, the pretrained weights that have been released by Google Brain are just the ones of the core network. They did not release task-specific (such as SQuAD) weights. To get a model that solves this task, you would have to train one yourself or get it from someone else.\\n\\nDoes it mean that training-phase will not train BERT transformer parameters? If BERT params are tuned during the training phase, it should be stored in the output model. During the prediction time, tuned params should be used instead of loading BERT params from the original file.'},\n",
       " '113': {'created_at': '2019-01-04T11:23:43Z',\n",
       "  'author': 'lynnna-xu',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'TypeError Traceback (most recent call last)\\n in ()\\n----> 1 from pytorch_pretrained_bert import BertTokenizer\\n/ FILEPATH ()\\n1 version = \" VERSION \"\\n2 from .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer\\n----> 3 from .modeling import (BertConfig, BertModel, BertForPreTraining,\\n4 BertForMaskedLM, BertForNextSentencePrediction,\\n5 BertForSequenceClassification, BertForMultipleChoice,\\n/ FILEPATH ()\\n152\\n153 try:\\n--> 154 from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\\n155 except ImportError:\\n156 print(\"Better speed can be achieved with apex installed from URL )\\n/ FILEPATH ()\\n16 from apex.exceptions import (ApexAuthSecret,\\n17 ApexSessionSecret)\\n---> 18 from apex.interfaces import (ApexImplementation,\\n19 IApex)\\n20 from apex.lib.libapex import (groupfinder,\\n/ FILEPATH ()\\n8 pass\\n9\\n---> 10 class ApexImplementation(object):\\n11 \"\"\" Class so that we can tell if Apex is installed from other\\n12 applications\\n/ FILEPATH ()\\n12 applications\\n13 \"\"\"\\n---> 14 implements(IApex)\\n/ FILEPATH (*interfaces)\\n481 # the coverage for this block there. :(\\n482 if PYTHON3:\\n--> 483 raise TypeError(_ADVICE_ERROR % \\'implementer\\')\\n484 _implements(\"implements\", interfaces, classImplements)\\n485\\nTypeError: Class advice impossible in Python3. Use the USER class decorator instead.'},\n",
       " '113_0': {'created_at': '2019-01-04T11:28:12Z',\n",
       "  'author': 'lynnna-xu',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '113',\n",
       "  'text': 'Hi, I came across this error after running import pytorch_pretrained_bert. My configurations are as follows:\\ntorch version VERSION \\npython version VERSION \\ncuda VERSION'},\n",
       " '113_1': {'created_at': '2019-01-07T05:48:58Z',\n",
       "  'author': 'lynnna-xu',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '113',\n",
       "  'text': 'I uninstalled the old version of apex and reinstalled a new version. It worked. Thanks.\\ngit clone URL \\ncd apex\\npython setup.py install'},\n",
       " '113_2': {'created_at': '2020-05-19T12:27:01Z',\n",
       "  'author': 'NeelKanwal',\n",
       "  'author_location': 'NO',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '113',\n",
       "  'text': 'I still have the problem in Google Colab'},\n",
       " '113_3': {'created_at': '2020-05-26T15:32:13Z',\n",
       "  'author': 'fbaeumer',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '113',\n",
       "  'text': 'I still have the problem in Google Colab\\n\\nHello! I also had the problem and now I could solve it. Please install apex exactly as described above:\\ngit clone URL \\ncd apex\\npython setup.py install\\nDouble check the following: The git command creates a folder called apex. In this folder is another folder called apex. This folder is the folder of interest. Please rename the folder on the top level (e.g. apex-2) and move the lower apex folder to the main level. Then python will also find the folder and it should work.\\n\\nMake sure that you have the version ( VERSION ). Double check it with: \"!pip list\".'},\n",
       " '113_4': {'created_at': '2020-05-26T17:33:51Z',\n",
       "  'author': 'kommerzienrat',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '113',\n",
       "  'text': \"The following command did the job for me (based on USER 's answer):\\npip install git+ URL\"},\n",
       " '114': {'created_at': '2019-01-04T14:20:49Z',\n",
       "  'author': 'minmummax',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'is the pretrained model downloaded include word embedding?\\nI do not see any embedding in your code\\nplease'},\n",
       " '114_0': {'created_at': '2019-01-07T10:26:38Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '114',\n",
       "  'text': 'All the code related to word embeddings is located there URL \\nIf you want to access pretrained embeddings, the easier thing to do would be to load a pretrained model and extract its embedding matrices.'},\n",
       " '114_1': {'created_at': '2019-01-07T10:57:42Z',\n",
       "  'author': 'minmummax',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '114',\n",
       "  'text': 'All the code related to word embeddings is located there\\n FILEPATH \\nLines 172 to 200 in COMMIT \\nclass BertEmbeddings(nn.Module):\\n\"\"\"Construct the embeddings from word, position and token_type embeddings.\\n\"\"\"\\ndef init(self, config):\\nsuper(BertEmbeddings, self).init()\\nself.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\\nself.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\\nself.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\\n # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load \\n # any TensorFlow checkpoint file \\n self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12) \\n self.dropout = nn.Dropout(config.hidden_dropout_prob) \\n\\n def forward(self, input_ids, token_type_ids=None): \\n seq_length = input_ids.size(1) \\n position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) \\n position_ids = position_ids.unsqueeze(0).expand_as(input_ids) \\n if token_type_ids is None: \\n token_type_ids = torch.zeros_like(input_ids) \\n\\n words_embeddings = self.word_embeddings(input_ids) \\n position_embeddings = self.position_embeddings(position_ids) \\n token_type_embeddings = self.token_type_embeddings(token_type_ids) \\n\\n embeddings = words_embeddings + position_embeddings + token_type_embeddings \\n embeddings = self.LayerNorm(embeddings) \\n embeddings = self.dropout(embeddings) \\n return embeddings \\n\\nIf you want to access pretrained embeddings, the easier thing to do would be to load a pretrained model and extract its embedding matrices.\\n\\noh I have seen this code these days . and from this code I think it dose not use the pretrained embedding paras , and what do you mean by load and extract a pretrained model ???? Is it from the original supplies'},\n",
       " '114_3': {'created_at': '2019-01-07T12:28:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '114',\n",
       "  'text': 'Thanks Gregory that the way to go indeed!'},\n",
       " '115': {'created_at': '2019-01-05T07:09:20Z',\n",
       "  'author': 'mvss80',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'In the example shown to get hidden states URL \\nI want to confirm - the final hidden layer corresponds to the last element of encoded_layers, right?'},\n",
       " '115_0': {'created_at': '2019-01-07T10:44:07Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '115',\n",
       "  'text': 'Yes you are right. The first value returned is the output for BertEncoder.forward.\\n URL'},\n",
       " '116_0': {'created_at': '2019-01-06T08:04:56Z',\n",
       "  'author': 'zhaoguangxiang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '116',\n",
       "  'text': 'I can reproduce the results, learning rate is 3e-5 , epoch is VERSION'},\n",
       " '116_1': {'created_at': '2019-01-06T08:05:25Z',\n",
       "  'author': 'zhaoguangxiang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '116',\n",
       "  'text': 'by using fp16, the f1 is VERSION'},\n",
       " '116_2': {'created_at': '2019-01-06T08:08:04Z',\n",
       "  'author': 'hmt2014',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '116',\n",
       "  'text': 'by using fp16, the f1 is VERSION \\n\\nSo the key is set fp16 is True?'},\n",
       " '116_3': {'created_at': '2019-01-07T10:37:28Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '116',\n",
       "  'text': 'USER could you give the exact command line that you use to train your model?'},\n",
       " '116_4': {'created_at': '2019-01-07T12:30:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '116',\n",
       "  'text': 'Yes, please use the command line example indicated here in the readme for SQuAD.'},\n",
       " '117': {'created_at': '2019-01-07T07:22:53Z',\n",
       "  'author': 'Gpwner',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I wonder how to pretrain with my own data.'},\n",
       " '117_0': {'created_at': '2019-01-07T08:51:42Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '117',\n",
       "  'text': 'FILEPATH ( URL ) and then convert tensorflow model as pytorch.'},\n",
       " '117_1': {'created_at': '2019-01-07T12:29:44Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '117',\n",
       "  'text': \"A pre-training script is now included in master thanks to USER 's PR ISSUE_REF\"},\n",
       " '117_2': {'created_at': '2019-01-07T12:58:46Z',\n",
       "  'author': 'Gpwner',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '117',\n",
       "  'text': \"A pre-training script is now included in master thanks to USER 's PR ISSUE_REF \\nThanks !!\\nDoes it support Multiple GPU?Because the official script does not support Multiple GPU\"},\n",
       " '117_3': {'created_at': '2019-01-07T13:02:08Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '117',\n",
       "  'text': 'It does (you can read more about it here in the readme)'},\n",
       " '117_4': {'created_at': '2019-01-07T13:03:05Z',\n",
       "  'author': 'Gpwner',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '117',\n",
       "  'text': 'It does\\n\\ngreat job,thanks ~'},\n",
       " '117_5': {'created_at': '2019-01-07T13:05:35Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '117',\n",
       "  'text': 'All thanks should go to USER :-)'},\n",
       " '118': {'created_at': '2019-01-08T07:08:35Z',\n",
       "  'author': 'l126t',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"What 's the mlm accuracy of pretrained model? In my case, I find the scores of candidate in top 10 are very close,but most are not suitable. Is this the same prediction as Google's original project?\\nOriginally posted by USER in ISSUE_REF (comment)\"},\n",
       " '118_0': {'created_at': '2019-01-08T10:07:23Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '118',\n",
       "  'text': \"Hi, we didn't evaluate this metric. If you do feel free to share the results.\\nRegarding the comparison between the Google and PyTorch implementations, please refere to the included Notebooks and the associated section of the readme.\"},\n",
       " '119_0': {'created_at': '2019-01-10T04:13:34Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Sir how to resolve this? i am beginner for pytorch.\\nThanks.'},\n",
       " '119_1': {'created_at': '2019-01-10T07:16:58Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'I will have a look, I am not familiar with run_lm_finetuning yet.\\nIn the meantime maybe USER has an advice?'},\n",
       " '119_2': {'created_at': '2019-01-10T08:35:47Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Haven\\'t seen this error before, but how does your training corpus \"vocab007.txt\" look like? Is training working successfully for the file \" FILEPATH \"?'},\n",
       " '119_3': {'created_at': '2019-01-10T17:54:30Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': \"FILEPATH 't tested before this sample _text.txt. directly i am put training to my vocab007.txt\\nThank you so much USER USER sir.\"},\n",
       " '119_4': {'created_at': '2019-01-11T06:25:54Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Not sure if I understood your last message. Is this solved?'},\n",
       " '119_5': {'created_at': '2019-01-11T06:34:33Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'USER sir, stil now i am not solving this issue.\\n\\nIs training working successfully for the file \" FILEPATH \"?\\n\\nNo, i am not train the file \" FILEPATH \"\\n\\nhow does your training corpus \"vocab007.txt\" look like?\\n\\nthis is line by line sentence like file \" FILEPATH \"\\nsir i think this shape issue. batch vice split datas for multi gpu. that time this issue occurred.\\nsir any suggestion? how to resolve is bug.\\nthanks.'},\n",
       " '119_7': {'created_at': '2019-01-11T08:54:58Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Check your local setup and try to run with the above corpus\\n\\nok USER sir, now i will check.'},\n",
       " '119_8': {'created_at': '2019-01-11T09:52:50Z',\n",
       "  'author': 'yuhui-zh15',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Hi, it can be solved by using python3.6.\\nSee ISSUE_REF'},\n",
       " '119_9': {'created_at': '2019-01-11T10:11:03Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Thanks sir.'},\n",
       " '119_10': {'created_at': '2019-01-14T09:15:38Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '119',\n",
       "  'text': 'Fixed on master now (compatible with Python VERSION again)'},\n",
       " '120': {'created_at': '2019-01-09T09:41:58Z',\n",
       "  'author': 'tomohideshibata',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Thank you for this great job.\\nIn the Usage section, the [CLS] and [SEP] tokens should be added in the beginning and ending of tokenized_text?\\n# Tokenized input\\ntext = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\\ntokenized_text = tokenizer.tokenize(text)\\n\\nIn the current example, if the first token is masked (this position should be reserved for [CLS]), the result will be strange.\\nThanks.'},\n",
       " '120_0': {'created_at': '2019-01-10T00:35:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '120',\n",
       "  'text': \"You are right, I'll fix the readme\"},\n",
       " '120_1': {'created_at': '2019-07-31T19:04:23Z',\n",
       "  'author': 'hughperkins',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '120',\n",
       "  'text': \"So, just to clarify, I should add '[CLS]' and '[SEP]' to the beginning and end of each utterance respectively, and it's a bug in the examples that they dont do this?\"},\n",
       " '120_2': {'created_at': '2019-08-19T12:51:50Z',\n",
       "  'author': 'wahlforss',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '120',\n",
       "  'text': 'USER did you get any clarification on this?'},\n",
       " '121': {'created_at': '2019-01-10T05:01:17Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"The file references args.do_lower_case, but doesn't have the corresponding parser.add_argument call.\\nAs an aside, has anyone successfully applied LM fine-tuning for a downstream task (using this code, or maybe using the original tensorflow implementation)? I'm not even sure if the code will run in its current state. And after fixing this issue locally, I've had no luck using the output from fine-tuning: I have a model that gets state-of-the-art results when using pre-trained BERT, FILEPATH ! I don't know whether to suspect that there are might be other bugs in the example code, or if the hyperparameters in the README are just a very poor starting point for what I'm doing.\"},\n",
       " '121_0': {'created_at': '2019-01-10T05:09:41Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': \"On a related note: I see there is learning rate scheduling happening here, but also inside the BertAdam class. Is this not redundant and erroneous? For reference I'm not using FP16 training, which has its own separate optimizer that doesn't appear to perform redundant learning rate scheduling.\\nThe same is true for other examples such as SQuAD (maybe it's the cause of ISSUE_REF , where results were reproduced only when using float16 training?)\"},\n",
       " '121_1': {'created_at': '2019-01-10T22:42:30Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': 'Here also USER , maybe you have some feedback from using the fine-tuning script?'},\n",
       " '121_2': {'created_at': '2019-01-11T00:07:10Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': \"I figured out why I was seeing such poor results while attempting to fine-tune: the example saves model.bert instead of model to pytorch_model.bin, so the resulting file can't just be zipped up and loaded with from_pretrained.\"},\n",
       " '121_3': {'created_at': '2019-01-11T07:42:43Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': \"I have just fixed the do_lower_case bug and adjusted the code for model saving to be in line with the other examples (see ISSUE_REF ). I hope this solves your issue. Thanks for reporting!\\n\\nAs an aside, has anyone successfully applied LM fine-tuning for a downstream task (using this code, or maybe using the original tensorflow implementation)?\\n\\nWe are currently using a fine-tuned model for a rather technical corpus and see improvements in terms of the extracted document embeddings in contrast to the original pre-trained BERT. However, we haven't done intense testing of hyperparameters or performance comparisons with the original pre-trained model yet. This is all still work in progress on our side. If you have results that you can share in public, I would be interested to see the difference you achieve. In general, FILEPATH .\\n\\nOn a related note: I see there is learning rate scheduling happening here, but also inside the BertAdam class.\\n\\nWe have only trained with fp16 so far. USER have you experienced issues with LR scheduling in the other examples? Just copied the code from there.\"},\n",
       " '121_4': {'created_at': '2019-01-11T08:10:51Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': \"Thanks for fixing these!\\n FILEPATH 'm seeing downstream performance comparable to using pre-trained BERT. I just got a big scare when the default log level wasn't high enough to notify me that weights were being randomly re-initialized instead of loaded from the file I specified. It's still too early for me to tell if there are actual benefits to fine-tuning, though.\"},\n",
       " '121_5': {'created_at': '2019-01-14T09:04:46Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': 'All this looks fine on master now. Please open a new issue (or re-open this one) if there are other issues.'},\n",
       " '121_6': {'created_at': '2019-01-15T14:34:14Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '121',\n",
       "  'text': \"I saw on ISSUE_REF (comment) that there's potentially some documentation effort underway beyond the README. Thanks a lot for this!\\nI wonder if there's the possibility to add more detail about how to properly prepare a custom corpus (e.g. to avoid catastrophical forgetting) finetune the models on. Asking this as my (few, so far) attempts to finetune on other corpora have been destructive for performance on GLUE tasks when compared to the original models (I just discovered this issue, maybe the things you mention above affected me too).\\nKudos USER USER for all your work on this!\"},\n",
       " '122': {'created_at': '2019-01-10T07:25:30Z',\n",
       "  'author': 'dalonlobo',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'issue',\n",
       "  'text': 'Can we use the pre-trained BERT model for Punctuation Prediction for Conversational Speech? Let say punctuating an ASR output?'},\n",
       " '122_0': {'created_at': '2019-01-14T09:05:22Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '122',\n",
       "  'text': \"Hi, I don't really now. I guess you should just give it a try.\"},\n",
       " '123_0': {'created_at': '2019-01-11T07:10:27Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '123',\n",
       "  'text': 'Hi!\\nThose messages are correct, the pretrained weights that have been released by Google Brain are just the ones of the core network. They did not release task specific weights. To get a model that solves a specific classification task, you would have to train one yourself or get it from someone else.\\n USER There have been multiple issues about this specific behavior, maybe we should add some kind of text either as a print while loading the model or in the documentation. I would be happy to do it. What would you prefer?'},\n",
       " '123_1': {'created_at': '2019-01-11T07:39:40Z',\n",
       "  'author': 'lemonhu',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '123',\n",
       "  'text': 'Oh, I see, I will train the model with my own dataset, thank you for your answer.'},\n",
       " '123_2': {'created_at': '2019-01-14T09:08:01Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '123',\n",
       "  'text': 'Yes you are right USER we should detail a bit the messages in modeling.py to say that These weights will be trained from scratch.'},\n",
       " '124': {'created_at': '2019-01-11T06:27:39Z',\n",
       "  'author': 'zhusleep',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"I run the bert-base-uncased model with task 'mrpc' in ubuntu,nvidia p4000 8G.\\nIt's a classification problem, and I use the default demo data.\\nBut the training speed is about 2 batch every second. Any problem?\\nI think it maybe too slow, but can not find why. I have another task with COMMIT data costs 6 hours per epoch.\"},\n",
       " '124_0': {'created_at': '2019-01-14T09:09:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '124',\n",
       "  'text': 'Maybe try to use a bigger batch size or try fp16 training?\\nPlease refer to the detailed instructions in the readme.'},\n",
       " '125': {'created_at': '2019-01-11T09:43:43Z',\n",
       "  'author': 'yuhui-zh15',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'When running run_lm_finetuning.py to fine-tune language model with default settings (see command below), sometimes I could run successfully, but sometimes I received different errors like RuntimeError: The size of tensor a must match the size of tensor b at non-singleton dimension 1, RuntimeError: FILEPATH :35 or RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1). This problem can be solved when updating python3.5 to python3.6.\\npython run_lm_finetuning.py \\\\\\n FLAG ~/ FILEPATH / \\\\\\n FLAG \\\\\\n FLAG ~/ FILEPATH \\\\\\n FLAG ~/ FILEPATH \\\\\\n FLAG VERSION \\\\\\n FLAG 3e-5 \\\\\\n FLAG 32 \\\\\\n FLAG 128 \\\\\\n FLAG'},\n",
       " '125_0': {'created_at': '2019-01-11T10:09:34Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '125',\n",
       "  'text': 'Thank you USER sir. i will check.'},\n",
       " '125_1': {'created_at': '2019-01-14T09:10:02Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '125',\n",
       "  'text': 'This should be fixed on master now (thanks to ISSUE_REF )'},\n",
       " '126_0': {'created_at': '2019-01-14T09:11:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '126',\n",
       "  'text': 'You should update to the latest version of pytorch_pretrained_bert(pip install pytorch_pretrained_bert FLAG )'},\n",
       " '127': {'created_at': '2019-01-11T10:35:36Z',\n",
       "  'author': 'artemisart',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'issue',\n",
       "  'text': \"URL \\nI don't understand how it is useful to wrap the BertLMPredictionHead class like that, perhaps it was forgotten in some refactoring ? I can do a PR if you confirm me it can be replaced.\\nBertOnlyMLMHead is only used in BertForMaskedLM.\"},\n",
       " '127_0': {'created_at': '2019-01-14T09:14:55Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '127',\n",
       "  'text': \"That's an heritage of how I converted the TF code (by reproducing the scope architecture in TF with PyTorch classes). We can't really change that now without re-converting all the TF code.\\nIf you want a more concise version of PyTorch BERT, you can check pytorchic-bert.\"},\n",
       " '128_0': {'created_at': '2019-01-14T09:16:36Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '128',\n",
       "  'text': \"Hi, I don't think you can do that in a clean way, sorry. That how BERT is trained.\"},\n",
       " '129': {'created_at': '2019-01-12T20:22:45Z',\n",
       "  'author': 'PetrochukM',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi There!\\nIs the weight decay fix from?\\n URL \\nThanks!'},\n",
       " '130_0': {'created_at': '2019-01-14T07:21:57Z',\n",
       "  'author': 'anz2',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '130',\n",
       "  'text': 'I trained BertForSequenceClassification model with cola dataset mode for binary classification. It saved only eval_results.txt and pytorch_model.bin files. When I am loading model again like:\\nmodel = BertForSequenceClassification.from_pretrained(\\'models/\\')\\nit produces such error:\\nwith open(json_file, \"r\", encoding=\\'utf-8\\') as reader:\\nFileNotFoundError: [Errno 2] No such file or directory: \\' FILEPATH \\'\\nI trained models using the command:\\nexport GLUE_DIR=data_dir_path; python run_classifier.py FLAG cola FLAG FLAG FLAG $ FILEPATH /\\nDo I have any error with training script?\\nHow can I produce such config.json file to load model successfully?'},\n",
       " '131': {'created_at': '2019-01-14T09:10:06Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"In the following two code snippets below:\\n URL \\n URL \\nit appears that learning rate warmup is being done twice: once in the example file, and once inside the BertAdam class. Am I reading this wrong? Because I'm pretty sure the BertAdam class performs its own warm-up when initialized with those arguments.\\nHere is an excerpt from the BertAdam class, where warm-up is also applied:\\n URL \\nThis also applies to other examples, e.g.\\n URL \\n URL\"},\n",
       " '131_0': {'created_at': '2019-01-14T09:21:10Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'Humm could be the case indeed. What do think about this USER ?'},\n",
       " '131_1': {'created_at': '2019-01-14T09:24:10Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'As far as I can tell this was introduced in COMMIT as a byproduct of adding float16 support, and was then copied to other example files as well.'},\n",
       " '131_2': {'created_at': '2019-01-17T13:26:04Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': \"I agree, there seems to be double LR scheduling. The applied LR is therefore lower than intended. Quick plot of the LR being set in the outer scope (i.e. in run_squad or run_lm_finetuning) vs. the inner one (in BERTAdam) shows this:\\n\\nIn addition, I have noticed two further parts for potential clean up:\\n\\nI don't see a reason why the function warmup_linear() is implemented in two places: In optimization.py and in each example script.\\nIs the method optimizer.get_lr() ever being called? There's actually another LR scheduling.\\n URL\"},\n",
       " '131_3': {'created_at': '2019-01-22T22:45:11Z',\n",
       "  'author': 'matej-svejda',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'There is als an additional problem that causes the learning rate to not be set correctly in run_classifier.py. I created a pull request for that (and the double warmup problem): ISSUE_REF'},\n",
       " '131_4': {'created_at': '2019-01-27T16:32:44Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'Is there are something done for this double warmup bug?'},\n",
       " '131_5': {'created_at': '2019-01-27T16:49:14Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'Yes, USER worked on this in ISSUE_REF'},\n",
       " '131_6': {'created_at': '2019-01-27T16:55:33Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': \"I see that, but it isn't merge now?\"},\n",
       " '131_7': {'created_at': '2019-01-27T17:20:45Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': \"No, not yet. As you can see in the PR it's still WIP and he committed only 4 hours ago. If you need the fix urgently, you can apply the changes easily locally. It's quite a small fix.\"},\n",
       " '131_8': {'created_at': '2019-01-27T17:40:13Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'Sorry,I forget to see the time :)'},\n",
       " '131_9': {'created_at': '2019-01-28T07:57:34Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': \"By the way, how can I draw a picture about the LR schedule about BERT like yours. I see if use print(optimizer.param_groups['lr'] , the learning rate is always like I init it.\"},\n",
       " '131_11': {'created_at': '2019-02-05T16:07:58Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '131',\n",
       "  'text': 'Ok this should be fixed in master now!'},\n",
       " '132': {'created_at': '2019-01-15T01:56:48Z',\n",
       "  'author': 'phatlast96',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'Has this been confirmed?\\n URL'},\n",
       " '132_0': {'created_at': '2019-01-16T12:23:14Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '132',\n",
       "  'text': \"Not really, I've moved to something else since I don't expect this to change significantly the results.\\nI will remove the TODO.\"},\n",
       " '133_0': {'created_at': '2019-01-16T08:23:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'Maybe use the torch.no_grad() context-manager which is the recommended way to perform inference with PyTorch now?\\nSee URL'},\n",
       " '133_1': {'created_at': '2019-01-23T16:12:52Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'Closing this. Feel free to re-open if the issue is still there.'},\n",
       " '133_2': {'created_at': '2019-12-18T07:59:21Z',\n",
       "  'author': 'RomanTeucher',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'Hey there, I also have some memory leak problem when using the BertModel to produce embeddings to be used as features later on.\\nI basically use the implementation as in the usage example.\\nself.tokenizer = BertTokenizer.from_pretrained(\\'bert-base-multilingual-cased\\')\\nself.model = trafo.BertModel.from_pretrained(\\'bert-base-multilingual-cased\\')\\nself.model.eval()\\n\\n...\\n\\ndef encode_text(self, text: str) -> np.ndarray:\\n to_tokenize = f\"[CLS] {text} [SEP]\"\\n tokenized_text = self.tokenizer.tokenize(to_tokenize)\\n tokenized_text = tokenized_text[0:500]\\n # Convert token to vocabulary indices\\n indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\\n with torch.no_grad():\\n tokens_tensor = torch.tensor([indexed_tokens]).data\\n outputs = self.model(tokens_tensor)\\n return outputs\\nI realized that if I comment out the line outputs = self.model(tokens_tensor) and just return some random numpy array as output, I have not increasing memory problem. So it seems to be calling the model with the tensor that increases the memory.\\nFurther, if I use the \\'bert-base-uncased\\' model, the memory stays the same as well. It only happens with the multi models.\\nI used this method in a flask server application and made REST requests to it.'},\n",
       " '133_3': {'created_at': '2019-12-18T08:22:47Z',\n",
       "  'author': 'TheEdoardo93',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'It\\'s useful your assertion that it occurs only when using multi-lingual BERT model. Can you try to use bert-base-multilingual-uncased in order to do a comparison between these two? Perhaps there is a performance bug in the multi-lingual setting.\\n\\nHey there, I also have some memory leak problem when using the BertModel to produce embeddings to be used as features later on.\\nI basically use the implementation as in the usage example.\\nself.tokenizer = BertTokenizer.from_pretrained(\\'bert-base-multilingual-cased\\')\\nself.model = trafo.BertModel.from_pretrained(\\'bert-base-multilingual-cased\\')\\nself.model.eval()\\n\\n...\\n\\ndef encode_text(self, text: str) -> np.ndarray:\\n to_tokenize = f\"[CLS] {text} [SEP]\"\\n tokenized_text = self.tokenizer.tokenize(to_tokenize)\\n tokenized_text = tokenized_text[0:500]\\n # Convert token to vocabulary indices\\n indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\\n with torch.no_grad():\\n tokens_tensor = torch.tensor([indexed_tokens]).data\\n outputs = self.model(tokens_tensor)\\n return outputs\\nI realized that if I comment out the line outputs = self.model(tokens_tensor) and just return some random numpy array as output, I have not increasing memory problem. So it seems to be calling the model with the tensor that increases the memory.\\nFurther, if I use the \\'bert-base-uncased\\' model, the memory stays the same as well. It only happens with the multi models.\\nI used this method in a flask server application and made REST requests to it.'},\n",
       " '133_4': {'created_at': '2019-12-19T07:13:21Z',\n",
       "  'author': 'RomanTeucher',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"So I tried it with bert-base-multilingual-uncased as well and it is the same behavior.\\nI do not understand, why memory constantly grows on inference. To my understanding, I only push data through the network and then use the result layer's output. Before using the transformers, I had been using custom word embeddings trained in own keras models and I did not have this behavior. What am I missing here?\"},\n",
       " '133_5': {'created_at': '2019-12-19T08:27:28Z',\n",
       "  'author': 'TheEdoardo93',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"I've just seen that you're using PyTorch VERSION ! What an oldest version you're using :D can you try to install the latest version of PyTorch ( VERSION ) through pip install FLAG torch and give us feedback? And please, if you can, update also the version of Transformers to the last ( VERSION ) through pip install FLAG transformers.\\n\\nSo I tried it with bert-base-multilingual-uncased as well and it is the same behavior.\\nI do not understand, why memory constantly grows on inference. To my understanding, I only push data through the network and then use the result layer's output. Before using the transformers, I had been using custom word embeddings trained in own keras models and I did not have this behavior. What am I missing here?\"},\n",
       " '133_6': {'created_at': '2019-12-19T12:56:06Z',\n",
       "  'author': 'RomanTeucher',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"Hey there, I'm using the newest pytorch and transformers. You are probably mistaking this because of the first comment of this thread (by zhangjcqq) but that was not mine. I just hijacked this thread because it seemed to be the same problem I now have and there was no solution here.\"},\n",
       " '133_7': {'created_at': '2019-12-19T13:32:44Z',\n",
       "  'author': 'TheEdoardo93',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"Hey there, I'm using the newest pytorch and transformers. You are probably mistaking this because of the first comment of this thread (by zhangjcqq) but that was not mine. I just hijacked this thread because it seemed to be the same problem I now have and there was no solution here.\\n\\nSo you have tried out to upgrade PyTorch to VERSION as suggested in my last comment, but there is the same error? If no, specify your environment and a piece of code in order to reproduce the bug.\"},\n",
       " '133_8': {'created_at': '2019-12-20T10:13:18Z',\n",
       "  'author': 'RomanTeucher',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"I have the newest version of pytorch and transformers, yes.\\nI have been monitoring the memory usage over 24h when I made ~ VERSION requests. It seems that the memory increases constantly for quite some time but also seems to stabilize at a certain maximum. So the application started using ~2.5GB RAM and now stays at ~4.3GB.\\nMaybe it has something to do with varying lengths of the texts I process? So that the longest texts are processed at a later point in time which then require the most RAM. Then, any subsequent text cannot need more so it stabilizes. Though this is just a thought.\\nThanks already for your help, I'm off to Christmas vacations for now and will have a look at the issue in January again. I'll see if memory usage increases by then.\"},\n",
       " '133_9': {'created_at': '2020-06-05T08:07:08Z',\n",
       "  'author': 'LowinLi',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'flask\\n\\nI miss in the same problems\\nbut without flask, it works'},\n",
       " '133_10': {'created_at': '2020-07-13T19:14:53Z',\n",
       "  'author': 'amjltc295',\n",
       "  'author_location': 'TW',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"I have the newest version of pytorch and transformers, yes.\\nI have been monitoring the memory usage over 24h when I made ~ VERSION requests. It seems that the memory increases constantly for quite some time but also seems to stabilize at a certain maximum. So the application started using ~2.5GB RAM and now stays at ~4.3GB.\\nMaybe it has something to do with varying lengths of the texts I process? So that the longest texts are processed at a later point in time which then require the most RAM. Then, any subsequent text cannot need more so it stabilizes. Though this is just a thought.\\nThanks already for your help, I'm off to Christmas vacations for now and will have a look at the issue in January again. I'll see if memory usage increases by then.\\n\\nI have similar problems too. The memory usage gradually grows from 1xxxM to 3xxxM. USER USER did you manage to solve the issue?\"},\n",
       " '133_11': {'created_at': '2020-09-16T04:37:10Z',\n",
       "  'author': 'aayagar001',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'USER Did you find any solution to above issue?'},\n",
       " '133_12': {'created_at': '2020-09-16T04:46:58Z',\n",
       "  'author': 'LowinLi',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'USER Did you find any solution to above issue?\\nwhen i run flask by:\\n\\nthreaded=False\\nit works'},\n",
       " '133_13': {'created_at': '2020-09-17T11:35:16Z',\n",
       "  'author': 'RomanTeucher',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'USER Did you find any solution to above issue?\\n\\nIt seems that any python process takes up more and more RAM over time. A co-worker of mine had issues as well but with some other python project. We have our applications in docker containers that are limited in RAM, so they all run at 100% after some time.\\nAnyways, the applications still works as it is supposed to be, so we did not put further research into that.'},\n",
       " '133_15': {'created_at': '2021-03-16T07:47:17Z',\n",
       "  'author': 'pkadambi',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'Reporting that this issue still exists with the forward pass of BertModel, specifically the call to BertModel.forward(), I notice that system RAM usage increases on this line each iteration.\\nTransformers VERSION \\nPytorch VERSION \\nCuda VERSION \\nCudnn VERSION .5_0\\nRTX 3090\\nI am unable to run MNLI because of this, the RAM maxes out, and then system crashes towards the end of the 3rd training epoch. I will do some more digging and report back if I find a solution.'},\n",
       " '133_16': {'created_at': '2022-08-05T11:45:18Z',\n",
       "  'author': 'BinzhuWang',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': 'Mark. Still suffering this problem in Aug. 2022.\\nSomeone can offer a solution for this could be highly appreciated'},\n",
       " '133_17': {'created_at': '2023-03-30T00:09:17Z',\n",
       "  'author': 'samlopezruiz',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '133',\n",
       "  'text': \"I'm having the same issue running in Databricks with the following versions:\\ntransformers VERSION \\npytorch VERSION +cu117\\nNvidia Tesla T4\"},\n",
       " '134_0': {'created_at': '2019-01-16T08:34:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '134',\n",
       "  'text': 'Hi, you need a (stable) internet connection to download the weights. This operation is only done once as the weights are then cached on you drive.'},\n",
       " '134_1': {'created_at': '2019-01-16T09:33:09Z',\n",
       "  'author': 'laibamehnaz',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '134',\n",
       "  'text': 'Thankyou so much!'},\n",
       " '134_2': {'created_at': '2021-06-29T03:12:10Z',\n",
       "  'author': 'moh-yani',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '134',\n",
       "  'text': 'USER have you solved the problem? I have a similar problem.'},\n",
       " '134_3': {'created_at': '2021-12-29T05:56:30Z',\n",
       "  'author': 'Arumoy91',\n",
       "  'author_location': 'IE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '134',\n",
       "  'text': 'Hi,\\nI am facing a similar issue, can anyone help with this?'},\n",
       " '135_0': {'created_at': '2019-01-18T08:33:03Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'Hi USER ,\\nTraining BERT from scratch takes a (very) long time (see the paper for TPU training, an estimation is training time using GPUs is about a week using 64 GPUs), this script is more for fine-tuning (using the pre-training objective) than to train from scratch.\\nDid you monitor the losses during training and wait for convergence?'},\n",
       " '135_1': {'created_at': '2019-01-18T17:33:08Z',\n",
       "  'author': 'Slash0BZ',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"Hi, I am trying to do something similar:) My guess is that sample.txt is too small.\\n USER Just to confirm, the above code should produce a new BERT model from scratch that's based on the existing vocab file right? Thanks!\"},\n",
       " '135_2': {'created_at': '2019-01-18T18:10:21Z',\n",
       "  'author': 'kkadowa',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'It seems to be problematic to generate new samples every epoch, at least for such a small corpus.\\nThe model convergenced for me with FLAG VERSION , if I reuse the same train_dataset by adding train_dataset = [train_dataset[i] for i in range(len(train_dataset))] in the code.'},\n",
       " '135_3': {'created_at': '2019-01-18T20:15:14Z',\n",
       "  'author': 'haoyudong-97',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'Hi USER ,\\nI trained the model for an hour but the loss is always around VERSION - VERSION and never converges. I know it\\'s computationally expensive to train the BERT; that\\'s why I choose the very small dataset (sample.txt, which only has 36 lines).\\nThe main issue is that I have tried the same dataset with the original tensorflow version BERT and it converges within 5 minutes:\\n\\nnext_sentence_accuracy = VERSION \\nnext_sentence_loss = 0. COMMIT \\n\\nThat\\'s why I\\'m wondering if something is wrong with the model. I have also checked the output of each forward step, and found out that the encoder_layers have similar row values, i.e. rows in the matrix \"encoder_layers\" are similar to each other.\\n encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=output_all_encoded_layers)'},\n",
       " '135_4': {'created_at': '2019-01-18T20:31:01Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"Ok, that's strange indeed. Can you share your code? I can have a look.\\nI haven't tried the pre-training script myself yet.\"},\n",
       " '135_5': {'created_at': '2019-01-18T21:53:11Z',\n",
       "  'author': 'haoyudong-97',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"Thanks for helping! I have created a github repo with my modified code. Also, I have tried what USER suggests (thanks!) and it does work.\\nBut I feel that shouldn't be the correct way for final solution as it stores every data on memory and it will require too much if training with real dataset.\"},\n",
       " '135_6': {'created_at': '2019-01-18T22:23:01Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"Thank, I'll have a look. Can you also show me what you did with the Tensorflow model so I can compare the behaviors in the two cases?\"},\n",
       " '135_7': {'created_at': '2019-01-18T22:26:28Z',\n",
       "  'author': 'haoyudong-97',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'I just follow the instructions under section Pre-training with BERT'},\n",
       " '135_8': {'created_at': '2019-01-19T09:10:51Z',\n",
       "  'author': 'kkadowa',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"But I feel that shouldn't be the correct way for final solution as it stores every data on memory and it will require too much if training with real dataset.\\n\\n USER Yes, I just showed one of the differences from Tensorflow version, and that's why I didn't send a PR addressing this. I'm even not sure whether this affects the model performance when you train with real dataset or not.\\nIncidentally, I'm also trying to do something similar, with real data, but still losses seems higher than that of Tensorflow version. I suspect some of minor differences (like this, issues 195 and 38), but not yet figured it out.\"},\n",
       " '135_9': {'created_at': '2019-01-25T14:48:28Z',\n",
       "  'author': 'snakers4',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'Hi guys,\\n\\nsee the paper for TPU training, an estimation is training time using GPUs is about a week using 64 GPUs\\n\\nBtw, there is an article on this topic URL \\nI was wondering, maybe someone tried tweaking some parameters in the transformer, so that it could converge much faster (ofc, maybe at the expense of accuracy), i.e.:\\n\\n FILEPATH ;\\nUsing a more standard 200 or 300 dimension embedding instead of 768 (also tweaking the hidden size accordingly);\\n\\nPersonally for me the allure of transformer is not really about the state-of-the-art accuracy, but about having the same architecture applicable for any sort of NLP task (i.e. QA tasks or SQUAD like objectives may require a custom engineering or some non-transferrable models).'},\n",
       " '135_10': {'created_at': '2019-02-22T04:34:16Z',\n",
       "  'author': 'BITLsy',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'HI,I have a problem that which line code leet the pretrained model freezed(fine-turn) but no trainable'},\n",
       " '135_11': {'created_at': '2019-03-06T12:16:55Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'Hi USER and USER , please open new issues for your problems and discussion.'},\n",
       " '135_12': {'created_at': '2019-08-14T19:43:17Z',\n",
       "  'author': 'ntomita',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'Hi USER Do you have any update on this? Is the issue resolved?'},\n",
       " '135_13': {'created_at': '2019-08-19T14:57:20Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'Hi USER yes, this is just a differing behavior between the TensorFlow and PyTorch training code.\\n\\nthe original TensorFlow code does static masking in which the masking of the training dataset is computed once for all so you can quickly overfit on a small training set with a few epochs\\nin our code we use dynamic masking where the masking is generated on the fly so overfitting a single batch takes more epochs.\\n\\nThe recent RoBERTa paper ( URL ) compares the two approaches (see section VERSION ) and conclude that dynamic masking is comparable or slightly better than static masking (as expected I would say).'},\n",
       " '135_14': {'created_at': '2019-08-19T16:53:46Z',\n",
       "  'author': 'ntomita',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"Hi USER that's awesome! I was working on pretraining a modified BERT model using this library with our own data for a quite while, struggled convergence, and wondering if I should try other libraries like original tf implementation or fairseq as other people reported slower convergence with this library. I use dynamic masking so what you're saying is reasonable. I also saw recently that MS azure group has successfully pretrained their models which are implemented with this library. Since you keep telling people that this library is not meant for pretraining I thought there are some critical bugs in models or optimization processes. I needed some confidence to keep working with this library so thanks for your follow-up!\"},\n",
       " '135_15': {'created_at': '2019-08-20T09:59:34Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': 'No \"critical bugs\" indeed lol :-)\\nYou can use this library as the basis for training from scratch (like Microsoft and NVIDIA did).\\nWe just don\\'t provide training scripts (at the current stage, maybe we\\'ll add some later but I would like to keep them simple if we do).'},\n",
       " '135_16': {'created_at': '2022-01-19T12:20:48Z',\n",
       "  'author': 'sipie800',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '135',\n",
       "  'text': \"Bert is way to sensitive to the learning rate and data as well.\\nSomehow it makes thing back to 20 years ago when deep learning is still a unusable approch.\\nIt's not the fault of libraries writers. The model itself has that problem.\"},\n",
       " '136': {'created_at': '2019-01-18T02:19:58Z',\n",
       "  'author': 'lhbrichard',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I wanna do the fine-tuning work by adding a textcnn on the base of BertModel. I write a new class and add two layers of conv (like a textcnn) basically on Embedding Layer. And then an error occurs, called \"grad can be implicitly created only for scalar outputs\" i search for the Internet and can\\'t find a good solution to that, hope someone can solve it'},\n",
       " '136_0': {'created_at': '2019-01-18T08:33:34Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '136',\n",
       "  'text': 'If you can share a (minimal) example reproducing the error, I can have a look.'},\n",
       " '136_1': {'created_at': '2019-01-23T16:34:28Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '136',\n",
       "  'text': \"I'm closing this. Feel free to re-open and share more information if you still have some issues.\"},\n",
       " '137': {'created_at': '2019-01-18T05:52:40Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Two to Three mask word prediction at the same sentence also very complex.\\nhow to get good accuracy?\\nif i have to pretrained bert model and own dataset with masked_lm_prob= VERSION ( URL ), what will happened?\\nThanks.'},\n",
       " '137_0': {'created_at': '2019-01-18T08:34:40Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '137',\n",
       "  'text': \"Hi USER , from my experiments two to three mask word prediction doesn't seems to be possible with BERT.\"},\n",
       " '137_1': {'created_at': '2019-01-22T16:51:09Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '137',\n",
       "  'text': 'thanks USER sir'},\n",
       " '138': {'created_at': '2019-01-18T14:04:11Z',\n",
       "  'author': 'jianyucai',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi, I noticed that there is something called Attention Mask in the model.\\nIn the annotation of class BertForQuestionAnswering,\\n INLINECODE : an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\\n input sequence length in the current batch. It's the mask that we typically use for attention when\\n a batch has varying length sentences.\\nAnd its usage is in class BertSelfAttention, function forward,\\n# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\\nattention_scores = attention_scores + attention_mask\\nIt seems the attention_mask is used to add 1 to the scores for positions that is taken up by real tokens, and add 0 to the positions outside current sequence.\\nThen, why not set the scores to FLAG where the positions are outside the current sequence. Then pass the scores to a softmax layer, those score will become 0 as we want.\"},\n",
       " '138_0': {'created_at': '2019-01-18T22:26:00Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '138',\n",
       "  'text': \"Yes, this conversion is done inside the model, see this line: URL \\n(we don't use infinity but a large value that works also when the model is used in half precision mode)\"},\n",
       " '138_1': {'created_at': '2019-01-19T02:37:56Z',\n",
       "  'author': 'jianyucai',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '138',\n",
       "  'text': \"Thanks for your answer. Well, I still have a little problem understanding what you mean in the last sentence:\\n\\n(we don't use infinity but a large value that works also when the model is used in half precision mode)\\n\\nI make a simple experiment:\\n\\narray( FLAG )\\n\\ntensor( FLAG , dtype=torch.float64)\\n\\ntensor( FLAG , dtype=torch.float16)\\nIt seems FLAG works well.\\nIn another experiment, I tried the following:\\n\\ntensor([1., 2., 3., FLAG , FLAG ])\\n\\ntensor([1., 2., 3., FLAG , FLAG ], dtype=torch.float16)\\nIt seems that both 2 experiments works well, so I don't get what is the problem to use FLAG in half precision mode\\nThank you\"},\n",
       " '138_2': {'created_at': '2022-08-19T19:37:44Z',\n",
       "  'author': 'mahdiabdollahpour',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '138',\n",
       "  'text': 'attention mask is - VERSION for positins that we do not want attention for\\nit is set to - VERSION in\\n # Since attention_mask is VERSION for positions we want to attend and VERSION for\\n # masked positions, this operation will create a tensor which is VERSION for\\n # positions we want to attend and - VERSION for masked positions.\\n # Since we are adding it to the raw scores before the softmax, this is\\n # effectively the same as removing these entirely.\\n\\nin get_extended_attention_mask'},\n",
       " '139': {'created_at': '2019-01-18T15:36:20Z',\n",
       "  'author': 'ironflood',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi,\\nI obtained strange classification eval results (always predicting the same label) when trying out the run_classifier.py after cloning the repo (no modif) so to dig a bit more I rebalanced the CoLA dataset (train.tsv and dev.tsv) to have a better understanding of what is happening. When running the classifier example the network still doesn't learn and keeps predicting the same label. Any idea why? FILEPATH , the results are the same. Any pointers? Am I missing something big here?\\nThe rebalanced & randomized CoLA dataset (simply sampled down the majority class to minority one).\\n URL \\nMy training command:\\n FILEPATH \\nThe output digits are always in favor of one class, example:\\neval batch#0 print(digits)\\n[[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]]\\n\\neval batch#1 print(digits)\\n[[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]\\n[ 0. COMMIT 0. COMMIT ]]\\n\\nSome training examples:\\n FILEPATH :28:12 - INFO - __main__ - *** Example ***\\n FILEPATH :28:12 - INFO - __main__ - guid: train-0\\n FILEPATH :28:12 - INFO - __main__ - tokens: [CLS] Ent ##hus ##ias ##tic golf ##ers with large hand ##icap ##s can be good company . [SEP]\\n FILEPATH :28:12 - INFO - __main__ - input_ids: 101 63412 15471 15465 13275 32288 10901 10169 12077 15230 73130 10107 10944 10347 15198 12100 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - label: 1 (id = 1)\\n FILEPATH :28:12 - INFO - __main__ - *** Example ***\\n FILEPATH :28:12 - INFO - __main__ - guid: train-1\\n FILEPATH :28:12 - INFO - __main__ - tokens: [CLS] The horse jump ##ed over the fe ##nce . [SEP]\\n FILEPATH :28:12 - INFO - __main__ - input_ids: 101 10117 30491 54941 10336 10491 10105 34778 12150 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - label: 1 (id = 1)\\n FILEPATH :28:12 - INFO - __main__ - *** Example ***\\n FILEPATH :28:12 - INFO - __main__ - guid: train-2\\n FILEPATH :28:12 - INFO - __main__ - tokens: [CLS] Brown equipped Jones a camera . [SEP]\\n FILEPATH :28:12 - INFO - __main__ - input_ids: 101 12623 41880 12298 169 26665 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - label: 0 (id = 0)\\n FILEPATH :28:12 - INFO - __main__ - *** Example ***\\n FILEPATH :28:12 - INFO - __main__ - guid: train-3\\n FILEPATH :28:12 - INFO - __main__ - tokens: [CLS] I destroyed there . [SEP]\\n FILEPATH :28:12 - INFO - __main__ - input_ids: 101 146 24089 11155 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :28:12 - INFO - __main__ - label: 0 (id = 0)\\n\\nSome eval examples:\\n FILEPATH :32:04 - INFO - __main__ - *** Example ***\\n FILEPATH :32:04 - INFO - __main__ - guid: dev-0\\n FILEPATH :32:04 - INFO - __main__ - tokens: [CLS] Dana walk ##ed and Leslie ran . [SEP]\\n FILEPATH :32:04 - INFO - __main__ - input_ids: 101 27149 33734 10336 10111 25944 17044 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - label: 1 (id = 1)\\n FILEPATH :32:04 - INFO - __main__ - *** Example ***\\n FILEPATH :32:04 - INFO - __main__ - guid: dev-1\\n FILEPATH :32:04 - INFO - __main__ - tokens: [CLS] The younger woman might have been tall and , and the older one def ##inite ##ly was , bl ##ond . [SEP]\\n FILEPATH :32:04 - INFO - __main__ - input_ids: 101 10117 27461 18299 20970 10529 10590 36243 10111 117 10111 10105 18757 10464 100745 100240 10454 10134 117 21484 26029 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - label: 0 (id = 0)\\n FILEPATH :32:04 - INFO - __main__ - *** Example ***\\n FILEPATH :32:04 - INFO - __main__ - guid: dev-2\\n FILEPATH :32:04 - INFO - __main__ - tokens: [CLS] What the water did to the bot ##tle was fill it . [SEP]\\n FILEPATH :32:04 - INFO - __main__ - input_ids: 101 12489 10105 12286 12172 10114 10105 41960 16406 10134 20241 10271 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n FILEPATH :32:04 - INFO - __main__ - label: 0 (id = 0)\"},\n",
       " '139_0': {'created_at': '2019-01-18T18:10:40Z',\n",
       "  'author': 'kkadowa',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '139',\n",
       "  'text': 'Try this for CoLA: FLAG bert-base-uncased FLAG .\\nYou may also need to increase num_train_epochs or learning_rate a little.'},\n",
       " '139_1': {'created_at': '2019-01-21T13:21:58Z',\n",
       "  'author': 'ironflood',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '139',\n",
       "  'text': \"Thanks for your suggestions, but when following them nothing changes, it always predict one class regardless.\\nTried:\\n\\n3 and 10 epochs\\ndifferent learning rates: 5e-3, 5e-4, 5e-5\\nall done with FLAG bert-base-uncased FLAG (but at the same time being able to use the multi language + Cased input would be an important plus)\\n\\nOne of the new command tested:\\n FILEPATH /\\nThe console output:\\n URL \\nThe rebalanced train & test dataset didn't change from first message and I checked its validity.\"},\n",
       " '139_2': {'created_at': '2019-01-21T14:45:50Z',\n",
       "  'author': 'kkadowa',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '139',\n",
       "  'text': \"I think 5e-3 and 5e-4 are too high; they are even higher than pre-training.\\nI'm getting eval_accuracy = VERSION (i.e. not one class) with FLAG 5e-5 FLAG VERSION (other arguments and dataset are same as yours), so I have no idea why you didn't.\"},\n",
       " '139_3': {'created_at': '2019-01-21T16:00:03Z',\n",
       "  'author': 'ironflood',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '139',\n",
       "  'text': \"Thanks for testing it out. I tried again 5e-5 and I'm getting close to your result as well. As I tried earlier this LR could it be that sometimes it doesn't converge properly because of random seed? Another question: testing out 5e-5 and 5e-6 on the cased multi language model I'm getting approx VERSION acc instead of VERSION with lowercased standard model on 3 and 10 epoch training, is it only because of the case adding more difficulty?\"},\n",
       " '139_4': {'created_at': '2019-01-21T16:30:31Z',\n",
       "  'author': 'kkadowa',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '139',\n",
       "  'text': 'No, random seed is always same. See this line in the code.\\nFor multilingual models, you should refer multilingual.md.'},\n",
       " '139_5': {'created_at': '2019-02-05T16:08:59Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '139',\n",
       "  'text': 'Closing since there is no recent activity. Feel free to re-open if needed.'},\n",
       " '140_0': {'created_at': '2019-01-28T10:33:15Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '140',\n",
       "  'text': 'Can you post a self-contained example to reproduce your error ? Which version of python, pytorch and pytorch-pretrained-bert are you using?'},\n",
       " '140_1': {'created_at': '2019-02-05T16:09:16Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '140',\n",
       "  'text': 'Closing since there is no recent activity. Feel free to re-open if needed.'},\n",
       " '140_2': {'created_at': '2019-02-12T23:09:48Z',\n",
       "  'author': 'ankit-ai',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '140',\n",
       "  'text': 'I ran into the same issue. Any pointers on how I could triage this further?'},\n",
       " '140_3': {'created_at': '2019-02-13T09:25:37Z',\n",
       "  'author': 'rahular',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '140',\n",
       "  'text': 'USER sorry, was busy with something else. Will post a self-sufficient example and possibly a PR with the fix soon.'},\n",
       " '141': {'created_at': '2019-01-19T06:55:30Z',\n",
       "  'author': 'jianyucai',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'URL \\nIt seems there should be a softmax after the linear layer, or did I miss something?'},\n",
       " '141_0': {'created_at': '2019-01-19T06:59:23Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '141',\n",
       "  'text': 'It depends on what you use as loss, as mentioned in the documentation:\\n\\nThis criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.'},\n",
       " '142': {'created_at': '2019-01-19T15:32:43Z',\n",
       "  'author': 'CaesarLuvAI',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'the above error arose when i ran the run_squad.py in pycharm(i just copied and ran locally). so can anbody tell how to input these two parameters \" FLAG \",\" FLAG \" in the IDE?'},\n",
       " '142_0': {'created_at': '2019-03-27T06:42:02Z',\n",
       "  'author': 'Vetrivel-PS',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '142',\n",
       "  'text': 'Check this site : URL and find your 2 Parameters \" FLAG \",\" FLAG \" . You\\'ll find the Below example \\nExample \\n FLAG : We can use BERT Models like : bert-base-uncased, bert-base-cased, bert-large-uncased, bert-large-cased, etc. All BERT models Refer : URL \\n FLAG : Output Directory in Local : C:/ FILEPATH \\n FLAG bert-base-uncased\\n-- FILEPATH /\\n\\nexport GLUE_DIR=/ FILEPATH \\npython run_classifier.py \\n FLAG MRPC \\n FLAG \\n FLAG \\n FLAG \\n FLAG $ FILEPATH / \\n FLAG bert-base-uncased \\n FLAG 128 \\n FLAG 32 \\n FLAG 2e-5 \\n FLAG VERSION \\n-- FILEPATH /'},\n",
       " '142_1': {'created_at': '2019-07-26T15:46:51Z',\n",
       "  'author': 'RyanBe123',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '142',\n",
       "  'text': 'What do you do specifically with the above? I have downloaded the BERT model, placed it in the same directory and am still getting this error.'},\n",
       " '143': {'created_at': '2019-01-19T16:02:47Z',\n",
       "  'author': 'semsevens',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'How convert pytorch to tf checkpoint?'},\n",
       " '143_0': {'created_at': '2019-01-21T03:19:20Z',\n",
       "  'author': 'nikitakit',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': \"I don't think such a conversion is currently implemented in this repository, but I have my own implementation here (if you're interested in adapting it for your use-case): URL\"},\n",
       " '143_1': {'created_at': '2019-01-28T10:32:13Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': 'Thanks USER , do you think your scripts would make sense in the present repo as well or is it tided to your parsing application?'},\n",
       " '143_2': {'created_at': '2019-02-05T16:09:48Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': 'Closing this for now. Feel free to re-open.'},\n",
       " '143_3': {'created_at': '2019-02-07T16:25:57Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': 'Would be actually quite nice to have such a conversion script in order to serve pytorch models via bert-as-service'},\n",
       " '143_4': {'created_at': '2019-03-15T13:09:24Z',\n",
       "  'author': 'maxlund',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': 'Any progress on this? As USER says, would be nice for bert-as-service. USER is it possible to run your script for any finetuned pytorch model? If so, FILEPATH ?'},\n",
       " '143_5': {'created_at': '2019-03-15T13:12:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': \"Re-opening the issue.\\nI don't have time to work on that at the moment but I would be happy to welcome a PR if there is interest in this feature.\"},\n",
       " '143_6': {'created_at': '2019-06-25T08:38:47Z',\n",
       "  'author': 'MarvinMogab',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '143',\n",
       "  'text': 'Any news on this ?'},\n",
       " '144': {'created_at': '2019-01-20T09:44:52Z',\n",
       "  'author': 'abril4416',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I try to install pytorch-bert using the command: pip install pytorch-pretrained-bert\\nHowever this doesn't work for me.\\nAnd the feedback is below:\\nCould not find a version that satisfies the requirement pytorch-pretrained-be\\nrt (from versions: )\\nNo matching distribution found for pytorch-pretrained-bert\\nI also tried to update my pip, but in vain.\\nSo how can I install bert? (Will the root of the issue be that I use python2.7?)\"},\n",
       " '144_0': {'created_at': '2019-01-20T22:53:34Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '144',\n",
       "  'text': \"Hi, most likely you'll have to switch to python VERSION or newer!\\n URL (check for requirements in the page: Requires: Python >= VERSION )\"},\n",
       " '144_1': {'created_at': '2019-02-05T16:10:10Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '144',\n",
       "  'text': 'Indeed'},\n",
       " '144_2': {'created_at': '2019-08-13T10:46:21Z',\n",
       "  'author': 'Chao-Tang',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '144',\n",
       "  'text': \"Need some help. I'm experiencing the same problem with Python VERSION \\nError code:\\nERROR: Could not find a version that satisfies the requirement torch>= VERSION (from pytorch-pretrained-bert== VERSION ) (from versions: VERSION , VERSION .post1, VERSION .post2)\\nERROR: No matching distribution found for torch>= VERSION (from pytorch-pretrained-bert== VERSION )\"},\n",
       " '144_3': {'created_at': '2019-08-19T13:28:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '144',\n",
       "  'text': 'Try installing pytorch first following the official instruction on the pytorch website: URL'},\n",
       " '145': {'created_at': '2019-01-21T07:04:08Z',\n",
       "  'author': 'susht3',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'on the examples, it loads bert-base model and do some tasks, the paper says that it will fix the parameters of bert and only update the parameters of our tasks, but i find that it seems not fix parameters of bert? just load the model, and adds some layers to train'},\n",
       " '145_0': {'created_at': '2019-01-21T09:03:20Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '145',\n",
       "  'text': \"Could you please show the part of the paper where you have seen mentioned, I haven't found it.\\nAre you talking about this paragraph?\\n\\nIn this section we evaluate how well BERT performs in the feature-based approach by generating ELMo-like pre-trained contextual representations on the CoNLL-2003 NER task. To do this, we use the same input representation as in Section VERSION , but use the activations from one or more layers with- out fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.\"},\n",
       " '145_1': {'created_at': '2019-02-05T16:10:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '145',\n",
       "  'text': 'Closing this since there no activity. Feel free to re-open if needed.'},\n",
       " '145_2': {'created_at': '2019-02-23T19:45:13Z',\n",
       "  'author': 'yuchenlin',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '145',\n",
       "  'text': 'i think it is asking whether we are fine-tuning the whole bert model or use bert outputs as a fixed feature for representing the sentences (like ELMO)'},\n",
       " '146': {'created_at': '2019-01-21T09:34:44Z',\n",
       "  'author': 'jianyucai',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi, I noticed that in the final linear layer of BertForQuestionAnswering, the loss is computed based on start_logits and end_logits . That means the positions of questions are also considered to compute loss. Maybe we should only care about the positions of context? e.g., by setting the question part of start_logits and end_logits to FLAG ?\\n URL'},\n",
       " '146_0': {'created_at': '2019-01-28T10:31:02Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '146',\n",
       "  'text': 'Yes you can also do that.'},\n",
       " '147': {'created_at': '2019-01-21T17:13:54Z',\n",
       "  'author': 'tgriseau',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI tried to fine tune BertForMaskedLM and it works. But i\\'m facing issues when I try to load the fine tuned model.\\nHere is the code I used to load the model :\\nmodel_state_dict = torch.load(\"./ FILEPATH \", map_location=\\'cpu\\')\\nmodel_fine = BertForMaskedLM.from_pretrained(pretrained_model_name=\\'bert-base-multilingual-cased\\', state_dict=model_state_dict, cache_dir=\\'./data\\')\\n\\nThe error I\\'m facing is : init() got an unexpected keyword argument \\'state_dict\\'\\nDoes someone already faced this issue ?\\nThanks\\nEdit : I trained my model on gpu and try to use it on cpu. When I use it on gpu it works !'},\n",
       " '147_0': {'created_at': '2019-01-22T17:00:53Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '147',\n",
       "  'text': 'Maybe update to a recent version of pytorch-pretrained-bert?'},\n",
       " '147_1': {'created_at': '2019-01-22T17:27:32Z',\n",
       "  'author': 'tgriseau',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '147',\n",
       "  'text': \"I'm already using the last release.\\nI don't have any issues running it on gpu. The problem append when using map_location\"},\n",
       " '147_2': {'created_at': '2019-01-22T17:33:55Z',\n",
       "  'author': 'MuruganR96',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '147',\n",
       "  'text': 'yeah yes. if you trained model in GPU, can\\'t be loaded. we will change map_location=\"CPU\" in modeling.py line 511.\\n URL \\nstate_dict = torch.load(weights_path, map_location=\"CPU\" )\\nnow it will load our finetuned model. this is only for inference running on CPU.'},\n",
       " '147_3': {'created_at': '2019-01-22T19:57:07Z',\n",
       "  'author': 'tgriseau',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '147',\n",
       "  'text': 'All good ! It works, thanks.\\nMaybe adding a device parameter to the function from_pretrained could be usefull.\\nThanks for your help.'},\n",
       " '147_4': {'created_at': '2019-01-23T22:12:30Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '147',\n",
       "  'text': 'Great, thanks USER'},\n",
       " '147_5': {'created_at': '2020-09-21T16:50:24Z',\n",
       "  'author': 'psureshmagadi17',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '147',\n",
       "  'text': 'USER - I want to fine-tune bert on MaskedLM using domain-specific text. could you please provide an example of how you fine-tuned or provide some details about what kind of inputs need to be passed? will I be using the true sentence as the output for fine-tuning?'},\n",
       " '148': {'created_at': '2019-01-22T18:14:52Z',\n",
       "  'author': 'satyakesav',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I am trying to run a classifier on the AGN data which has four classes. I am using the following command to train and evaluate the classifier.\\n FILEPATH \\n FLAG agn \\n FLAG \\n FLAG \\n FLAG \\n FLAG $ FILEPATH / \\n FLAG bert-base-uncased \\n FLAG 128 \\n FLAG 32 \\n FLAG 2e-5 \\n FLAG VERSION \\n-- FILEPATH /\\nI have created a task named agn similar to cola, mnli and others. The model is trained properly but during evaluation it throws the following error.\\n\\'\\'\\'\\n/ FILEPATH :105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [2,0,0] Assertion t >= 0 && t = 0 && t = 0 && t = 0 && t = 0 && t = 0 && t < n_classes failed.\\nTraceback (most recent call last):\\nFile \" FILEPATH \", line 690, in \\nmain()\\nFile \" FILEPATH \", line 663, in main\\nlogits = logits.detach().cpu().numpy()\\nRuntimeError: CUDA error: device-side assert triggered\\n\\'\\'\\'\\nThe reason for this issue is:\\nThe model is trained with output size of 4 (since four classes), but during testing the model has output size of 2 because the BertForSequenceClassification class has default value for num_labels as 2.\\nSo, if we change the following line in run_classifier.py\\nmodel = BertForSequenceClassification.from_pretrained(args.bert_model, state_dict=model_state_dict)\\nto\\nmodel = BertForSequenceClassification.from_pretrained(args.bert_model, state_dict=model_state_dict, num_labels=num_labels), the issue will be resolved.\\nPlease let me know If I can push the changes.'},\n",
       " '148_0': {'created_at': '2019-01-23T07:04:41Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '148',\n",
       "  'text': 'What version of pytorch-pretrained-BERT are you using?\\nIt seems to me that the change you are describing is already implemented.\\n URL'},\n",
       " '148_1': {'created_at': '2019-01-23T13:38:42Z',\n",
       "  'author': 'satyakesav',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '148',\n",
       "  'text': 'Okay. It is my bad that I did not have the latest version while debugging the issue. Thanks for pointing though. I will close the issue.'},\n",
       " '149': {'created_at': '2019-01-22T21:55:45Z',\n",
       "  'author': 'sebastianruder',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'issue',\n",
       "  'text': \"I am loading a pretrained BERT model with BertModel.from_pretrained as I feed the pooled_output representation directly to a loss without a head. After fine-tuning the model, I save it as in run_classifier.py.\\nAfterwards, I want to load the fine-tuned model, again without a head, so I'm using BertModel.from_pretrained model again to initialize it, this time from the directory where the config and model files are stored.\\nWhen trying to load the pretrained model, none of the weights are found and I get:\\nWeights of BertModel not initialized from pretrained model: ['bert.embeddings.word_embeddings.weight'\\n, 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert\\n.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self\\n.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', ...]\\n\\nThis seems to be due to this line in modeling.py. As BertModel.from_pretrained does not create a bert attribute (in contrast to the BertModels with a head), the bert. prefix is used erroneously instead of the '' prefix, which causes the weights of the fine-tuned model not to be found.\\nIf I change this line to check additionally if we load a fine-tuned model, then this works:\\nload(model, prefix='' if hasattr(model, 'bert') or pretrained_model_name not in PRETRAINED_MODEL_ARCHIVE_MAP else 'bert.')\\n\\nDoes this make sense? Let me know if I'm using BertModel.from_pretrained in the wrong way or if I should be using a different model for fine-tuning if I just care about the pooled_output representation.\"},\n",
       " '149_0': {'created_at': '2019-01-23T06:59:21Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '149',\n",
       "  'text': \"I think that you have find the problem but I'm not sure if your fix is the most appropriate way to deal with it. As this problem will only happen when we are loading a BertModel pretrained instance, maybe\\nload(model, prefix='' if hasattr(model, 'bert') or cls == BertModel else 'bert.')\\n\\nwould be more logical. Could you check if this change also fixes your problem?\"},\n",
       " '149_1': {'created_at': '2019-01-23T11:59:17Z',\n",
       "  'author': 'sebastianruder',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '149',\n",
       "  'text': \"The problem is that this only happens when we load a BertModel that was previously fine-tuned. If we load a pretrained BertModel, then the pretrained parameters don't have the bert. prefix, so we have to add it and it works. However, if we load the fine-tuned BertModel, then the parameters already have the bert. prefix, so we don't need to add it anymore. But this is not recognized at the moment.\\nSo the above change causes the loading of a pretrained BertModel to fail.\"},\n",
       " '149_4': {'created_at': '2019-01-23T18:25:16Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '149',\n",
       "  'text': \"I don't get the warnings that you are mentioning in your original post, the piece of code that I've created seems to fail for another reason. Could you please try to reproduce your original problem in a minimal piece of code?\"},\n",
       " '149_5': {'created_at': '2019-01-23T18:38:02Z',\n",
       "  'author': 'sebastianruder',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '149',\n",
       "  'text': \"That's the message printed by the logger here. You need to enable logging first. You can also just print the same message instead.\"},\n",
       " '149_7': {'created_at': '2019-01-24T04:57:24Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '149',\n",
       "  'text': \"Actually Sebastian, since the model you save and the model you load are instances of the same BertModel class, you can also simply use the standard PyTorch serialization practice (we only have a special from_pretrained loading function to be able to load various type of models using the same pre-trained model stored on AWS).\\nJust build a new BertModel using the configuration file you saved.\\nHere is a snippet :\\n# Saving (same as you did)\\nmodel_to_save = model_base.module if hasattr(model_base, 'module') else model_base\\ntorch.save(model_to_save.state_dict(), save_file)\\nwith open(config_file, 'w') as f:\\n f.write(model_base.config.to_json_string())\\n\\n# Loading (using standard PyTorch loading practice)\\nconfig = BertConfig(config_file)\\nmodel = BertModel(config)\\nmodel.load_state_dict(torch.load(save_file))\"},\n",
       " '149_8': {'created_at': '2019-01-24T09:42:11Z',\n",
       "  'author': 'sebastianruder',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '149',\n",
       "  'text': \"Thanks a lot for the comprehensive suggestions, USER . You're totally right that just loading it as normally in PyTorch is the most straightforward and simplest way. Your last suggestion works. Thanks!\"},\n",
       " '150': {'created_at': '2019-01-23T07:21:51Z',\n",
       "  'author': 'fenneccat',\n",
       "  'author_location': 'KR',\n",
       "  'type': 'issue',\n",
       "  'text': 'In evaluation step, it seems it only shows the predicted label for the data instance.\\nHow can I get the confidence score for each class?'},\n",
       " '150_0': {'created_at': '2019-01-23T07:36:01Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '150',\n",
       "  'text': 'You can use torch.nn.functional.softmax on the logits that the model outputs here:\\n URL \\nIt will give you the confidence score for each class.'},\n",
       " '151': {'created_at': '2019-01-23T08:19:56Z',\n",
       "  'author': 'schipiga',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hello folks! Can you provide simple example how to use pytorch bert with pretrained model for questions answering?'},\n",
       " '151_0': {'created_at': '2019-01-23T08:29:59Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'Hi!\\nYou can check this file that implements question answering on the SQuAD dataset: URL'},\n",
       " '151_1': {'created_at': '2019-01-23T08:32:15Z',\n",
       "  'author': 'schipiga',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'Thank you very much!'},\n",
       " '151_2': {'created_at': '2019-08-25T14:41:39Z',\n",
       "  'author': 'adilmukhtar82',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'How can we use pre trained BertForQuestionAnswering model? I have looked into BertForNextSentencePrediction and output of model makes sense given the input vector, but unable to find any good example on BertForQuestionAnswering.'},\n",
       " '151_3': {'created_at': '2019-08-26T21:25:41Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'Have you tried looking at the official documentation that provides a simple example for each model?'},\n",
       " '151_4': {'created_at': '2019-08-29T06:31:04Z',\n",
       "  'author': 'Arjunsankarlal',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'Hi USER , the official example was not clear to me. I understood the part of encoding. But I am looking for something like, I will give a question and a paragraph which would contain the answer, and I need the model to predict the answer span. But in the example they have done it with a single sentence, which is quite confusing!'},\n",
       " '151_5': {'created_at': '2019-08-29T10:27:41Z',\n",
       "  'author': 'Arjunsankarlal',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': \"Hey USER , Sorry my bad, didn't look at run_squad.py, it has been changed a lot since I saw it first during which BERT was only released! It is so good to see everything being integrated at a single place! Thanks for the great work you guys!\"},\n",
       " '151_6': {'created_at': '2019-08-29T18:34:35Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'USER Glad you could get what you were looking for!'},\n",
       " '151_7': {'created_at': '2019-08-29T19:11:46Z',\n",
       "  'author': 'adilmukhtar82',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'USER USER can you guys help me with the example. I got an error when I ran the example given in the documentation when encoding the sequence, that tokernizer doesn\\'t have attribute \"encode\". So I updated the code as follows:\\n`from pytorch_pretrained_bert import BertTokenizer, BertForQuestionAnswering\\nimport torch\\ntokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\nmodel = BertForQuestionAnswering.from_pretrained(\\'bert-base-uncased\\')\\ntokenized_text = tokenizer.tokenize(\"Hello, my dog is cute\")\\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\\ninput_ids = torch.tensor([indexed_tokens]) # Batch size 1\\nstart_positions = torch.tensor([1])\\nend_positions = torch.tensor([3])\\noutputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\\nprint(outputs)`\\nthis is the output\\ntensor( VERSION , grad_fn=)\\nI believe it\\'s a loss but I don\\'t understand the example as in how does it answer the question. Also there isn\\'t any start and end span. Can you please explain the example. Much appreciated.'},\n",
       " '151_8': {'created_at': '2019-08-29T20:05:37Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': \"Hi USER , could you give a look at the run_squad.py example, it shows how to use several models to do question answering.\\nYou should probably update your repository version to pytorch-transformers too, most of the examples on our documentation won't work with pytorch_pretrained_bert.\"},\n",
       " '151_9': {'created_at': '2019-08-30T09:08:40Z',\n",
       "  'author': 'adilmukhtar82',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'USER Thanks I have updated the repository and example is working fine. I am confused about the example mentioned in documentation (\"hello, my dog is cute\") as to how does it do with single sentence and not paragraph along with it.'},\n",
       " '151_11': {'created_at': '2020-05-10T12:54:56Z',\n",
       "  'author': 'mariusjohan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'USER Hey, could you maybe also provide a tensorflow example?'},\n",
       " '151_12': {'created_at': '2020-05-12T20:23:17Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'Thanks USER !\\n USER there are PyTorch and TensorFlow examples in the usage section of the documentation.'},\n",
       " '151_13': {'created_at': '2020-08-11T19:18:12Z',\n",
       "  'author': 'kmair',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'USER The link is now updated to URL'},\n",
       " '151_16': {'created_at': '2021-04-10T06:13:41Z',\n",
       "  'author': 'saurabhhssaurabh',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '151',\n",
       "  'text': 'Thank you, USER'},\n",
       " '152': {'created_at': '2019-01-23T10:26:54Z',\n",
       "  'author': 'gqoew',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI want to use BERT to train a QA model on a custom SQuAD-like dataset. Ideally, I would like to leverage the learning from the SQuAD dataset, and add fine-tuning on my custom dataset, which has specific vocabulary.\\nWhat is the best way to do this?'},\n",
       " '152_0': {'created_at': '2019-01-23T10:50:44Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '152',\n",
       "  'text': \"I think that you should start by pretraining a BERT model on SQuAD to give it a sense on how to perform question answering and then try finetuning it to your task. This may already give you good results, if it doesn't you might have to dig a bit deeper in the model.\\nI don't really know how adding your domain specific tokens to the vocabulary would interact with the tokenizer.\"},\n",
       " '152_1': {'created_at': '2019-01-28T10:30:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '152',\n",
       "  'text': 'One nice recent example is \"A BERT Baseline for the Natural Questions\" by Chris Alberti, Kenton Lee and Michael Collins from Google Research: URL'},\n",
       " '152_2': {'created_at': '2019-07-02T09:23:26Z',\n",
       "  'author': 'gqoew',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '152',\n",
       "  'text': 'This might help other dev who want to use BERT for custom QA: URL'},\n",
       " '153': {'created_at': '2019-01-24T02:01:16Z',\n",
       "  'author': 'renjunxiang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nalthough I have download BERT pretrained model, \"ConnectionError\" returned if my Internet network is not very stable.\\nFunction file_utils.cached_path needs stable internet.\\nIs there any way to avoid checking for amazonaws before loading bert-embedding?'},\n",
       " '153_0': {'created_at': '2019-01-25T10:22:02Z',\n",
       "  'author': 'cimeister',\n",
       "  'author_location': 'CH',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '153',\n",
       "  'text': 'I\\'m guessing you\\'re using some of the classes defined in modeling.py, such as one of the Bert \"pretrained models\" (e.g. any of the models that inherit from PreTrainedBertModel)? On construction, each of these classes takes a config argument, where config is a BertConfig object (also defined in modeling.py). The BertConfig can either be created from a model at one of the links in PRETRAINED_MODEL_ARCHIVE_MAP or from a config file stored in a local directory. You just have to set the pretrained_model_name to a local directory containing a bert_config.json file and a pytorch_model.bin file rather than one of \\'bert-base-uncased\\', \\'bert-large-uncased\\' etc. Setting pretrained_model_name to one of the latter options will try to pull from the Amazon AWS repositories. So if you\\'re running the run_classification.py script, you would set the \\'bert-model\\' flag to the directory with your downloaded bert model if you don\\'t want it to pull from AWS. One thing is if you\\'ve downloaded one of the original Google Bert models, you\\'ll need to convert tf checkpoints to pytorch bin files. There\\'s a script for this in the repository. You shouldn\\'t need to worry about this if you\\'ve downloaded one of the models at the links in PRETRAINED_MODEL_ARCHIVE_MAP (defined at the top of modeling.py)\\nTLDR: Set FLAG to the directory with your downloaded Bert model\\nDoes that make any sense?'},\n",
       " '153_1': {'created_at': '2019-01-28T02:53:25Z',\n",
       "  'author': 'renjunxiang',\n",
       "  'author_location': 'CN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '153',\n",
       "  'text': 'Yes! Set local directory in modeling.py and tokenization.py can solve my problem. Thank you so much!'},\n",
       " '153_2': {'created_at': '2019-01-28T10:29:08Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '153',\n",
       "  'text': 'Thanks USER !'},\n",
       " '154': {'created_at': '2019-01-24T02:42:38Z',\n",
       "  'author': 'hahmyg',\n",
       "  'author_location': 'KR',\n",
       "  'type': 'issue',\n",
       "  'text': 'for specific task, it is required to add new vocabulary for tokenizer.\\nIt is ok that re-training for those vocabulary for me :)\\nIs it possible to add new vocabulary for tokenizer?'},\n",
       " '154_0': {'created_at': '2019-01-24T05:13:10Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '154',\n",
       "  'text': 'Hi USER , please refer to the relevant section in the original implementation repository: URL'},\n",
       " '155': {'created_at': '2019-01-24T03:16:17Z',\n",
       "  'author': 'RayXu14',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'is there an max sentence length for this bert code?'},\n",
       " '155_0': {'created_at': '2019-01-24T05:11:14Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '155',\n",
       "  'text': 'Hi, 512 tokens if you use the pre-trained models. Any length you want if you train your models from scratch.'},\n",
       " '155_1': {'created_at': '2019-02-13T03:55:46Z',\n",
       "  'author': 'OswaldoBornemann',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '155',\n",
       "  'text': 'could we set it smaller ? cause if i set it as 512, then result is out of memory'},\n",
       " '155_2': {'created_at': '2019-02-13T08:19:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '155',\n",
       "  'text': 'You can just send a smaller input in the model, no need to go to the max'},\n",
       " '155_3': {'created_at': '2019-02-14T02:34:05Z',\n",
       "  'author': 'OswaldoBornemann',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '155',\n",
       "  'text': 'thank you USER'},\n",
       " '156': {'created_at': '2019-01-25T11:51:02Z',\n",
       "  'author': 'snakers4',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\n USER USER \\n USER USER \\nMany thanks for amazing work with this repository =)\\nI maybe grossly wrong or just missed some line of the code somewhere, FILEPATH , right?\\n\\nContext\\nI have already been able to fit the model to the Russian version of the SQUAD dataset from scratch (so-called SberSQUAD from sdsj 2017), and I was able to obtain ~40% FILEPATH , ~60% EM is about the top result on this dataset, achieved using BiDAF, so the model worksm which is good =).\\nAnyway this was a sanity check for me to see that the model is sound, obviously to achieve good results you need to pre-train first ( FILEPATH , right?).\\nSo now I am planning to pre-train BERT for the Russian language with various pre-processing ideas:\\n\\nBPE (like in the original);\\nEmbedding bag (works well for \"difficult\" languages) + ;\\n\\nThe Problem\\nFirst of all let\\'s quote the paper\\nIn order to train a deep bidirectional representation, we take a straightforward approach of masking \\nsome percentage of the input tokens at random, and then predicting only those masked tokens. \\n\\nWe refer to this procedure as a masked LM (MLM), although it is often referred to as a Cloze task in \\nthe literature (Taylor, 1953). In this case, the fi- nal hidden vectors corresponding to the mask tokens are\\n fed into an output softmax over the vo- cabulary, as in a standard LM. In all of our exper- iments, we \\nmask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders \\n(Vincent et al., 2008), we only pre- dict the masked words rather than reconstructing the entire input. \\n\\nSo as far as I can see:\\n\\n FILEPATH (afaik the masking scheme here is correct) and make the model correct our \"mistakes\". It only makes sense - we break the input, and the model corrects it;\\n\\nBut if you look here, here and here - it seems to me that in the code:\\n\\n FILEPATH ;\\nThe lm targets are the \"messed up\" tokens;\\n\\nSo, the training is kind of reversed.\\nThe correct sequence is passed, but the incorrect sequence is the target.\\nAnyway - I may just have missed some line of code, that changes everything.\\nI am just trying to understand the model properly, because I need to do a total rewrite of the pre-processing, because in my domain usage of embedding bags proved to be more beneficial than BPE.\\nMany thanks!'},\n",
       " '156_0': {'created_at': '2019-01-25T14:00:31Z',\n",
       "  'author': 'kkadowa',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '156',\n",
       "  'text': 'Hi, USER ,\\nI think this part is correct.\\nThe input comes from tokens and input_ids in line 371, FILEPATH , and the LM targets are lm_label_ids, which contain the original tokens.\\nNote that random_word, called in line 331 and 332, masks the words in tokens_a and tokens_b in-place; t1_random and tokens_a refer the same object actually.\\nIf you are trying to pre-train a model from scratch and having slow convergence issue, see discussions in ISSUE_REF .'},\n",
       " '156_1': {'created_at': '2019-01-25T14:35:18Z',\n",
       "  'author': 'snakers4',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '156',\n",
       "  'text': 'The input comes from tokens and input_ids in line 371, FILEPATH , and the LM targets are lm_label_ids, which contain the original tokens.\\n\\nAh, you are right, I see it here, sorry. I totally missed the in-place part.\\nThis bit explains why lm_label_ids are the original tokens.\\nThis was a bit counter-intuitive.\\nAnyway, thanks for the explanations, now everything is clear.'},\n",
       " '157_1': {'created_at': '2019-01-25T15:41:52Z',\n",
       "  'author': 'danyaljj',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '157',\n",
       "  'text': \"Ah I didn't realize they don't work in-place (unlike the syntax for model files model.to(device)).\"},\n",
       " '157_3': {'created_at': '2019-07-17T11:10:41Z',\n",
       "  'author': 'DreamInvoker',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '157',\n",
       "  'text': 'Great thanks!'},\n",
       " '158': {'created_at': '2019-01-26T09:09:36Z',\n",
       "  'author': 'ZhaofengWu',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"As I understand, say if I'm doing a classification task, then the transformer weights, along with the top classification layer weights, are both trainable (i.e. requires_grad=True), correct? If so, is there a way to freeze the transformer weights, but only train the top layer? Is that a good idea in general when I have a small dataset?\"},\n",
       " '158_0': {'created_at': '2019-01-26T09:28:41Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '158',\n",
       "  'text': 'Hi!\\nYou can modify the trainable attributes as described in ISSUE_REF .'},\n",
       " '159': {'created_at': '2019-01-26T11:01:42Z',\n",
       "  'author': 'ootts',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'If true, is there an example?'},\n",
       " '159_0': {'created_at': '2019-01-28T10:28:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '159',\n",
       "  'text': 'It is, check the nice recent work of Guillaume Lample and Alexis Conneau: URL'},\n",
       " '160': {'created_at': '2019-01-26T23:18:42Z',\n",
       "  'author': 'ZhaofengWu',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'What is inside ~/.pytorch_pretrained_bert? Is it just the downloaded pre-trained model weights? Is it safe to remove this directory?'},\n",
       " '160_0': {'created_at': '2019-01-28T10:13:10Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '160',\n",
       "  'text': 'This folder contains the pretrained model weights as they have been trained by google and the vocabulary files for the tokenizer.\\nI would not remove it unless you are really tight on disk space, in this case I guess you could only keep the .json files with the vocabulary and load your finetuned model.'},\n",
       " '160_1': {'created_at': '2019-01-28T10:27:04Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '160',\n",
       "  'text': 'Yes it contains the weights, configuration and vocabulary files. You can remove it if you want. In that case the weights will be downloaded again the next time you initiate a BertModel.'},\n",
       " '161': {'created_at': '2019-01-27T17:22:32Z',\n",
       "  'author': 'yamrzou',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi !\\nSorry if this is a dumb question, but I don't understand why is the bias added separately to the decoder weights instead of using self.decoder = nn.Linear(num_features, num_tokens, bias=True)? Isn't it equivalent?\"},\n",
       " '161_0': {'created_at': '2019-01-28T10:02:46Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '161',\n",
       "  'text': 'The code section you linked follows the original TensorFlow code: URL'},\n",
       " '162': {'created_at': '2019-01-28T13:19:06Z',\n",
       "  'author': 'kugwzk',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I use a Model based on BertModel, and when I use the BertAdam the learning rate isn't changed. And when I use get_lr(), the return result is [0]. And I see the length of state isn't 0, but why I get that?\"},\n",
       " '162_0': {'created_at': '2019-02-05T16:12:33Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '162',\n",
       "  'text': 'You can use it to get the current learning rate of the BertAdam optimizer (which vary according to the schedules discussed in ISSUE_REF ).'},\n",
       " '163': {'created_at': '2019-01-29T13:36:53Z',\n",
       "  'author': 'Alexadar',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi!\\n\\nHelp me please figure out, what would be optimal batch size for evaluating nextSentencePrediction model? For performance. Is it same as used during pre-training (128)?\\nIf i building high performance evaluating backend on CUDA, would it be a good idea to use several threads with bert model in each, or its better to use one thread with proper batching?'},\n",
       " '163_0': {'created_at': '2019-01-29T14:21:30Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': 'For evaluation I would advise the maximum batch size that your GPU allows. You will be able to use more efficiently this way.\\n\\nI think you will be better off by using a single thread.'},\n",
       " '163_1': {'created_at': '2019-01-29T15:25:23Z',\n",
       "  'author': 'Alexadar',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': 'Thanks! How can i figure out optimal batch size? I want to try tesla k80'},\n",
       " '163_2': {'created_at': '2019-01-29T15:27:05Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': 'You increase it gradually and when the program crashes, it is too big ^^.'},\n",
       " '163_4': {'created_at': '2019-01-29T21:43:56Z',\n",
       "  'author': 'Alexadar',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': 'Guys, sorry i reopen this issue, but it might be helpful and on topic of evaluation\\nI want to load batch of data into model for evaluation. Batch have size of 16 sentences of different length\\nCode:\\ntokens_tensor = torch.tensor(indexed_tokens)\\nsegments_tensors = torch.tensor(segments_ids)\\npredictions = model(tokens_tensor, segments_tensors)\\n\\nindexed_tokens are array of size 16 of arrays of inputs.\\nI got error\\nValueError: expected sequence of length 121 at dim 1 (got 23)\\nwhen i create tensor from a single element\\ntokens_tensor = torch.tensor([indexed_tokens[0]])\\nit works\\nWhat im doing wrong?\\nThanks!'},\n",
       " '163_5': {'created_at': '2019-01-30T06:39:03Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': 'Could you create of minimal program that reproduces your problem (with the code you are using to generate indexed_tokens)?'},\n",
       " '163_6': {'created_at': '2019-01-30T21:45:49Z',\n",
       "  'author': 'Alexadar',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': \"Tensor Input array should have same length for all rows. My sentences had various length. That's why pytorch raise exception\\nIf you add zeros to the end of input arrays, to make all rows equal, evaluation will be slower than one per sentence. Batching not improving speed.\"},\n",
       " '163_7': {'created_at': '2019-03-06T12:18:20Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': \"Hi USER , you have to batch your examples and pad them indeed. No other way I'm afraid.\"},\n",
       " '163_8': {'created_at': '2019-03-06T13:02:06Z',\n",
       "  'author': 'Alexadar',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '163',\n",
       "  'text': 'Sorry, i missed your post request for example.\\nYes, padding is only way to batch. It is slower than process sentencess one by one, i tested on GPU.'},\n",
       " '164': {'created_at': '2019-01-29T13:47:29Z',\n",
       "  'author': 'PeliconA',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"When I try to run BERT training, I get the following error during the vocabulary download:\\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: / FILEPATH (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 110] Connection timed out'))\\nI am running the script behind a proxy server which I suspect is the cause of this error. Is there any way to remedy this?\"},\n",
       " '164_0': {'created_at': '2019-02-05T16:13:25Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '164',\n",
       "  'text': \"You can download the Tensorflow weights from Google's BERT repo and convert them as detailed in the readme of the present repo.\"},\n",
       " '164_1': {'created_at': '2019-09-20T00:38:51Z',\n",
       "  'author': 'SaulML',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '164',\n",
       "  'text': \"Hi tomwolf,\\nI am new to XLNet and have the same issue as above.\\nCould you direct me to the readme of the issue? I am not able to find it.\\nI modified my code to resolve the issue to\\nmodel_file_address = '/ FILEPATH '\\nBut I get the below error on the line :\\nmodel = XLNetForSequenceClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))\\nError:\\nUnpicklingError: invalid load key, '{'.\\nI am pretty much stucked.\\nYour help will be appreciated.\\nThanks,\\nSaul\"},\n",
       " '164_3': {'created_at': '2019-10-08T08:20:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '164',\n",
       "  'text': 'You can now supply a proxies argument to from_pretrained when you are using proxies.\\nCheck the doc and docstrings.'},\n",
       " '164_4': {'created_at': '2020-01-24T21:48:01Z',\n",
       "  'author': 'avidale',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '164',\n",
       "  'text': 'Got the same error as USER and USER . Has anyone solved it?'},\n",
       " '164_5': {'created_at': '2020-10-18T11:04:12Z',\n",
       "  'author': 'ShivanshuPurohit',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '164',\n",
       "  'text': 'Got the same unpickling error. Was it solved?'},\n",
       " '164_6': {'created_at': '2021-05-05T15:01:13Z',\n",
       "  'author': 'acd424',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '164',\n",
       "  'text': 'I think you need to just have the path as the directory rather than the config file.'},\n",
       " '165': {'created_at': '2019-01-29T15:56:30Z',\n",
       "  'author': 'StalVars',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi, FILEPATH \\nCUDA out of memory. Tried to allocate VERSION MiB (GPU 5; VERSION GiB total capacity;\\nThis error seems to happen in pytorch when there are lengthier data points ( pytorch tells how much it tried to allocate as opposed to normal CUDA out of memory error).\\ntensorflow code for bert doesn' FILEPATH .\"},\n",
       " '165_0': {'created_at': '2019-01-29T16:03:27Z',\n",
       "  'author': 'StalVars',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '165',\n",
       "  'text': 'Ok. I see you included this. (max_sent_length, max_query_length)\\nI will debug my error.You probably can close this issue.'},\n",
       " '166_0': {'created_at': '2019-01-30T06:37:02Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'Hi,\\nIf you want to modify the vocabulary, you should refer to this part of the original repo README URL'},\n",
       " '166_1': {'created_at': '2019-01-30T07:17:30Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'If you don\\'t want a complete new vocabulary (which would require training from scratch), but extend the pretrained one with a couple of domain specific tokens, this comment from Jacob Devlin might help:\\n\\n[...] if you want to add more vocab you can either:\\n(a) Just replace the \"[unusedX]\" tokens with your vocabulary. Since these were not used they are effectively randomly initialized.\\n(b) Append it to the end of the vocab, and write a script which generates a new checkpoint that is identical to the pre-trained checkpoint, but but with a bigger vocab where the new embeddings are randomly initialized (for initialized we used tf.truncated_normal_initializer(stddev= VERSION )). This will likely require mucking around with some tf.concat() and tf.assign() calls.\\n\\n( FILEPATH #9)\\nI am currently experimenting with approach a). Since there are 993 unused tokens this might already help for the most important tokens in your domain.'},\n",
       " '166_2': {'created_at': '2019-02-05T16:14:19Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'USER and USER answers are the way to go.\\nClosing this issue since there no activity.\\nFeel free to re-open if needed.'},\n",
       " '166_3': {'created_at': '2019-03-20T21:42:40Z',\n",
       "  'author': 'chenshaolong',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'If you don\\'t want a complete new vocabulary (which would require training from scratch), but extend the pretrained one with a couple of domain specific tokens, this comment from Jacob Devlin might help:\\n\\n[...] if you want to add more vocab you can either:\\n(a) Just replace the \"[unusedX]\" tokens with your vocabulary. Since these were not used they are effectively randomly initialized.\\n(b) Append it to the end of the vocab, and write a script which generates a new checkpoint that is identical to the pre-trained checkpoint, but but with a bigger vocab where the new embeddings are randomly initialized (for initialized we used tf.truncated_normal_initializer(stddev= VERSION )). This will likely require mucking around with some tf.concat() and tf.assign() calls.\\n\\n( FILEPATH #9)\\nI am currently experimenting with approach a). Since there are 993 unused tokens this might already help for the most important tokens in your domain.\\n\\n USER I have exactly the same situation as you had. I\\'m wondering If you can tell me how your experiment with approach (a) went. Did it improve the accuracy. I really appreciate if you can share your conclusion.'},\n",
       " '166_4': {'created_at': '2019-10-03T13:07:05Z',\n",
       "  'author': 'vyraun',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'USER and USER answers are the way to go.\\nClosing this issue since there no activity.\\nFeel free to re-open if needed.\\n\\nHi USER , FILEPATH ?'},\n",
       " '166_5': {'created_at': '2020-01-07T22:59:10Z',\n",
       "  'author': 'sachinshinde1391',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'USER Can you guide me on how you are counting 993 unused tokens? I see only first 100 places of unused tokens?'},\n",
       " '166_6': {'created_at': '2023-07-17T18:54:15Z',\n",
       "  'author': 'aribenjamin',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '166',\n",
       "  'text': 'For those finding this on the web, I found the following answer helpful: ISSUE_REF (comment)'},\n",
       " '168_0': {'created_at': '2019-01-31T21:10:32Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '168',\n",
       "  'text': \"I see. This is because they didn't use the same names for the adam optimizer variables than the Google team. I'll see if I can find a simple way around this for future cases.\\nIn the mean time, you can install pytorch-pretrained-bert from the master (git clone ... and pip install FLAG .) and add the names of these variables (BERTAdam to the black-list line 53 in the conversion script: URL\"},\n",
       " '168_1': {'created_at': '2019-02-01T14:33:47Z',\n",
       "  'author': 'leanderloew',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '168',\n",
       "  'text': 'Hmm, loading bioberts parameters works for me. Mabye as a future feature we could have the option to load biobert parameters as an option in the package?\\nI ran it like this:\\nconvert_tf_checkpoint_to_pytorch(\" FILEPATH \",\\n \" FILEPATH \",\" FILEPATH \")\\n\\nit also loads afterwards.'},\n",
       " '168_2': {'created_at': '2019-02-18T11:12:25Z',\n",
       "  'author': 'MeRajat',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '168',\n",
       "  'text': 'This can help. URL'},\n",
       " '168_4': {'created_at': '2019-03-06T08:54:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '168',\n",
       "  'text': 'This is normal. Closing the issue now.'},\n",
       " '169_0': {'created_at': '2019-01-30T22:14:04Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '169',\n",
       "  'text': 'WordPiece tokenization depends on the particular BERT model: In general, one model, say, bert-based-cased will produce a different tokenization than another, say, bert-large-uncased.\\nIf you try all models, one or more might produce the tokenization shown in the example in the paper.\\nIt might also happen that none of them does, in which case the example was probably produced with an unpublished model.\\nA bug would be if the same model leads to different tokenizations in the pytorch and tensorflow version.'},\n",
       " '169_1': {'created_at': '2019-01-30T23:54:10Z',\n",
       "  'author': 'JohnGiorgi',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '169',\n",
       "  'text': \"Hmm, I see. I didn't know that, thanks for pointing it out!\\nFor what it's worth, bert-base-multilingual-cased is the only model (from those currently listed in the readme of this repo) that produces the tokenization shown in the example in the paper.\"},\n",
       " '170_2': {'created_at': '2019-01-31T10:47:34Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '170',\n",
       "  'text': 'Added a PR to simplify this.'},\n",
       " '170_3': {'created_at': '2019-01-31T11:05:39Z',\n",
       "  'author': 'imhuim982',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '170',\n",
       "  'text': \"Btw, would inplace mask pollute training data in 'on_memory' way?\"},\n",
       " '170_4': {'created_at': '2019-01-31T11:15:24Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '170',\n",
       "  'text': 'You mean if the original training sentence stored in train_dataset.all_docs get somehow modified (= masked)?!\\n=> No, FILEPATH'},\n",
       " '171': {'created_at': '2019-01-31T19:15:10Z',\n",
       "  'author': 'joelgrus',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': \"we've been getting some requests to incorporate newer features into allennlp that are only on master (e.g. never_split).\\nthanks!\"},\n",
       " '171_0': {'created_at': '2019-01-31T21:06:25Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '171',\n",
       "  'text': 'Hi Joel, yes the new release ( VERSION ) is pretty much ready (remaining work on branches fifth-release and transfo-xl to finish testing the newly added pre-trained OpenAI GPT and Transformer-XL).\\nLikely next week.'},\n",
       " '171_1': {'created_at': '2019-01-31T21:11:27Z',\n",
       "  'author': 'joelgrus',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '171',\n",
       "  'text': 'Awesome, thanks!\\n...\\nOn Thu, Jan 31, 2019, 11:06 AM Thomas Wolf ***@***.***> wrote:\\n Hi Joel, yes the new release ( VERSION ) is pretty much ready (remaining work\\n on branches fifth-release and transfo-xl to finish testing the newly\\n added pre-trained OpenAI GPT and Transformer-XL).\\n\\n Likely next week.\\n\\n \\n You are receiving this because you authored the thread.\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .'},\n",
       " '171_2': {'created_at': '2019-02-11T15:13:17Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '171',\n",
       "  'text': 'Ok USER , the new release is out: URL'},\n",
       " '172': {'created_at': '2019-02-01T15:48:37Z',\n",
       "  'author': 'nicolas-mng',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi,\\nI'm not sure I'm posting this at the right spot but I am trying to use your excellent implementation to do some multi label classification on some text. I basically adapted the run_classifier.py code to a Jupyter Notebook and change a little bit the BERT Sequence Classifier model so it can handle multilabel classification. However, my loss tends to diverge and my outputs are either all ones or all zeros.\\nThe labels distribution in my train dataset is :\\narray([ 65, 564, 108, 17, 40, 26, 306, 195, 25, 345, 54, 80, 214]) \\ni.e. the label 1 is used 65 times, the label 2 is used 564 times etc... Each sample has between 1 and 4 labels.\\nI am using the Adam Optimizer on the BCEWithLogitsLoss and I am unable to figure out where the problem comes from? Should I add some weights in my loss function? Do I use it in a right way? Is my model wrong somewhere? I attach to this post a Notebook of my test. Maybe someone encountered the same problem before and could help me?\\nNOTEBOOK\\nThanks !\"},\n",
       " '172_0': {'created_at': '2019-02-03T21:26:07Z',\n",
       "  'author': 'zhipeng-fan',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '172',\n",
       "  'text': \"Hey, I am working on something similar. I feel like the original code might be incorrect. They seem to directly take the output of the model as 'loss' without applying any criteria. But I might be totally wrong.\"},\n",
       " '172_1': {'created_at': '2019-02-03T22:34:38Z',\n",
       "  'author': 'nicolas-mng',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '172',\n",
       "  'text': \"Hey! :)\\n\\nReally? What do you mean by criteria?\\n\\nI tried to artificially change my dataset so that the target outputs are\\n[1, 0, 0,..., 0] for every sample. I wanted to see whether the model was\\nable to learn this dummy case. However, it fails and it predicts the exact\\nopposite, i.e. [0, 1, 1,.... 1]. That's why I think I must be doing\\nsomething wrong somewhere.\"},\n",
       " '172_2': {'created_at': '2019-06-05T06:28:19Z',\n",
       "  'author': 'Saurabh7',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '172',\n",
       "  'text': 'Hi USER USER Did you guys manage to get a multilabel problem to work ? Could you please share a gist ?'},\n",
       " '173': {'created_at': '2019-02-03T06:08:52Z',\n",
       "  'author': 'MathewAlexander',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'I just wanted to ask if the weights of the Bert base model are getting updated while fine tuning Bert for Question answering. I see that the Bert for QA is a model with A linear layer on top of Bert pre-trained model. I am trying to reproduce the same model in keras. Could any one tell me if i should freeze the layers in the Bert base model or not?'},\n",
       " '173_0': {'created_at': '2019-02-04T06:10:26Z',\n",
       "  'author': 'rodgzilla',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '173',\n",
       "  'text': 'The weights of the BERT base model are getting updated while finetuning the network.'},\n",
       " '173_1': {'created_at': '2019-02-06T08:07:50Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '173',\n",
       "  'text': 'Indeed'},\n",
       " '174': {'created_at': '2019-02-05T18:39:37Z',\n",
       "  'author': 'danyaljj',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Just a clarification question:\\nwhen tuning bert parameters (say, for SQUAD), does it tune the parameters of the final parameter or the whole BERT model?'},\n",
       " '174_0': {'created_at': '2019-02-06T08:07:13Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '174',\n",
       "  'text': 'In the examples scripts, it tunes the whole model.\\nBut BERT models classes are just regular PyTorch nn.Modules so you can also freeze layer like you would do in any PyTorch module.'},\n",
       " '175_0': {'created_at': '2019-02-06T08:05:59Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '175',\n",
       "  'text': 'Hi USER ,\\nYou need to install apex with the C++ and CUDA extensions:\\ngit clone URL \\ncd apex\\npython setup.py install FLAG FLAG'},\n",
       " '175_1': {'created_at': '2019-02-06T19:35:21Z',\n",
       "  'author': 'chenyangh',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '175',\n",
       "  'text': 'USER \\nThanks!'},\n",
       " '175_2': {'created_at': '2019-05-06T09:41:58Z',\n",
       "  'author': 'kbulutozler',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '175',\n",
       "  'text': 'USER After doing what you wrote, I got this error.\\ntorch.version = VERSION .post2\\nTraceback (most recent call last):\\nFile \"setup.py\", line 60, in \\nraise RuntimeError(\" FLAG was requested, but nvcc was not found. Are you sure your environment has nvcc available? If you\\'re installing within a container from URL only images whose names contain \\'devel\\' will provide nvcc.\")\\nRuntimeError: FLAG was requested, but nvcc was not found. Are you sure your environment has nvcc available? If you\\'re installing within a container from URL only images whose names contain \\'devel\\' will provide nvcc.\\nWhat else should I do for this?'},\n",
       " '175_3': {'created_at': '2019-05-06T10:13:53Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '175',\n",
       "  'text': \"You should refer to apex installation instructions.\\nApex has slightly changed since my comment so best is to go read NVIDIA's README and installation instructions here: URL\"},\n",
       " '176': {'created_at': '2019-02-06T01:57:28Z',\n",
       "  'author': 'bsugerman',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I'm looking through this code (thanks so much for writing it, btw) and I'm not seeing whether it actually uses eval_batch_size at all. If it doesn't, is it still performing an evaluation step to assess goodness of fit?\"},\n",
       " '176_0': {'created_at': '2019-02-06T07:56:02Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '176',\n",
       "  'text': \"No, there's no evaluation step in the example script yet. What I can recommend is using your downstream task for evaluation of the pretrained BERT. Alternatively, FILEPATH .\"},\n",
       " '176_1': {'created_at': '2019-02-06T19:12:49Z',\n",
       "  'author': 'bsugerman',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '176',\n",
       "  'text': 'Perhaps for clarity then, that parameter should be taken out of the script?'},\n",
       " '176_2': {'created_at': '2019-02-07T09:10:16Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '176',\n",
       "  'text': 'Sure, makes sense. I created a PR. Thanks for pointing this out USER .'},\n",
       " '176_3': {'created_at': '2019-03-06T08:55:13Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '176',\n",
       "  'text': 'Fixed in master now.'},\n",
       " '177': {'created_at': '2019-02-06T12:44:46Z',\n",
       "  'author': 'mttk',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"this is a major nitpick but it was a bit confusing at first:\\n URL \\nL212 can simply be replaced by self.all_head_size = config.hidden_size as you already error out if the result of division isn't a whole number.\"},\n",
       " '177_0': {'created_at': '2019-03-06T08:56:01Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '177',\n",
       "  'text': \"Yes, feel free to submit a PR. Otherwise, I'll fix it in the next release.\"},\n",
       " '178': {'created_at': '2019-02-06T23:16:04Z',\n",
       "  'author': 'bsugerman',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"I fine-tuned the pytorch_model.bin on a GPU machine (google cloud) but need to use it on my home computer (no GPU). When I tried to open it using model = BertForMaskedLM.from_pretrained(bert_version) I got the following error:\\n RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() \\n is False. If you are running on a CPU-only machine, please use torch.load with map_location='cpu'\\n to map your storages to the CPU.\\n\\nPerhaps you can add an option into from_pretrained() such as cpu=True which will then call\\ntorch.load(weights_path, map_location=lambda storage, location: 'cpu')\"},\n",
       " '178_0': {'created_at': '2019-02-08T09:37:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'Indeed, this will be in the next release, thanks!'},\n",
       " '178_1': {'created_at': '2019-06-12T07:02:15Z',\n",
       "  'author': 'ahmedbahaaeldin',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'is there is a change according the CPU issue ?'},\n",
       " '178_2': {'created_at': '2019-06-12T08:05:01Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'Should be fixed now. Do you still have an error?'},\n",
       " '178_3': {'created_at': '2019-06-12T13:23:30Z',\n",
       "  'author': 'ahmedbahaaeldin',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'i am loading the model and i dont know how to load on CPU , gives me \"model.to\" to is not defined. Can you tell me how to send the model to run on CPU if trained on GPU.'},\n",
       " '178_4': {'created_at': '2020-08-05T07:54:25Z',\n",
       "  'author': 'CBvanYperen',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'Would love some explanation on how to do this as well!'},\n",
       " '178_5': {'created_at': '2020-08-20T20:53:17Z',\n",
       "  'author': 'Zhen-hao',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'I have the same question. how to load a model trained on GPU to CPU?'},\n",
       " '178_6': {'created_at': '2020-09-17T18:40:19Z',\n",
       "  'author': 'ariellasmo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'I am watching this as well.'},\n",
       " '178_7': {'created_at': '2021-01-16T12:30:32Z',\n",
       "  'author': 'poriniki',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'I have the same question. Is there any option for using CPU-only?'},\n",
       " '178_8': {'created_at': '2022-03-24T12:21:34Z',\n",
       "  'author': 'davidsbatista',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '178',\n",
       "  'text': 'watching this as well'},\n",
       " '179': {'created_at': '2019-02-07T00:55:12Z',\n",
       "  'author': 'WilliamTambellini',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'hi, FILEPATH ?\\n URL \\nkind'},\n",
       " '179_0': {'created_at': '2019-02-11T15:47:36Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': 'Hi USER , have you tried to follow the standard ONNX procedure for converting a PyTorch model?\\nThe model in this repo are just regular PyTorch models.'},\n",
       " '179_1': {'created_at': '2019-02-22T21:43:16Z',\n",
       "  'author': 'WilliamTambellini',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': 'Hello Thomas, I ve not yet tried, just seen :\\n ISSUE_REF \\n URL \\nWill try, tks.'},\n",
       " '179_2': {'created_at': '2019-04-26T04:35:54Z',\n",
       "  'author': 'geekboood',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': \"Hi, when I try to export a TokenClassification model to a ONNX model, I encounter RuntimeError: ONNX export failed: Couldn't export operator aten::erf, does that mean some part of BERT model layers not supported by ONNX?\\nI think that problem comes from the definition of GELU function, which is x * VERSION * ( VERSION + torch.erf( FILEPATH ( VERSION ))). Should I try to use other way to calculate this function or wait for ONNX to support this opertator?\"},\n",
       " '179_3': {'created_at': '2019-05-23T03:01:53Z',\n",
       "  'author': 'maeotaku',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': 'USER update your pytorch version to latest and the problem will most likely go away.'},\n",
       " '179_5': {'created_at': '2019-08-21T08:58:36Z',\n",
       "  'author': 'paulachocron',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': 'for anyone who is looking for the answer:\\ntorch= VERSION \\npython= VERSION \\ntorch.onnx.export(model, (input_ids, segment_ids, input_mask), \"bert.onnx\", verbose=False)\\nworks well for me\\n\\nHi, thanks for the answer. Do you get good results when using the exported model for inference in another framework? I exported a BertForQuestionAnswering model to ONNX without errors, but I\\'m getting wrong predictions when using onnxruntime or a second export to TF Serving and I can\\'t figure out why!'},\n",
       " '179_6': {'created_at': '2019-09-18T01:58:43Z',\n",
       "  'author': 'chessgecko',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': 'Not sure if this is still an issue for you but in the BertForSequenceClassification model the parameters are in a different order\\ntorch.onnx.export(model, (input_ids, input_mask, segment_ids), \"bert.onnx\", verbose=False)\\nworks as intended'},\n",
       " '179_7': {'created_at': '2019-10-09T10:21:11Z',\n",
       "  'author': 'paulachocron',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '179',\n",
       "  'text': \"USER wow you're right, thanks! working now\"},\n",
       " '180_0': {'created_at': '2019-09-04T08:52:03Z',\n",
       "  'author': 'ArthurCamara',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': \"I know this is closed, but I'm running into a similar issue. For the first ~100 batches, the script runs with an OK speed ( FILEPATH , batch size 64, 512 tokens, 8x GTX 1080Ti) (specific for DistilBertForSequenceClassification in my case).\\nAfter that, the speed drops significantly, FILEPATH , with the GPUs mostly sitting idle. (0% on gpustat.\\nAny idea on what could be causing this?\"},\n",
       " '180_1': {'created_at': '2019-09-05T08:06:16Z',\n",
       "  'author': 'ArthurCamara',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': 'Update: Looks like I was accumulating the gradient for too long.'},\n",
       " '180_2': {'created_at': '2020-10-28T10:43:31Z',\n",
       "  'author': 'ilya-palachev',\n",
       "  'author_location': 'RU',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': 'Update: Looks like I was accumulating the gradient for too long.\\n\\n USER So, how did you fix the problem in your case? Did you change the gradient_accumulation_steps parameter of Trainer? What were the initial value and the value which helped to resolve the problem?\\nAnd how did you understand that the problem was really in this?'},\n",
       " '180_3': {'created_at': '2020-10-28T12:42:06Z',\n",
       "  'author': 'ilya-palachev',\n",
       "  'author_location': 'RU',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': \"I have the same problem. I noticed that the training speed slows down as GPU temperature goes up... When the temperature goes down (if I wait after terminating the process), the speed becomes okay again.\\nThis issue happens only when I use Trainer. When I don't use it (i.e. use PyTorch utilities directly), the training speed is stable and the temperature doesn't go up.\\n USER Did you fix your issue?\"},\n",
       " '180_4': {'created_at': '2020-10-30T11:19:01Z',\n",
       "  'author': 'ArthurCamara',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': 'Update: Looks like I was accumulating the gradient for too long.\\n\\n USER So, how did you fix the problem in your case? Did you change the gradient_accumulation_steps parameter of Trainer? What were the initial value and the value which helped to resolve the problem?\\nAnd how did you understand that the problem was really in this?\\n\\nI think the initial setting was 5 or something. I dropped to 1 and it was fine then.'},\n",
       " '180_5': {'created_at': '2020-10-30T12:47:48Z',\n",
       "  'author': 'ilya-palachev',\n",
       "  'author_location': 'RU',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': 'Update: Looks like I was accumulating the gradient for too long.\\n\\n USER So, how did you fix the problem in your case? Did you change the gradient_accumulation_steps parameter of Trainer? What were the initial value and the value which helped to resolve the problem?\\nAnd how did you understand that the problem was really in this?\\n\\nI think the initial setting was 5 or something. I dropped to 1 and it was fine then.\\n\\nAs I see now the default value is 1, but I still observe the slow down. USER Do you have any idea on that?'},\n",
       " '180_6': {'created_at': '2020-11-11T09:34:16Z',\n",
       "  'author': 'ArthurCamara',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '180',\n",
       "  'text': \"Update: Looks like I was accumulating the gradient for too long.\\n\\n USER So, how did you fix the problem in your case? Did you change the gradient_accumulation_steps parameter of Trainer? What were the initial value and the value which helped to resolve the problem?\\nAnd how did you understand that the problem was really in this?\\n\\nI think the initial setting was 5 or something. I dropped to 1 and it was fine then.\\n\\nAs I see now the default value is 1, but I still observe the slow down. USER Do you have any idea on that?\\n\\nI was not using the Trainer, but my own training loop. dropping the accumulation steps to 1 helped because it was overwhelming the GPUs memory and that makes the GPUs sit idly. If the GPUs on nvidia-smi are idle, but their memory is full, it's probably something related to memory usage. Otherwise, no idea.\"},\n",
       " '181': {'created_at': '2019-02-08T22:11:49Z',\n",
       "  'author': 'jayleicn',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\ntoken_type_ids is not set for this line:\\nall_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\\n URL this does not affect single sequence feature extraction, but for a pair of sequence, the model will process the pair as a single sequence and add A embedding to the two sequences, which should add A, B respectively. Seems like a bug.\\nBest,\\nJie'},\n",
       " '181_0': {'created_at': '2019-02-11T15:45:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '181',\n",
       "  'text': 'Hi Jie,\\nextract_feature.py is an example script. If you want to adapt it for sentences-pair, we would be happy to welcome a PR :)'},\n",
       " '182_0': {'created_at': '2019-02-11T15:38:22Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '182',\n",
       "  'text': 'reduce batch size?'},\n",
       " '182_1': {'created_at': '2019-02-11T16:12:06Z',\n",
       "  'author': 'laibamehnaz',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '182',\n",
       "  'text': 'It is 32 as of now . What do you think I should reduce it to ?'},\n",
       " '182_2': {'created_at': '2019-02-11T16:15:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '182',\n",
       "  'text': 'Start very low and increase while looking at nvidia-smi or a similar GPU memory visualization tool.'},\n",
       " '182_3': {'created_at': '2019-03-06T08:57:51Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '182',\n",
       "  'text': 'Closing this for now, feel free to re-open if you have other issues.'},\n",
       " '183': {'created_at': '2019-02-11T13:01:38Z',\n",
       "  'author': 'carolinlawrence',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nwhen I change the FLAG argument, I get a high variance between different runs on my dataset. So I was wondering where the sources of variance might come from. I see that the seed is set (e.g. in run_squad.py) via:\\nrandom.seed(args.seed) np.random.seed(args.seed) torch.manual_seed(args.seed)\\nBut how can I find out where randomization is actually used?\\nI found the RandomSampler and replaced it with a SequentialSampler, but the variance remains high.\\nI know that modeling.py randomly initializes the weights, but these are overwritten by the fixed weights when loading a pre-trained BERT model, e.g. bert-base-uncased, correct?\\nCan anyone point me in any other direction where my source of variance might come from?\\nThanks!'},\n",
       " '183_0': {'created_at': '2019-02-11T15:43:39Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '183',\n",
       "  'text': \"Hi Carolin,\\nDepending on the model you are using, not all the weights are initialized from the pre-trained models. Check the details in the overview section of the readme to see if it's the case for you.\\nApart from weights initialization and dataset shuffling other typical source of variances are the dropout layers.\\nBert fine-tuning has been reported to be a high-variance process indeed, in particular on small datasets.\"},\n",
       " '183_1': {'created_at': '2019-02-12T10:06:10Z',\n",
       "  'author': 'carolinlawrence',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '183',\n",
       "  'text': \"Hi Thomas,\\nthanks for the quick reply!\\nI'm using BertForMaskedLM, so the weights should be set. But yes, I didn't think of dropout, thanks for pointing that out!\"},\n",
       " '184_1': {'created_at': '2019-02-12T19:09:04Z',\n",
       "  'author': 'bsobolik',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '184',\n",
       "  'text': 'I think the tokenizer issue has been resolved in the latest version ( VERSION ).'},\n",
       " '184_4': {'created_at': '2019-12-05T15:26:07Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '184',\n",
       "  'text': 'What is your issue USER ? Do you mind opening a new issue with your problem?'},\n",
       " '184_5': {'created_at': '2019-12-05T15:36:25Z',\n",
       "  'author': 'yenicelik',\n",
       "  'author_location': 'CH',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '184',\n",
       "  'text': 'ah, my apologies: ISSUE_REF \\napparently a PR is on the way!'},\n",
       " '185': {'created_at': '2019-02-12T09:15:46Z',\n",
       "  'author': 'stefan-it',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nthanks so much for the new VERSION release. I wanted to train a TransfoXLModel model, as described in the README here.\\nUnfortunately, the files transfo_xl_train.py and transfo_xl_eval.py are not located in the examples directory.\\nCould you please add them to repository? Thanks'},\n",
       " '185_0': {'created_at': '2019-02-12T09:25:24Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '185',\n",
       "  'text': \"Oh yes, that was a typo, there is only one example for Transformer-XL and it' FILEPATH .\\nI've updated the readme, thanks.\"},\n",
       " '186': {'created_at': '2019-02-12T16:25:18Z',\n",
       "  'author': 'hugochan',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nThank you for supporting the pretrained Transformer-XL model! I was wondering if it makes sense to get hidden states from all layers of Transformer-XL as the output, just as what can be done for BERT. It seems this is not supported currently. Practically I found this strategy worked well for BERT and gave better results. Not sure if it is a good idea for Transformer-XL. Thank you!'},\n",
       " '186_0': {'created_at': '2019-02-13T09:33:48Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '186',\n",
       "  'text': \"Hi USER , actually that what's in the mems of the Transformer-XL are (maybe you can read again the paper).\\nOne thing to be careful about is that the mems have transposed first dimensions and are longer (see the readme). Here is how to extract the hidden states from the model output:\\nhidden_states, mems = model(tokens_tensor)\\nseq_length = hidden_states.size(1)\\nlower_hidden_states = list(t[ FLAG :, ...].transpose(0, 1) for t in mems)\\nall_hidden_states = lower_hidden_states + [hidden_states]\"},\n",
       " '186_1': {'created_at': '2019-02-14T01:39:09Z',\n",
       "  'author': 'hugochan',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '186',\n",
       "  'text': 'Hi USER , thank you for your answer! Just one quick question. It seems that mems already contains a list of num_layer hidden states, what is the difference between lower_hidden_states[-1] and hidden_states in your code? Thank you!'},\n",
       " '186_2': {'created_at': '2019-02-14T07:34:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '186',\n",
       "  'text': 'Actually mems contains all the hidden states PLUS the output of the embeddings (lower_hidden_states[0]) so lower_hidden_states[-1] is the output of the hidden state of the layer below the last layer and hidden_states is the output of the last layer (before the softmax).\\nI will add a note on that in the readme.'},\n",
       " '187_1': {'created_at': '2019-02-13T08:45:48Z',\n",
       "  'author': 'ZhuoranLyu',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '187',\n",
       "  'text': 'Same situation, on GPU.'},\n",
       " '187_2': {'created_at': '2019-02-13T08:46:19Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '187',\n",
       "  'text': \"Indeed, there was a bug in the loading of the TransfoXLModel from the S3 dump (which is a converted TransfoXLLMHeadModel) so the weights were not loaded.\\nYou can see that the weights are not loaded if you activate the logger before loading the model:\\nimport logging\\nlogging.basicConfig(level=logging.INFO)\\nI've fixed it in release VERSION .\\nI've also fixed another issue you ( USER ) mentions in ISSUE_REF which is the dependency of OpenAIGPTTokenizer on SpaCy and ftfy by adding a fallback on BERT's BasicTokenizer (should be fine for normal usage, SpaCy+ftfy were included to exactly reproduce the paper's pre-processing steps).\"},\n",
       " '187_3': {'created_at': '2019-02-13T08:47:00Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '187',\n",
       "  'text': 'Publishing VERSION as soon as all the tests are checked.'},\n",
       " '187_4': {'created_at': '2019-02-13T09:27:14Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '187',\n",
       "  'text': 'Ok VERSION is published: URL'},\n",
       " '188_1': {'created_at': '2019-02-13T08:15:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '188',\n",
       "  'text': 'which version of python are you using?'},\n",
       " '188_3': {'created_at': '2019-03-06T08:59:07Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '188',\n",
       "  'text': 'Maybe indeed. Do you want to submit a PR to fix this?'},\n",
       " '189': {'created_at': '2019-02-13T02:45:55Z',\n",
       "  'author': 'tuhinjubcse',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': 'So my LM sample.txt is such that each doc has only one line\\nSo in BERTDataSet len is giving negative\\nI tried changing it to self.num_docs - 1\\ndef len(self):\\nprint(self.corpus_lines ,self.num_docs)\\nreturn self.corpus_lines - self.num_docs - 1\\nI am also getting errors at multiple steps, Is the code written with the assumption that each document will have multiple lines in it?'},\n",
       " '189_0': {'created_at': '2019-02-13T06:21:43Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '189',\n",
       "  'text': 'Yes, you need documents with multiple lines because only sentences from the same doc are used as positive examples for the nextSentence prediction.'},\n",
       " '189_1': {'created_at': '2019-03-06T09:00:20Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '189',\n",
       "  'text': 'Seems like the expected behavior. Feel free to open a PR to extend the example if you want USER .'},\n",
       " '190': {'created_at': '2019-02-13T09:50:51Z',\n",
       "  'author': 'moonblue333',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I am trying on text8 dataset. I want to print next token. The model in source code forward() output is loss, but I want to get logits and softmax result, and finally get next token in vocab.\\n FILEPATH , on text8?\\nThanks'},\n",
       " '190_0': {'created_at': '2019-02-13T15:00:45Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '190',\n",
       "  'text': 'Hi,\\nThere is no pretrained character-level model for text8 right now.\\nOnly a word-level model trained on wikitext 103.'},\n",
       " '191': {'created_at': '2019-02-13T15:29:35Z',\n",
       "  'author': 'dileep1996',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi, I am trying to finetune LM and am facing the following issue.\\nargparse.ArgumentError: argument FLAG : conflicting option string: FLAG'},\n",
       " '191_0': {'created_at': '2019-02-13T15:33:05Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '191',\n",
       "  'text': 'Hi USER , this has just been fixed in master (#275)!'},\n",
       " '192': {'created_at': '2019-02-13T15:48:43Z',\n",
       "  'author': 'gqoew',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'issue',\n",
       "  'text': 'I just fine-tuned BERT-base on the SQuAD dataset with an AWS EC2 p3.2xlarge Deep Learning AMI with a single Tesla V100 16GB:\\nI used the config in your README:\\nexport SQUAD_DIR=/ FILEPATH \\n\\npython run_squad.py \\\\\\n FLAG bert-base-uncased \\\\\\n FLAG \\\\\\n FLAG \\\\\\n FLAG \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG $ FILEPATH \\\\\\n FLAG 12 \\\\\\n FLAG 3e-5 \\\\\\n FLAG VERSION \\\\\\n FLAG 384 \\\\\\n FLAG 128 \\\\\\n -- FILEPATH /\\n\\nIt took 80min. According to your README:\\n\\nThis example code fine-tunes BERT on the SQuAD dataset. It runs in 24 min (with BERT-base) or 68 min (with BERT-large) on a single tesla V100 16GB.\\n\\nHow to explain this difference? Is there any way to accelerate the training to 24min as well? Thanks'},\n",
       " '192_0': {'created_at': '2019-02-13T16:01:13Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '192',\n",
       "  'text': 'You should use 16bit training ( FLAG argument). You can use the dynamic loss scaling or tune the loss scale yourself if the results are not the best.'},\n",
       " '192_1': {'created_at': '2019-02-15T14:10:40Z',\n",
       "  'author': 'gqoew',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '192',\n",
       "  'text': 'USER Thanks! FILEPATH ?'},\n",
       " '192_2': {'created_at': '2019-03-06T09:00:50Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '192',\n",
       "  'text': 'Sounds good.'},\n",
       " '192_3': {'created_at': '2019-06-20T02:59:46Z',\n",
       "  'author': 'pachiko',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '192',\n",
       "  'text': 'USER \\nMay I know what is the expected EM & F1 score if users train for 2-3 epochs? I got 43 and 48 respectively.'},\n",
       " '192_4': {'created_at': '2019-06-20T06:50:36Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '192',\n",
       "  'text': 'You can have a look at the readme examples but it should be a lot higher, around 88-90.\\nMaybe your batch size is too small, look at the readme for more information.'},\n",
       " '193': {'created_at': '2019-02-14T00:57:02Z',\n",
       "  'author': 'juditacs',\n",
       "  'author_location': 'HU',\n",
       "  'type': 'issue',\n",
       "  'text': 'Adding [PAD] symbols to an input sentence changes the output of the model. I put together a small example here:\\n URL \\nI also noticed that the seed state affects the output as well. Resetting it in every run ensures that the output is always the same. Is this because of layernorm?'},\n",
       " '193_0': {'created_at': '2019-02-14T07:32:29Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '193',\n",
       "  'text': \"Hi Judit:\\n\\nRegarding the padding: you should send an attention_mask with the input if the input is smaller than the tensor you are sending in (see the description on BertModel in the README).\\nRegarding the seed: don't forget to put your model in eval mode (model.eval()) to disable the dropout layers.\"},\n",
       " '193_2': {'created_at': '2019-08-20T12:19:41Z',\n",
       "  'author': 'atulkakrana',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '193',\n",
       "  'text': \"I am having same problem and couldn't find a reason or fix yet.\"},\n",
       " '193_3': {'created_at': '2019-09-04T12:01:55Z',\n",
       "  'author': 'HarmC',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '193',\n",
       "  'text': 'Due to Position Embeddings every token results in different vectors.\\nYou might want to google \"How the Embedding Layers in BERT Were Implemented\"'},\n",
       " '193_4': {'created_at': '2020-07-14T10:54:03Z',\n",
       "  'author': 'MFajcik',\n",
       "  'author_location': 'CZ',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '193',\n",
       "  'text': 'Due to Position Embeddings every token results in different vectors.\\n\\nCould you be more specific what is the source of this numerical instability? Perhaps refer to exact code? I am still not exactly sure why output changes slightly when using attention mask, when I use differently padded inputs. There should be no self-attention over padded inputs. Self-attention scores are set to large negative number before softmax:\\nattention_scores = attention_scores + attention_mask\\nCould it be that sometimes -10_000 might not be enough to get 0 from softmax? I have recorded differences at most in the order of 2e-6.\\nOr is it because of arithmetic errors? According to URL upped bound for the relative error in 32bit format is somewhere at 1.19e-07, which is still an order away. Could that be because of the error propagation through many FP32 operations?'},\n",
       " '194': {'created_at': '2019-02-14T06:06:38Z',\n",
       "  'author': 'hongkahjun',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'issue',\n",
       "  'text': 'Simialr to this issue: URL when I run run_lm_finetuning.py using 4 GPUs on Microsoft Azure, the first GPU will have 4000MB Memory usage while the other 3 are at 700MB. The Volatile Util for the first GPU also is at 100% while the rest are at 0%.\\nIt seems that the solution might be something to do with incorporating the loss calculation in the forward pass but I do not know how to solve it.'},\n",
       " '194_1': {'created_at': '2019-03-06T09:01:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '194',\n",
       "  'text': \"Yes, there is no mechanism to balance memory in the examples. In NVIDIA's tests, it didn't help.\"},\n",
       " '195': {'created_at': '2019-02-15T03:39:14Z',\n",
       "  'author': 'guotong1988',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Thank you very much!'},\n",
       " '196': {'created_at': '2019-02-15T07:52:25Z',\n",
       "  'author': 'lahwran',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hey! This seems like something a lot of folks will want. I'd like to be able to load GPT-2 117M and fine-tune it. What's necessary to convert it? I looked at the tensorflow code a little and it looks vaguely related to transformer xl, but I haven't looked at the paper yet or etc.\"},\n",
       " '196_0': {'created_at': '2019-02-15T13:04:58Z',\n",
       "  'author': 'oguzserbetci',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '196',\n",
       "  'text': \"I'd like to help out on this. I will have a look and try to understand the earlier bridges in this repo. Let me know if you see anywhere a newcomer can be helpful with.\"},\n",
       " '196_1': {'created_at': '2019-02-15T13:11:31Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '196',\n",
       "  'text': \"Sure, would be happy to welcome a PR.\\nYou can start from modeling_openai.py and tokenization_openai.py's codes.\\nIt's pretty much the same architecture (read OpenAI's paper first!)\\nYou should mostly reimplement the BPE tokenization to work byte-level and move the layer norms modules to the input rather than the output of the layers.\"},\n",
       " '196_2': {'created_at': '2019-02-18T10:15:15Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '196',\n",
       "  'text': 'Ok, GPT-2 should be in the coming VERSION release (see ISSUE_REF )'},\n",
       " '196_3': {'created_at': '2019-02-18T10:43:00Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '196',\n",
       "  'text': \"Ok it's on pip: URL \\nPlease read the updated README for details. All should be there (model and examples).\\nHave a nice week y'all.\"},\n",
       " '196_4': {'created_at': '2019-03-11T05:53:11Z',\n",
       "  'author': 'AbhimanyuAryan',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '196',\n",
       "  'text': 'USER do we have pytorch implementation of GPT-2 small?'},\n",
       " '196_5': {'created_at': '2019-03-11T08:04:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '196',\n",
       "  'text': 'Yes, just read the README'},\n",
       " '197': {'created_at': '2019-02-15T11:21:08Z',\n",
       "  'author': 'elensergwork',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'The general run_squad.py doesn\\'t appear to work properly for python VERSION because of the json dumping string vs unicode issues during the eval.\\npython2.7 run_squad.py \\n FLAG bert-base-uncased \\n FLAG \\n FLAG \\n FLAG \\n FLAG $ FILEPATH \\n FLAG $ FILEPATH \\n FLAG 12 \\n FLAG 3e-5 \\n FLAG VERSION \\n FLAG 384 \\n FLAG 128 \\n-- FILEPATH /\\n FILEPATH :55:17 - INFO - main - Writing predictions to: / FILEPATH \\n FILEPATH :55:17 - INFO - main - Writing nbest to: / FILEPATH \\nTraceback (most recent call last):\\nFile \"run_squad.py\", line 1077, in \\nmain()\\nFile \"run_squad.py\", line 1073, in main\\nargs.version_2_with_negative, args.null_score_diff_threshold)\\nFile \"run_squad.py\", line 619, in write_predictions\\nwriter.write(json.dumps(all_predictions, indent=4) + \"\\\\n\")\\nTypeError: write() argument 1 must be unicode, not str'},\n",
       " '197_0': {'created_at': '2019-02-16T13:49:30Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '197',\n",
       "  'text': \"Yes, the examples are not adapted for Python 2, only the library.\\nI don't plan to adapt or maintain them but feel free to submit a PR!\"},\n",
       " '198_0': {'created_at': '2019-02-20T15:46:50Z',\n",
       "  'author': 'Hyperparticle',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '198',\n",
       "  'text': 'This was an error in apex, due to mismatched compiled libraries. Fix can be found here.\\n\\nTry a full pip uninstall apex, then cd apex_repo_dir; rm-rf build; python setup.py install FLAG FLAG and see if the segfault persists.'},\n",
       " '199_0': {'created_at': '2019-03-06T09:01:56Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '199',\n",
       "  'text': 'Closing for now.'},\n",
       " '199_2': {'created_at': '2019-03-06T12:13:18Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '199',\n",
       "  'text': \"I'm trying to clean up the issues to get a better view of what needs to be fixed.\\n FILEPATH 's add labels instead.\"},\n",
       " '200': {'created_at': '2019-02-18T11:27:20Z',\n",
       "  'author': 'davidefiocco',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'issue',\n",
       "  'text': 'Thought to flag, also given the terrific work on this repo (and others), that the company name in the code here seems to be systematically spelt wrong (?)\\n URL'},\n",
       " '200_0': {'created_at': '2019-03-06T09:02:34Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '200',\n",
       "  'text': \"Thanks, I'll fix that in a future release.\"},\n",
       " '201': {'created_at': '2019-02-18T16:30:54Z',\n",
       "  'author': 'mahnerak',\n",
       "  'author_location': 'GB',\n",
       "  'type': 'issue',\n",
       "  'text': 'As a library, it is preferred to have no unnecessary prints in the repo. Using the pytorch-pretrained-BERT makes it impossible to use stdout as main output mechanism for my code.\\nFor example: it prints directly to stdout \"Better speed can be achieved with apex installed from URL \\nI guess printing this kind of messages to stderr will be a better idea.'},\n",
       " '201_0': {'created_at': '2019-02-18T16:32:19Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '201',\n",
       "  'text': \"Oh that's right, this one should be a logging.info event like the other ones.\"},\n",
       " '201_1': {'created_at': '2019-03-06T09:05:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '201',\n",
       "  'text': 'Fixed'},\n",
       " '202': {'created_at': '2019-02-19T10:36:26Z',\n",
       "  'author': 'danlou',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi everyone,\\nI'm interested in extracting token-level embeddings from the pre-trained GPT2 and Transformer-XL models and noticed that extract_features.py seems to be specific to BERT.\\nCan you let us know if you have any plans to provide a similar implementation for models other than BERT?\\nAlternatively, could you possibly provide some hints for us to extract the token-level embeddings the code you already made available with the models?\\nMany thanks, great work!\"},\n",
       " '202_0': {'created_at': '2019-02-20T07:59:58Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '202',\n",
       "  'text': \"Hi Dan,\\nYou can extract all the hidden-states of Transformer-XL using the snippet indicated in the readme here.\\nFor the GPT-2 it's not possible right now.\\nI can add it in the next release (or you can submit a PR).\"},\n",
       " '202_1': {'created_at': '2020-03-13T01:52:09Z',\n",
       "  'author': 'shamanez',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '202',\n",
       "  'text': 'USER Can we extract final hidden layer representations from GPT-2 VERSION billion models now?'},\n",
       " '203_0': {'created_at': '2019-05-09T15:20:22Z',\n",
       "  'author': 'MeenaAlfons',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': 'I have the same question. I need to change the hidden_dropout_prob. How is that possible?'},\n",
       " '203_1': {'created_at': '2021-04-03T13:59:04Z',\n",
       "  'author': 'Opdoop',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': 'Same question. Any best practice?'},\n",
       " '203_3': {'created_at': '2021-07-30T11:42:24Z',\n",
       "  'author': 'kaankork',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': \"Hi USER - this code overrides the config parameters of the pertained BERT model. Did I understand correctly?\\nAlso, how can we ensure that the num_labels parameter is also updated? I don't see it in the output of print(model).\"},\n",
       " '203_4': {'created_at': '2021-08-03T07:29:20Z',\n",
       "  'author': 'Opdoop',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': \"USER \\n\\nFor override:\\nYes. Your understand is correct.\\nFor num_labels:\\nIt's been a long time. I didn't sure. But you should see num_labels by the last layer shape in print(model) .\"},\n",
       " '203_5': {'created_at': '2021-08-04T08:12:23Z',\n",
       "  'author': 'pratikchhapolika',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': 'Any update on this?'},\n",
       " '203_6': {'created_at': '2021-11-23T11:15:05Z',\n",
       "  'author': 'arkhan19',\n",
       "  'author_location': 'NL',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': 'I need to pass some values to config as well, will save me a lot of time....'},\n",
       " '203_7': {'created_at': '2022-07-08T03:34:30Z',\n",
       "  'author': 'guoyang9',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '203',\n",
       "  'text': \"Just use the update method.\\nFor example, if you want to change the number of hidden layers, simply use config.update({'num_hidden_layers': 1}).\"},\n",
       " '204': {'created_at': '2019-02-19T22:09:33Z',\n",
       "  'author': 'fabiocapsouza',\n",
       "  'author_location': 'BR',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI am fine-tuning BERT model (based on BertForTokenClassification) to a NER task with 9 labels (\"O\" + BILU tags for 2 classes) and sometimes during training I run into this odd behavior: a network with 99% accuracy that is showing a converging trend suddenly shifts all of its predictions to a single class. This happens during the interval of a single epoch.\\nBelow are the confusion matrices and some other metrics one epoch before the event and after the event:\\n FILEPATH : FILEPATH , val_acc= VERSION % ( FILEPATH ), val_acc_bilu= VERSION % ( FILEPATH ), val_rec= VERSION %, val_prec= VERSION %, val_f1= VERSION %\\nConfusion matrix:\\n [[53229 2 66 25 2 25 8]\\n [ 0 7 0 7 0 0 0]\\n [ 0 0 14 0 0 0 0]\\n [ 0 0 0 67 0 0 1]\\n [ 1 0 0 3 11 0 1]\\n [ 1 0 1 0 0 14 0]\\n [ 0 0 0 7 1 0 49]]\\n\\n FILEPATH : FILEPATH , val_acc= VERSION % ( FILEPATH ), val_acc_bilu= VERSION % ( FILEPATH ), val_rec= VERSION %, val_prec= VERSION %, val_f1= VERSION %\\nConfusion matrix:\\n [[ 0 0 0 0 53357 0 0]\\n [ 0 0 0 0 14 0 0]\\n [ 0 0 0 0 14 0 0]\\n [ 0 0 0 0 68 0 0]\\n [ 0 0 0 0 16 0 0]\\n [ 0 0 0 0 16 0 0]\\n [ 0 0 0 0 57 0 0]]\\n\\nI am using the default configs for bert-base-multilingual-cased and standard CrossEntropyLoss. The optimizer is BertAdam untouched with learning rate 1e-5. The dataset is highly unbalanced (very few named entities, so >99% of the tokens are \"O\" tags), so I use a weight of VERSION to the \"O\" tag in CE.\\nHas anyone faced a similar issue?\\nThanks in advance'},\n",
       " '204_0': {'created_at': '2019-02-20T19:26:32Z',\n",
       "  'author': 'fabiocapsouza',\n",
       "  'author_location': 'BR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '204',\n",
       "  'text': \"I manage to solve this problem. There is an issue in the calculation of the total optimization steps in run_squad.py example that results in a negative learning rate because of the warmup_linear schedule. This happens because t_total is calculated based on len(train_examples) instead of len(train_features). That may not be a problem for datasets with short sentences, but, for long sentences, one example may generate many entries in train_features due to the strategy of dividing an example in DocSpan's.\"},\n",
       " '204_1': {'created_at': '2019-09-23T15:42:41Z',\n",
       "  'author': 'MendesSP',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '204',\n",
       "  'text': 'USER I am trying to handle text classification but my dataset is also highly unbalanced. I am trying to find where I can adjust the class weights when training transformers. Which parameter you changed in your case?'},\n",
       " '204_2': {'created_at': '2019-09-23T16:48:06Z',\n",
       "  'author': 'fabiocapsouza',\n",
       "  'author_location': 'BR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '204',\n",
       "  'text': 'USER , since the provided BERT model classes have the loss function hardcoded in the forwardmethod, I had to write a subclass to override the CrossEntropyLoss definition passing a weight tensor.'},\n",
       " '205': {'created_at': '2019-02-19T22:49:57Z',\n",
       "  'author': 'stefan-it',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI wanted to convert the TensorFlow checkpoint for the lm1b model to PyTorch with the convert_transfo_xl_checkpoint_to_pytorch.py script.\\nI downloaded the checkpoint with the download.sh script.\\nThen I called the convert script with:\\n$ python3 convert_transfo_xl_checkpoint_to_pytorch.py FLAG converted FLAG \\n/ FILEPATH \\nThen the following error message is returned:\\n2019-02-19 22:46: VERSION : FILEPATH :95] FILEPATH : Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\\nTraceback (most recent call last):\\n File \"convert_transfo_xl_checkpoint_to_pytorch.py\", line 116, in \\n args.transfo_xl_dataset_file)\\n File \"convert_transfo_xl_checkpoint_to_pytorch.py\", line 81, in convert_transfo_xl_checkpoint_to_pytorch\\n model = load_tf_weights_in_transfo_xl(model, config, tf_path)\\n File \"/ FILEPATH \", line 141, in load_tf_weights_in_transfo_xl\\n init_vars = tf.train.list_variables(tf_path)\\n File \"/ FILEPATH \", line 95, in list_variables\\n reader = load_checkpoint(ckpt_dir_or_file)\\n File \"/ FILEPATH \", line 64, in load_checkpoint\\n return pywrap_tensorflow.NewCheckpointReader(filename)\\n File \"/ FILEPATH \", line 382, in NewCheckpointReader\\n return CheckpointReader(compat.as_bytes(filepattern), status)\\n File \"/ FILEPATH \", line 548, in __exit__\\n c_api.TF_GetCode(self.status.status))\\ntensorflow.python.framework.errors_impl.DataLossError: FILEPATH : Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\\nI\\'m using the VERSION version of pytorch-pretrained-BERT and the latest tf-nightly-gpu package that ships TensorFlow 1.13dev.'},\n",
       " '205_2': {'created_at': '2019-02-20T07:49:46Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': 'Hi Stefan,\\nYou have to create the configuration yourself indeed \\nI usually do it by looking at the training parameters of the Tensorflow code related to the model you are trying to load.'},\n",
       " '205_4': {'created_at': '2019-02-20T14:34:42Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': \"That's probably a question of weights or projection tying, try to set tie_weight or proj_share_all_but_first to False ( FILEPATH ).\\n(I can convert this model later if you don't manage to but not before next week unfortunately)\"},\n",
       " '205_5': {'created_at': '2019-02-20T22:32:08Z',\n",
       "  'author': 'stefan-it',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': \"Thanks for your help USER ! I'll try to find the correct configuration settings.\\nWe are currently trying to integrate the Transformer-XL model into flair, and we would really like to use a larger (in terms of training size) model for downstream tasks like NER :)\"},\n",
       " '205_7': {'created_at': '2019-03-06T09:05:55Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': 'Did you manage to convert this model USER ?'},\n",
       " '205_8': {'created_at': '2019-03-06T09:32:30Z',\n",
       "  'author': 'stefan-it',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': \"Sadly, I couldn't managed to convert it (I tried several options)\"},\n",
       " '205_9': {'created_at': '2019-12-06T04:26:13Z',\n",
       "  'author': 'irugina',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': 'USER did you ever manage to convert this model?'},\n",
       " '205_10': {'created_at': '2019-12-09T23:16:21Z',\n",
       "  'author': 'stefan-it',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '205',\n",
       "  'text': \"Hi USER , unfortunately, I wasn't able to convert the model\"},\n",
       " '206': {'created_at': '2019-02-20T01:11:52Z',\n",
       "  'author': 'g-karthik',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': \"Steps to reproduce:\\n\\nClone the repo.\\nSet up a plain virtual environment venv for the repo with Python VERSION .\\nRun pip install . (using the [ FLAG ] didn't work and there was some error, so I just removed it)\\nRun pip install spacy ftfy== VERSION and python FLAG spacy download en -- SUCCESSFUL.\\nRun pip install pytest -- SUCCESSFUL.\\n FILEPATH .\\n\\n FILEPATH ::GPT2ModelTest::test_config_to_json_string PASSED\\n FILEPATH ::GPT2ModelTest::test_default dyld: lazy symbol binding failed: Symbol not found: _PySlice_Unpack\\n Referenced from: / FILEPATH \\n Expected in: flat namespace\\n\\ndyld: Symbol not found: _PySlice_Unpack\\n Referenced from: / FILEPATH \\n Expected in: flat namespace\\n\\nAbort trap: 6\\n\\nIn case it helps, these are the packages in my venv after I perform the first 5 steps.\\natomicwrites | VERSION | VERSION \\nattrs | VERSION | VERSION \\nboto3 | VERSION | VERSION \\nbotocore | VERSION | VERSION \\ncertifi | VERSION | VERSION \\nchardet | VERSION | VERSION \\ncymem | VERSION | VERSION \\ncytoolz | VERSION | VERSION \\ndill | VERSION | VERSION \\ndocutils | VERSION | VERSION \\nen-core-web-sm | VERSION | \\nftfy | VERSION | VERSION \\nhtml5lib | VERSION | VERSION \\nidna | VERSION | VERSION \\njmespath | VERSION | VERSION \\nmore-itertools | VERSION | VERSION \\nmsgpack | VERSION | VERSION \\nmsgpack-numpy | VERSION | VERSION \\nmurmurhash | VERSION | VERSION \\nnumpy | VERSION | VERSION \\npip | VERSION | VERSION \\nplac | VERSION | VERSION \\npluggy | VERSION | VERSION \\npreshed | VERSION | VERSION \\npy | VERSION | VERSION \\npytest | VERSION | VERSION \\npython-dateutil | VERSION | VERSION \\npytorch-pretrained-bert | VERSION | VERSION \\nregex | VERSION | VERSION \\nrequests | VERSION | VERSION \\ns3transfer | VERSION | VERSION \\nsetuptools | VERSION | VERSION \\nsix | VERSION | VERSION \\nspacy | VERSION | VERSION \\nthinc | VERSION | VERSION \\ntoolz | VERSION | VERSION \\ntorch | VERSION .post2 | VERSION .post2\\ntqdm | VERSION | VERSION \\nujson | VERSION | VERSION \\nurllib3 | VERSION | VERSION \\nwcwidth | VERSION | VERSION \\nwebencodings | VERSION | VERSION \\nwrapt | VERSION | VERSION\"},\n",
       " '206_0': {'created_at': '2019-02-20T01:43:48Z',\n",
       "  'author': 'g-karthik',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '206',\n",
       "  'text': \"Update:\\nI manually uninstalled PyTorch (torch | VERSION .post2 | VERSION .post2 in the list above) from my venv and installed PyTorch VERSION (I couldn't find the whl file for VERSION for Mac OS, which is my platform) by running\\npip install URL \\nI then ran the tests and I saw that 25 tests passed.\\nSo I have just a couple questions at this point:\\n\\nGiven that the requirements.txt file states that the expected torch version is >= VERSION , and the above observation that 25 tests passed with torch VERSION , would all the code in this repo work with VERSION ?\\n\\nIt is clear that the error described above occurs when using the latest version of torch, specifically torch VERSION .post2, but not with VERSION . Could you please make the necessary changes to your repo to support VERSION .post2?\\n\\nThanks!\"},\n",
       " '206_1': {'created_at': '2019-02-20T02:17:47Z',\n",
       "  'author': 'g-karthik',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '206',\n",
       "  'text': 'Update:\\nWas able to find the whl for VERSION , so I uninstalled VERSION from my venv and installed VERSION . Ran the tests again and 25 tests passed. So question 1 above is irrelevant now. Could I have an answer for question 2?'},\n",
       " '206_2': {'created_at': '2019-02-20T07:54:22Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '206',\n",
       "  'text': \"Hi Karthik,\\nOn my fresh install with pytorch VERSION .post2 (python VERSION ) all 25 tests pass without error (also on the continuous integration by the way).\\nMaybe try to create a clean environment? You don't have to install pytorch prior to pytorch-pretrained-bert, it's a dependency.\"},\n",
       " '206_3': {'created_at': '2019-02-20T23:44:45Z',\n",
       "  'author': 'g-karthik',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '206',\n",
       "  'text': \"Hi Thomas,\\nI did start off with a clean virtual environment and I didn't install PyTorch prior to pytorch-pretrained-bert because I saw it's a dependency. The only difference I see between what you've described above and what I did is the version of Python: you used VERSION while I used VERSION . Maybe that has something to do with this? Could you try with Python VERSION ?\"},\n",
       " '206_4': {'created_at': '2019-03-06T09:11:51Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '206',\n",
       "  'text': 'Tested on a clean python VERSION install and all the tests pass.\\nHonestly there is not much more I can do at this stage.\\nClosing for now. Feel free to re-open if you find something.'},\n",
       " '207': {'created_at': '2019-02-20T02:56:09Z',\n",
       "  'author': 'SinghJasdeep',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': \"Hey! Sorry if this is redundant.\\nI saw 3 other issues asking similar questions but couldn't find these exact layers mentioned.\\nAre bert.pooler.dense.weight & bert.pooler.dense.bias randomly initialized?\\nThank you so much!\"},\n",
       " '207_0': {'created_at': '2019-02-20T07:56:09Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '207',\n",
       "  'text': \"Hi Jasdeep,\\nNo, they are initialized from Google's pretrained model (they are trained for next sentence prediction task during pretraining).\"},\n",
       " '208_0': {'created_at': '2019-02-20T07:57:14Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '208',\n",
       "  'text': 'Hi Ben,\\nPlease read the relevant example section in the readme.'},\n",
       " '208_1': {'created_at': '2019-06-12T06:24:46Z',\n",
       "  'author': 'abs51295',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '208',\n",
       "  'text': 'USER I see that the data is downloaded and cached in case of not providing the train_dataset and eval_dataset parameters: URL but it fails here: URL So either we make the parameters required or do some additional steps which requires untar of the downloaded data and pointing towards that data. Let me know what you think and I can send a PR.'},\n",
       " '209_0': {'created_at': '2019-02-20T15:11:28Z',\n",
       "  'author': 'stefan-it',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '209',\n",
       "  'text': 'This could be related to ISSUE_REF - are you using the latest version of pytorch-pretrained-BERT?'},\n",
       " '209_1': {'created_at': '2019-02-20T15:43:19Z',\n",
       "  'author': 'blester125',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '209',\n",
       "  'text': 'No, I was on VERSION , I upgraded to VERSION and it worked.'},\n",
       " '210': {'created_at': '2019-02-20T18:24:41Z',\n",
       "  'author': 'graykode',\n",
       "  'author_location': 'KR',\n",
       "  'type': 'issue',\n",
       "  'text': \"USER \\nI am trying simply implementing gpt-2 on Pytorch\\nI have trouble in trasfering tensorflow checkpoint to pytorch :(\\n URL \\nCould I do this code reference in implementing my code? I'll write reference in my code!!\\nThanks for awesome sharing!\"},\n",
       " '210_0': {'created_at': '2019-02-21T08:26:03Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '210',\n",
       "  'text': 'Hi USER ,\\nWhat do you mean by \"code reference\"?'},\n",
       " '210_1': {'created_at': '2019-02-21T08:29:55Z',\n",
       "  'author': 'graykode',\n",
       "  'author_location': 'KR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '210',\n",
       "  'text': 'USER \\nHello thomwolf!\\nIt mean that I apply your code about GPT-2 model and transferring tensorflow checkpoint to pytorch in my project!\\nI show the origin of the information in my project code comment when I refer to your code.\\nThanks'},\n",
       " '210_2': {'created_at': '2019-02-21T08:43:28Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '210',\n",
       "  'text': 'Oh yes, no problem.\\nJust reference the origin of the work and the licences (inherited from the relevant authors and code I started from)'},\n",
       " '210_3': {'created_at': '2019-02-21T08:45:41Z',\n",
       "  'author': 'graykode',\n",
       "  'author_location': 'KR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '210',\n",
       "  'text': 'USER Sure, What license i follow? FILEPATH , but pytorch-pretrained-BERT is Apache VERSION !\\nI want to just use gpt-2 model and model transferring code!!'},\n",
       " '211': {'created_at': '2019-02-21T02:33:50Z',\n",
       "  'author': 'weiczhu',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nWe are using your brilliant project for working on the Japanese BERT model with Sentence Piece.\\n URL \\nWe are trying to use the convert to to convert below TF BERT model to PyTorch.\\n URL \\nBut we see error logs:\\nTraceback (most recent call last):\\nFile \"/ FILEPATH \", line 66, in \\nargs.pytorch_dump_path)\\nFile \"/ FILEPATH \", line 37, in convert_tf_checkpoint_to_pytorch\\nload_tf_weights_in_bert(model, tf_checkpoint_path)\\nFile \"/ FILEPATH \", line 95, in load_tf_weights_in_bert\\npointer = getattr(pointer, l[0])\\nFile \"/ FILEPATH \", line 535, in getattr\\ntype(self).name, name))\\nAttributeError: \\'BertForPreTraining\\' object has no attribute \\'global_step\\'\\nCould you kindly help with how we can avoid this?\\nThank you so much!'},\n",
       " '211_0': {'created_at': '2019-02-21T02:52:30Z',\n",
       "  'author': 'weiczhu',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'I resolved this issue by adding the global_step to the skipping list. I think global_step is not required for using pretrained model. Please correct me if I am wrong.'},\n",
       " '211_1': {'created_at': '2019-02-27T15:40:57Z',\n",
       "  'author': 'naga-dsalgo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'Is Pytorch requires a TF check point converted? am finding hard to load the checkpoint I generated.BTW is it safe to convert TF checkpoint ?'},\n",
       " '211_2': {'created_at': '2019-02-27T15:41:40Z',\n",
       "  'author': 'naga-dsalgo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'I resolved this issue by adding the global_step to the skipping list. I think global_step is not required for using pretrained model. Please correct me if I am wrong.\\n\\ncan you explain me what is skipping list?'},\n",
       " '211_4': {'created_at': '2019-03-18T18:36:35Z',\n",
       "  'author': 'naga-dsalgo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'Is it possible to load Tensorflow checkpoint using pytorch and do fine tunning?\\nI can load pytorch_model.bin and finding hard to load my TF checkpoint.Documentation says it can load a archive with bert_config.json and model.chkpt but I have bert_model_ckpt.data-0000-of-00001 in my TF checkpoint folder so am confused. Is there specific example how to do this?'},\n",
       " '211_5': {'created_at': '2019-03-18T20:35:50Z',\n",
       "  'author': 'maxlund',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'There is a conversion script to convert a tf checkpoint to pytorch: URL'},\n",
       " '211_6': {'created_at': '2019-03-30T19:14:14Z',\n",
       "  'author': 'naga-dsalgo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'In the file modeling.py add it to the list at:\\nif any(n in [\"adam_v\", \"adam_m\"] for n in name):\\n\\nadded global_step in skipping list but still getting same issue.'},\n",
       " '211_7': {'created_at': '2019-04-02T08:37:38Z',\n",
       "  'author': 'shivamakhauri04',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'USER Is it fixed? I too added \"global_step\" to the list. But still get the error'},\n",
       " '211_8': {'created_at': '2019-05-16T14:06:38Z',\n",
       "  'author': 'naga-dsalgo',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '211',\n",
       "  'text': 'Yes it is fixed for me ... I edited installed version not the downloaded\\ngit version ..\\n...\\nOn Tue, Apr 2, 2019 at 4:37 AM Shivam Akhauri ***@***.***> wrote:\\n USER Is it fixed? I too added\\n \"global_step\" to the list. But still get the error\\n\\n \\n You are receiving this because you were mentioned.\\n\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .'},\n",
       " '213': {'created_at': '2019-02-21T08:41:35Z',\n",
       "  'author': 'AprilSongRits',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': \"Hi, FILEPATH \\n FILEPATH \\nmodeling_test.py:25: in \\nfrom pytorch_pretrained_bert import (BertConfig, BertModel, BertForMaskedLM,\\n/ FILEPATH :7: in \\nfrom .modeling import (BertConfig, BertModel, BertForPreTraining,\\n/ FILEPATH :218: in \\nfrom apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\\n/ FILEPATH :18: in \\nfrom apex.interfaces import (ApexImplementation,\\n/ FILEPATH :10: in \\nclass ApexImplementation(object):\\n/ FILEPATH :14: in ApexImplementation\\nimplements(IApex)\\n/ FILEPATH :483: in implements\\nraise TypeError(_ADVICE_ERROR % 'implementer')\\nE TypeError: Class advice impossible in Python3. Use the USER class decorator instead.\\nMy configurations are as follows:\\npython version VERSION \\nCUDA Version VERSION \\ntorch== VERSION .post2\\napex== VERSION .dev0\\nzope.interface== VERSION \\nThanks\"},\n",
       " '213_0': {'created_at': '2019-03-01T06:25:23Z',\n",
       "  'author': 'lemo2012',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '213',\n",
       "  'text': 'any solution here?'},\n",
       " '213_1': {'created_at': '2019-03-06T09:16:15Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '213',\n",
       "  'text': 'This looks like an incompatibility between apex and zope.\\nHave you tried without installing apex?'},\n",
       " '213_2': {'created_at': '2019-03-07T03:06:21Z',\n",
       "  'author': 'AprilSongRits',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '213',\n",
       "  'text': 'This looks like an incompatibility between apex and zope.\\nHave you tried without installing apex?\\n\\nI uninstalled apex, it works now!\\nThank you so much!!!!'},\n",
       " '214': {'created_at': '2019-02-21T11:09:22Z',\n",
       "  'author': 'spolu',\n",
       "  'author_location': nan,\n",
       "  'type': 'issue',\n",
       "  'text': \"Conv1D seems to be inherited from GPT but does not seem to serve any special purpose in GPT2 (BERT uses Linear).\\nShould GPT2's model be moved to using Linear (which is easier to grasp obvioulsy)?\"},\n",
       " '214_0': {'created_at': '2019-02-21T11:10:07Z',\n",
       "  'author': 'spolu',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '214',\n",
       "  'text': \"Maybe this would break pre-trained weights loading? Interested to understand if that's the only reason?\"},\n",
       " '214_1': {'created_at': '2019-03-06T09:16:59Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '214',\n",
       "  'text': 'Possibility, feel free to test the modification and submit a PR USER !'},\n",
       " '214_2': {'created_at': '2023-04-17T14:47:26Z',\n",
       "  'author': 'IdoAmit198',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '214',\n",
       "  'text': 'Hi guys,\\nI also wondered whether anyone modified the gpt2 model to have nn.Linear instead of Conv1D layers (using the pre-trained weights).\\nDid any of you success or found such implementation?'},\n",
       " '214_4': {'created_at': '2024-07-28T05:15:07Z',\n",
       "  'author': 'lakshith-403',\n",
       "  'author_location': 'LK',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '214',\n",
       "  'text': 'USER USER \\nThe difference between Convo1D and Linear layers is,\\n\\nIn Linear layers weight matrix is transposed when doing the calculation\\nIn Convo1D no transpose occurs\\n\\nSo what you can do is, get the state dict from the pre trained weights, get Convo1d layer entries and transpose those tensors.\\nAfter this just replace convo1D with linear layers and load the state dict to this new model.'},\n",
       " '215': {'created_at': '2019-02-22T01:47:02Z',\n",
       "  'author': 'jwhite2a',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'My goal is to convert and train on the BioBERT pretrained checkpoints in pytorch and train on the SQuAD VERSION Dataset.\\nI have (seemingly) FILEPATH .\\nI loaded the converted checkpoint into the run_squad.py example. I also changed the tokenizer to use the vocab file found with the BioBERT model. At this point, I was able to train the model and observe the loss to decrease.\\nMy first issue appeared when trying to write the SQuAD predictions. best_non_null_entry.start_logit did not have a start_logit because the best_non_null_entry was NoneType. This error resembles the previous issue ISSUE_REF . I implemented the solution found and my code was able to run.\\nMy results from training have been the same or worse than a random model. Nearly all of the SQuAD predictions are the \"empty\" string text from the fix of ISSUE_REF .\\nI believe the original cause of the NoneType error for best_non_null_entry is the reason for the failure to predict anything.\\nAre there specs to obey when converting a TF pretrained BERT model?\\nWhat would cause the NoneType error for best_non_null_entry?\\nAny and all help is appreciated.'},\n",
       " '215_1': {'created_at': '2019-03-10T02:35:51Z',\n",
       "  'author': 'sophial05',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '215',\n",
       "  'text': \"I'm trying to convert BioBert to Pytorch also, so just wondering if you could share a bit more details on how you are doing the conversion. Thanks!\"},\n",
       " '215_2': {'created_at': '2019-03-13T00:54:28Z',\n",
       "  'author': 'jwhite2a',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '215',\n",
       "  'text': \"First, I downloaded the BioBERT TF checkpoints here. Each model (i.e. biobert_pmc) should have three .ckpt files, a vocab.txt file, and a bert_config.json file.\\nInitially, I tried to use the command line interface pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch using the bert_config.json and .ckpt files seen above. I ran into AttributeError: 'Parameter' object has no attribute 'BERTAdam'. I followed the solution here.\\nTo do this, I copied the convert_tf_checkpoint_to_pytorch.py file and the load_tf_weights_in_bert function found in modeling.py. I then added the two lines seen in the solution above in my own version of the function and file.\\nGiven correct file paths, this worked to convert all three BioBERT checkpoints into pytorch .bin files.\"},\n",
       " '215_3': {'created_at': '2019-03-21T12:38:46Z',\n",
       "  'author': 'JohnGiorgi',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '215',\n",
       "  'text': 'USER Thank you! This worked for me.'},\n",
       " '216': {'created_at': '2019-02-22T04:42:39Z',\n",
       "  'author': 'Shi-Linqing-Jason',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'When I run_lm_finetuning with the exemplary training corpus (small_wiki_sentence_corpus.txt), I printed the tr_loss every 20 steps. I found that the tr_loss increases very fast. I wonder what the reason is.'},\n",
       " '216_0': {'created_at': '2019-03-01T08:58:07Z',\n",
       "  'author': 'tholor',\n",
       "  'author_location': 'DE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '216',\n",
       "  'text': 'What tr_loss are you exactly printing here? Is it possible that you just print this one here? URL If yes, you should divide it by the number of training steps (nb_tr_steps) first to get your average train loss.'},\n",
       " '217_0': {'created_at': '2019-02-22T17:18:09Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '217',\n",
       "  'text': \"Unfortunately, apex (and fp16 in general) only work on GPU. So you can't use it on MacOS :/\"},\n",
       " '218_1': {'created_at': '2019-03-03T02:47:30Z',\n",
       "  'author': 'mrkchangold',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '218',\n",
       "  'text': 'The above does not resolve the err'},\n",
       " '218_2': {'created_at': '2019-03-06T09:20:02Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '218',\n",
       "  'text': \"Let's rather keep the library's internal using Path and fix the examples by adding str there instead.\\nFixed on master now.\"},\n",
       " '219': {'created_at': '2019-02-23T17:49:23Z',\n",
       "  'author': 'howardhsu',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': \"I recently noticed that using fp16 dropped the performance of BERT on my own dataset but improved on another (it works fine on examples like MPRC). It's about 4% so unlikely to be random noise.\\nI'm trying to see the reason and noticed examples from apex:\\n URL \\nactually uses a global copy of the parameters during training but examples in this repository just use fp16 for all steps (and for saving parameters.) Wondering will this be the potential reason?\"},\n",
       " '220': {'created_at': '2019-02-24T06:52:50Z',\n",
       "  'author': 'bergen',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'TransfoXLLMHeadModel gives an output of log probabilities of shape [batch_size, sequence_length, n_tokens]. What do these probabilities represent? For example, what distribution is output at the first sequence position? Is it the conditional distribution given the first word? If so, how can the probability of a complete sentence be computed, including the first word?\\nAlso, the readme states:\\n\\nsoftmax_output: output of the (adaptive) softmax:\\nif target is None: Negative log likelihood of shape [batch_size, sequence_length]\\n\\nThis appears to be incorrect. From current behavior, it should say: if target is not None'},\n",
       " '220_0': {'created_at': '2019-03-06T09:25:22Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '220',\n",
       "  'text': \"Hi,\\n FILEPATH 's the usual language modeling probabilities: each token probability given the previous tokens\\n FILEPATH , fixed.\"},\n",
       " '221_0': {'created_at': '2020-06-12T20:06:09Z',\n",
       "  'author': 'dr-aheydari',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '221',\n",
       "  'text': 'Hi USER \\nWere you able to fix this issue? Because I run into a similar issue.\\nI would appreciate any guidance on this issue.\\nThank you.'},\n",
       " '221_1': {'created_at': '2020-07-08T09:43:35Z',\n",
       "  'author': 'PetreanuAndi',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '221',\n",
       "  'text': 'same. bump. Latest master does not handle cache_dir or mode'},\n",
       " '221_3': {'created_at': '2020-07-09T19:49:27Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '221',\n",
       "  'text': 'Hi! That\\'s weird, the cache_dir argument is available on the GlueDataset:\\n\\n \\n \\n FILEPATH \\n \\n \\n Lines 58 to 75\\n in\\n COMMIT \\n \\n \\n \\n \\n\\n \\n \\n class GlueDataset(Dataset): \\n \\n\\n \\n \\n \"\"\" \\n \\n\\n \\n \\n This will be superseded by a framework-agnostic approach \\n \\n\\n \\n \\n soon. \\n \\n\\n \\n \\n \"\"\" \\n \\n\\n \\n \\n \\n \\n\\n \\n \\n args: GlueDataTrainingArguments \\n \\n\\n \\n \\n output_mode: str \\n \\n\\n \\n \\n features: List[InputFeatures] \\n \\n\\n \\n \\n \\n \\n\\n \\n \\n def __init__( \\n \\n\\n \\n \\n self, \\n \\n\\n \\n \\n args: GlueDataTrainingArguments, \\n \\n\\n \\n \\n tokenizer: PreTrainedTokenizer, \\n \\n\\n \\n \\n limit_length: Optional[int] = None, \\n \\n\\n \\n \\n mode: Union[str, Split] = Split.train, \\n \\n\\n \\n \\n cache_dir: Optional[str] = None, \\n \\n\\n \\n \\n ): \\n \\n \\n \\n\\nIs it possible you haven\\'t pulled the new changes in a while? If you get an error, could you please open a new issue with all the information relative to your environment, the command you ran and the stack trace? Thanks a lot!'},\n",
       " '222': {'created_at': '2019-02-26T08:56:20Z',\n",
       "  'author': 'leonwyang',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'I am running the squad example.\\nI have a Tesla M60 GPU which has about 8GB of memory. For bert-large-uncased model, I can only take batch size as 2, even after I used FLAG . Is it normal?'},\n",
       " '222_0': {'created_at': '2019-03-03T00:21:25Z',\n",
       "  'author': 'leonwyang',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '222',\n",
       "  'text': 'solved'},\n",
       " '223': {'created_at': '2019-02-26T12:18:20Z',\n",
       "  'author': 'Jasperty',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'i use my output dir as bert_model, but cannot find the model'},\n",
       " '223_0': {'created_at': '2019-03-06T09:26:00Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '223',\n",
       "  'text': \"Should work. Without more information I can't really help you.\"},\n",
       " '224': {'created_at': '2019-02-26T12:53:33Z',\n",
       "  'author': 'vebits',\n",
       "  'author_location': 'NO',\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI am trying to pre-train using BertForPreTraning in run_lm_finetuning.py. My target corpus is based on very many tweets and I am unsure how the model will tackle that since they are mostly only one sentence. Will it affect the IsNextSentence task?\\nShould my .txt input file consist of one tweet on each line where each tweet is seperated by an empty line?'},\n",
       " '224_0': {'created_at': '2019-02-27T20:48:17Z',\n",
       "  'author': 'tuhinjubcse',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '224',\n",
       "  'text': 'ISSUE_REF \\nI had the same issue and but apparently this cant be done in BERT'},\n",
       " '224_1': {'created_at': '2019-03-06T09:26:53Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '224',\n",
       "  'text': \"Yes, can't be done currently. Feel free to submit a PR to extend the run_lm_finetuning example USER !\"},\n",
       " '225_0': {'created_at': '2019-02-28T00:01:29Z',\n",
       "  'author': 'eric-yates',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '225',\n",
       "  'text': 'I have a similar problem. I labeled the tokens as \"X\" and then got an error relating to NUM_LABELS. BERT appears to have thought the X was a third label, and I only specified there to be two labels.'},\n",
       " '225_1': {'created_at': '2019-02-28T15:23:21Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '225',\n",
       "  'text': 'You do not need to introduce an additional tag. This is explained here:\\n ISSUE_REF (comment)'},\n",
       " '225_2': {'created_at': '2019-03-06T09:28:10Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '225',\n",
       "  'text': \"Yes, I've left ISSUE_REF open to discuss all these questions. Feel free to read the discussion there and ask questions if needed. Closing this issue.\"},\n",
       " '225_3': {'created_at': '2021-07-16T13:34:41Z',\n",
       "  'author': 'ritwikmishra',\n",
       "  'author_location': 'IN',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '225',\n",
       "  'text': 'USER AFAIK\\nTo answer your question \"How the sub-tokens could be masked during training & testing\"\\nThere is no need of masking. The sub-word token_ids (except for the first) are not fed to the BERT model.\\nPlease tell me if i am wrong.'},\n",
       " '226': {'created_at': '2019-02-26T16:14:45Z',\n",
       "  'author': 'lukovnikov',\n",
       "  'author_location': 'BE',\n",
       "  'type': 'issue',\n",
       "  'text': 'OpenAIAdam version of warmup_linear does not linearly increase lr, instead it looks like this:\\n\\nThis is different from BertAdam version of warmup_linear. Should they not be the same (Bert version)?\\n\\nif t_total is specified incorrectly (too small), learning rate becomes negative after t_total for both versions. Probably it would be better to set lr to 0 to avoid situations like Issue#297. Also, with a too small t_total, there is a drop in lr right after warmup is reached:\\n\\nLet me know if I should PR a fix for both.'},\n",
       " '226_0': {'created_at': '2019-02-27T13:59:25Z',\n",
       "  'author': 'valsworthen',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '226',\n",
       "  'text': 'I just ran into this problem while running BERT on large samples from run_squad.py. I think a fix would be welcome because this is a really disturbing and hard to catch issue.\\nIt would probably be enough to move the optimizer creation + computing of num_train_optimization_steps inside the train loop.'},\n",
       " '226_1': {'created_at': '2019-02-27T14:15:43Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '226',\n",
       "  'text': \"Happy to welcome a PR on this indeed.\\nI'm not super fan of silently hide a wrong t_total by setting lr to zero so maybe sending a warning logger. warning at the same time would be nice too.\"},\n",
       " '226_3': {'created_at': '2019-03-06T09:28:38Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '226',\n",
       "  'text': 'Fixed in master now, thanks USER !'},\n",
       " '227': {'created_at': '2019-02-27T04:24:42Z',\n",
       "  'author': 'PaulZhangIsing',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'issue',\n",
       "  'text': 'Thanks for giving such awesome project.\\nHowever, I have encountered some problem.\\nAfter training the model, I just want to do eval on another dataset with the trained model. Therefore I only open do_eval.\\nHowever, it gives me this error:\\nTraceback (most recent call last):\\nFile \"run_classifier_torch.py\", line 687, in \\nmain()\\nFile \"run_classifier_torch.py\", line 677, in main\\n\\'loss\\': FILEPATH }\\nUnboundLocalError: local variable \\'tr_loss\\' referenced before assignment\\nIt seems that the loss and tr_loss are only declared in train. If we neglect the training process, the error shall pop up.\\nsolution : using eval_loss instead'},\n",
       " '227_0': {'created_at': '2019-02-28T02:51:14Z',\n",
       "  'author': 'bryandai',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '227',\n",
       "  'text': 'evalonly works last ver, need to mv some code out of train...'},\n",
       " '227_1': {'created_at': '2019-02-28T02:57:11Z',\n",
       "  'author': 'PaulZhangIsing',\n",
       "  'author_location': 'SG',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '227',\n",
       "  'text': 'evalonly works last ver, need to mv some code out of train...\\n\\nyup I agree. I shall do the eval loss and do it'},\n",
       " '227_2': {'created_at': '2019-03-06T09:29:50Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '227',\n",
       "  'text': \"Seems fixed in master, right? Feel free to re-open the issue if it's not the case.\"},\n",
       " '228': {'created_at': '2019-02-27T20:57:13Z',\n",
       "  'author': 'AshwinAmbal',\n",
       "  'author_location': 'US',\n",
       "  'type': 'issue',\n",
       "  'text': 'I have been using your PyTorch implementation of Googles BERT by HuggingFace for the MADE VERSION dataset for quite some time now. Up until last time (11-Feb), I had been using the library and getting an F-Score of VERSION for my Named Entity Recognition task by Fine Tuning the model. But this week when I ran the exact same code which had compiled and run earlier, it threw an error when executing this statement:\\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=long, truncating=post, padding=post)\\n\\nValueError: Token indices sequence length is longer than the specified\\nmaximum sequence length for this BERT model (632 > 512). Running this\\nsequence through BERT will result in indexing errors\\n\\nThe full code is available in this colab notebook.\\nTo get around this error I modified the above statement to the one below by taking the first 512 tokens of any sequence and made the necessary changes to add the index of [SEP] FILEPATH .\\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt[:512]) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=long, truncating=post, padding=post)\\nThe result shouldnt have changed because I am only considering the first 512 tokens in the sequence and later truncating to 75 as my (MAX_LEN=75) but my F-Score has dropped to VERSION and my precision to VERSION while the Recall remains the same ( VERSION ). I am unable to share the dataset as I have signed a confidentiality clause but I can assure all the preprocessing as required by BERT has been done and all extended tokens like (Johanson > Johan ##son) have been tagged with X and replaced later after the prediction as said in the BERT Paper.\\nHas anyone else faced a similar issue or can elaborate on what might be the issue or what changes the PyTorch (Huggingface) has done on their end recently?'},\n",
       " '228_0': {'created_at': '2019-03-01T20:51:43Z',\n",
       "  'author': 'AshwinAmbal',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"I've found a fix to get around this.\\nRunning the same code with pytorch-pretrained-bert== VERSION solves the issue and the performance is restored to normal.\\nThere's something messing with the model performance in BERT Tokenizer or BERTForTokenClassification in the new update which is affecting the model performance.\\nHoping that HuggingFace clears this up soon. :)\\nThanks.\"},\n",
       " '228_1': {'created_at': '2019-03-05T18:57:47Z',\n",
       "  'author': 'jplehmann',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"There's something messing with the model performance in BERT Tokenizer or BERTForTokenClassification in the new update which is affecting the model performance.\\nHoping that HuggingFace clears this up soon. :)\\n\\nSounds like the issue should remain open?\"},\n",
       " '228_2': {'created_at': '2019-03-06T06:13:48Z',\n",
       "  'author': 'AshwinAmbal',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"Oh. I didn't know I closed the issue. Let me reopen it now.\\n\\nThanks.\\n...\\nOn Tue, 5 Mar, 2019, 10:57 AM John Lehmann, ***@***.***> wrote:\\n There's something messing with the model performance in BERT Tokenizer or\\n BERTForTokenClassification in the new update which is affecting the model\\n performance.\\n Hoping that HuggingFace clears this up soon. :)\\n\\n Sounds like the issue should remain open?\\n\\n \\n FILEPATH .\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .\"},\n",
       " '228_3': {'created_at': '2019-03-06T06:14:01Z',\n",
       "  'author': 'AshwinAmbal',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"Sorry about that. Didn't realise I closed the issue.\\nReopened it now. :)\"},\n",
       " '228_4': {'created_at': '2019-03-06T09:36:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"Seems strange that the tokenization changed.\\nSo you were only having sequence with less than 512 tokens before and now some sequences are longer?\\nWithout having access to your dataset I can't really help you but if you can compare the tokenized sequences in your dataset with pytorch-pretrained-bert== VERSION versus sequences tokenized with the current pytorch-pretrained-bert== VERSION to identify a sequence which is tokenized differently it could help find the root of the issue.\\nThen maybe you can just post some part of a sequence or example which is tokenized differently without breaching your confidentiality clause?\"},\n",
       " '228_5': {'created_at': '2019-05-10T15:33:41Z',\n",
       "  'author': 'savindi-wijenayaka',\n",
       "  'author_location': 'NZ',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': 'I had the same issue when trying to use it with Flair for text classification. Can I know the root cause of this issue? Does this mean that my text part in the dataset is too long?'},\n",
       " '228_6': {'created_at': '2019-05-10T20:21:06Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': 'Yes, BERT only accepts inputs smaller or equal to 512 tokens.'},\n",
       " '228_7': {'created_at': '2019-06-19T16:20:50Z',\n",
       "  'author': 'Ezekiel25c17',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': 'Seems strange that the tokenization changed.\\nSo you were only having sequence with less than 512 tokens before and now some sequences are longer?\\nWithout having access to your dataset I can\\'t really help you but if you can compare the tokenized sequences in your dataset with pytorch-pretrained-bert== VERSION versus sequences tokenized with the current pytorch-pretrained-bert== VERSION to identify a sequence which is tokenized differently it could help find the root of the issue.\\nThen maybe you can just post some part of a sequence or example which is tokenized differently without breaching your confidentiality clause?\\n\\nI think I found a little bug in tokenization.py that may be related to this issue.\\nI was facing a similar problem that using the newest version leads to a huge accuracy drop (from 88% to 22%) in a very common multi-class news title classification task. Using pytorch-pretrained-bert== VERSION was actually a workaround so I did a comparison of the tokenization logs of these two versions.\\nthe main problem was that many tokens have different ids during training and evaluation. Compared to VERSION , FILEPATH .\\nIn my case, this generated vocab.txt differs from the original one because in URL the tokenizer deletes all the trailing spaces. This actually strips different tokens, say a normal space and a non-break space into an identical empty token \"\". After changing this line to \"token = token.rstrip(\"n\") \", I was able to reproduce the expected accuracy using the newest version'},\n",
       " '228_8': {'created_at': '2019-06-19T19:58:55Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"USER I'm a bit surprised that training spaces would be important in the vocabulary so I would like to investigate this deeper.\\nCan you give me the reference of the following elements you were using in your tests:\\n\\nthe python version,\\nversions of pytorch-pretrained-bert\\nthe pretrained model,\\nthe vocabulary (probably same as the model I guess),\\nthe example script.\\n\\nSo I can reproduce the behavior\"},\n",
       " '228_10': {'created_at': '2019-06-20T16:49:29Z',\n",
       "  'author': 'AshwinAmbal',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': 'USER Shuffled indices would make sense for the accuracy to drop.\\n USER I had longer sequences before too but in pytorch-pretrained-bert== VERSION the statement\\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=long, truncating=post, padding=post) did not have a very strict implementation but in VERSION it threw a Value Error which I overcame by truncating the sequences to 512 before feeding it to the \"tokenizer.convert_tokens_to_ids(txt)\" function. Either way, I was using only the first 75 tokens of the sentence (MAX_LEN=75). So it didn\\'t matter to me. When I was re-running the same code this was the only statement that threw an error which was why I thought there must have been a change in this functionality in the update.'},\n",
       " '228_11': {'created_at': '2019-08-11T00:47:56Z',\n",
       "  'author': 'IINemo',\n",
       "  'author_location': 'AE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"The issue is still there (current master or VERSION . release). Looks like 'BertForTokenClassification' is broken since VERSION . With current version any trained model produces very low scores (dozens of percentage points lower than VERSION ).\"},\n",
       " '228_12': {'created_at': '2019-08-19T12:31:54Z',\n",
       "  'author': 'IINemo',\n",
       "  'author_location': 'AE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"Sorry for misleading comment. BertForTokenClassification is fine, I just did not use the proper padding label (do not use 'O' label for padding, use a separate label, e.g. '[PAD]').\"},\n",
       " '228_13': {'created_at': '2019-09-11T06:49:36Z',\n",
       "  'author': 'akashsara',\n",
       "  'author_location': 'CA',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"USER if you are using an attention mask, then wouldn't the label for the padding not matter at all?\"},\n",
       " '228_14': {'created_at': '2019-11-24T12:21:30Z',\n",
       "  'author': 'IINemo',\n",
       "  'author_location': 'AE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': \"Hi,\\n\\nIf you use O in versions of pytorch pretrained bert >= VERSION , the problem\\nhappens because loss on padded tokens is ignored, then any wrong output of\\nthe model on padded tokens will not be penalized and the model will learn\\nwrong signal for labels O.\\n\\nThe full fixed version of the code that does sequence tagging with BERT and\\nnewest version of pytorch pretrained bert is here:\\n URL \\n\\nThere is a class SequenceTaggerBert that works with tokenized sequences\\n(e.g., nltk tokenizer) and does all the necessary preprocessing under the\\nhood.\\n\\nBest\\n...\\nOn Wed, Sep 11, 2019 at 9:50 AM Akash Saravanan ***@***.***> wrote:\\n USER if you are using an attention mask,\\n then wouldn't the label for the padding not matter at all?\\n\\n \\n You are receiving this because you were mentioned.\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .\"},\n",
       " '228_15': {'created_at': '2020-03-24T05:44:44Z',\n",
       "  'author': 'Swty13',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '228',\n",
       "  'text': 'Yes, BERT only accepts inputs smaller or equal to 512 tokens.\\n\\nHi , I wanted to trained BERT for text more than 512 tokens ,I can not truncate text to 512 as there will be loss of information in that case.Could you please help how can I handle this or any other suggestion to build customized NER for my usecase using BERT.\\nThanks'},\n",
       " '229': {'created_at': '2019-02-28T05:05:55Z',\n",
       "  'author': 'naga-dsalgo',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Trying to get run_lm_finetunning example on working on below GPU machine but finding getting zeroDivisonError.Any idea what could be causing this error?\\n\\n FILEPATH'},\n",
       " '229_0': {'created_at': '2019-03-06T09:38:58Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '229',\n",
       "  'text': \"Seems like an error with t_total. t_total is the number of training optimization steps of the optimizer defined here in the run_lm_finetuning example.\\nCan you make sure it's not zero?\"},\n",
       " '229_1': {'created_at': '2019-03-06T13:40:16Z',\n",
       "  'author': 'vebits',\n",
       "  'author_location': 'NO',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '229',\n",
       "  'text': 'Your batch_size of 32 is too big for such a small train_file, i.e. sample_text.txt. Try setting batch_size to 16 or send in a larger train file with more text.'},\n",
       " '230': {'created_at': '2019-02-28T06:43:35Z',\n",
       "  'author': 'kennethliukai',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Is this pre-trained BERT good for NER or classification on Chinese corpus?\\nThanks.'},\n",
       " '230_0': {'created_at': '2019-03-06T09:40:42Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '230',\n",
       "  'text': 'You should probably use the bert-base-chinese model to start from.\\nPlease refer to the original bert tensorflow implementation from Google.\\nThere are a lot of discussion about chinese models in the issues of this repo.'},\n",
       " '231': {'created_at': '2019-02-28T08:36:02Z',\n",
       "  'author': 'guotong1988',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'How can we edit BERT to do the next-word-predict task?\\nThank you very much!'},\n",
       " '232': {'created_at': '2019-02-28T08:42:09Z',\n",
       "  'author': 'navdeep1604',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi all\\nI have trained bert question answering on squad v 1 data set. As I was using colab which was slow . so I used 5000 examples from squad and trained the model which took 2 hrs and gave accuracy of 51%. My question is that\\n\\nAs i saved pytorch_bin file after trainining. Can i use this new bin file and again train next 5000 from squad.should i replace this bin file with old pytorch bin file created in uncased folder. What steps i need to follow\\n\\ni have a custom data. To train on custom qyestion answer. Do i need to include same dataset(append) FILEPATH =custom data.? How can i leverage squad trained model to further train on custom data\\n\\ncan anybody help me with script to convert my data to squad format\\n\\nDetailed steps are appreciated fo leveraging squad traines model and train for custom data on top of same'},\n",
       " '232_2': {'created_at': '2019-06-27T12:20:48Z',\n",
       "  'author': 'gqoew',\n",
       "  'author_location': 'IT',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': \"This might help you setup a QA system with custom data, it's built on top of this repo: URL\"},\n",
       " '232_3': {'created_at': '2019-07-16T22:01:01Z',\n",
       "  'author': 'Swathygsb',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': 'Hi USER ,\\nI faced similar issue, since my custom training data (240 QA pairs) was very less.'},\n",
       " '232_4': {'created_at': '2019-08-15T16:28:00Z',\n",
       "  'author': 'cformosa',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': \"Hi, for anyone who has made a custom QA dataset, how did you go about get the start position and end position for the answers or did you already have them easily accessible? I have a large dataset set of questions with corresponding context given by people; however, I don't have the specific answers as there can be many acceptable answers. My goal is to determine whether the context contains an answer to the question (similar to squad VERSION ). Preliminary results after fine tuning on Squad VERSION weren't super great so I wanted to add more examples. Any recs on how I could label my data in the correct format for say a bert or would I need to crowd source labels from a vendor?\"},\n",
       " '232_5': {'created_at': '2019-09-12T08:07:30Z',\n",
       "  'author': 'andrelmfarias',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': 'Hi USER ,\\nThe package for QA system mentioned above also has an annotation tool that can help you with that task:\\n URL'},\n",
       " '232_6': {'created_at': '2019-09-13T14:07:07Z',\n",
       "  'author': 'cformosa',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': 'Thanks for the link USER . I was looking over it and it seems extremely useful for sure. Seems like it will take a long time to generate a large corpus of training data but nevertheless its seems quite helpful. Thanks!'},\n",
       " '232_7': {'created_at': '2019-10-04T05:38:08Z',\n",
       "  'author': 'rsomani95',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': \"Hi USER , thank you for sharing this great resource!\\nThe cdQA-suite seems to cater to a specific kind of question answering, as described in your Medium article. To summarise, it looks for the answer to a question from a collection of documents -- all these documents most likely contain different kinds of information regarding a particular topic. For example, a system built using cdQA could contain 10 different documents regarding 10 different historical periods, and you could ask it a question about any of these time periods, and it would search for the relevant answer within these 10 documents.\\nHowever, if the system you want to build is such: you have 10 court orders, and you want to ask the system the same set of questions for each court order. For example:\\n\\nWhen was the order filed?\\nWho filed the order?\\nWho was the order filed against?\\nWas there a settlement?\\n\\nIn this case, I wouldn't want the system to search through every document, but instead look for answers within the document itself. Exactly like SQuaD VERSION .\\nMy assessment is that I wouldn't be able to build such a system using cdQA but I could use the cdQA annotator to build my dataset. Is that a sound assessment?\\nAlso, I'm curious to hear your thoughts on how feasible it would be to expect good results when the context is rather long (anywhere between 2-10 pages).\\nThank you :)\"},\n",
       " '232_8': {'created_at': '2019-10-07T11:14:15Z',\n",
       "  'author': 'andrelmfarias',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '232',\n",
       "  'text': 'Hi USER ,\\nAs your questions are particularly related to cdQA I opened an issue with your questions in our repository to avoid spamming here: ISSUE_REF \\nI just answered them there.'},\n",
       " '233_0': {'created_at': '2019-03-06T09:41:27Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '233',\n",
       "  'text': 'Yes, feel free to submit a PR for that.'},\n",
       " '234_0': {'created_at': '2019-03-03T11:02:09Z',\n",
       "  'author': 'abeljim',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'Did you run python install FLAG .'},\n",
       " '234_1': {'created_at': '2019-03-03T12:42:01Z',\n",
       "  'author': 'bheinzerling',\n",
       "  'author_location': 'JP',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"There's a way to install cloned repositories with pip, but the easiest way is to use plain python for this:\\nAfter cloning and changing into the pytorch-pretrained-BERT directory, run python setup.py develop.\"},\n",
       " '234_2': {'created_at': '2019-03-06T09:42:17Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'Yes, please follow the installation instructions on the readme here'},\n",
       " '234_4': {'created_at': '2019-11-27T09:20:10Z',\n",
       "  'author': 'jerrypcl',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'Anybody know why \"pip install [ FLAG ] .\" failed here? It is some missing python package needed for this?'},\n",
       " '234_5': {'created_at': '2019-11-27T09:41:32Z',\n",
       "  'author': 'TheEdoardo93',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'Please open a command line and enter pip install git+ URL for installing Transformers library from source. However, Transformers v- VERSION has been just released yesterday and you can install it from PyPi with pip install transformers\\nTry to install this latest version and launch the tests suite and keep us updated on the result!\\n\\nAnybody know why \"pip install [ FLAG ] .\" failed here? It is some missing python package needed for this?'},\n",
       " '234_6': {'created_at': '2019-11-27T09:55:20Z',\n",
       "  'author': 'jerrypcl',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'USER \\nThis is indeed the latest version installed( installed a few hours before)\\nName: transformers\\nVersion: VERSION \\nSummary: State-of-the-art Natural Language Processing for TensorFlow VERSION and PyTorch\\nHome-page: URL \\nAuthor: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\\nAuthor-email: thomas@huggingface.co\\nLicense: Apache\\nLocation: / FILEPATH \\nRequires: sacremoses, numpy, requests, boto3, regex, tqdm, sentencepiece\\nRequired-by:'},\n",
       " '234_9': {'created_at': '2019-11-27T10:14:21Z',\n",
       "  'author': 'jerrypcl',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'I did not install TensorFlow which is the reason for skips. I need reasons for failure. I guess I will install TensorFlow and see how it goes.'},\n",
       " '234_10': {'created_at': '2019-11-27T10:16:08Z',\n",
       "  'author': 'TheEdoardo93',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"In the README.md file, Transformers' authors says to install TensorFlow VERSION and PyTorch VERSION + before installing Transformers library.\\n\\nI did not install TensorFlow which is the reason for skips. I need reasons for failure. I guess I will install TensorFlow and see how it goes.\"},\n",
       " '234_11': {'created_at': '2019-11-27T10:23:05Z',\n",
       "  'author': 'jerrypcl',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': '\"First you need to install one of, or both, TensorFlow VERSION and PyTorch.\" I don\\'t think that is the reason for failure.'},\n",
       " '234_12': {'created_at': '2019-11-27T14:49:52Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"Hi, I believe these two tests fail with an error similar to:\\n RuntimeError: expected device cpu and dtype Long but got device cpu and dtype Bool\\n\\nIf I'm not mistaken you're running with torch VERSION and we're testing with torch VERSION . This is a bug as we aim to support torch from VERSION +. Thank you for raising the issue, you can fix it by installing torch VERSION + while we work on fixing this.\"},\n",
       " '234_13': {'created_at': '2019-11-27T15:05:59Z',\n",
       "  'author': 'jerrypcl',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"Thanks! Yeah, I found it too by verbose mode. I suddenly remember some\\ntensorflow code have similar problem before. In my case,it is some const,\\nI just changed it from int to float. Indeed I am using torch1.2. Will\\nsee whether it works here or not. Any idea why the pip FLAG option is\\nnot working?\\n\\nOn Wed, Nov 27, 2019 at 22:49 Lysandre Debut \\nwrote:r\\n...\\n Hi, I believe these two tests fail with an error similar to:\\n\\n RuntimeError: expected device cpu and dtype Long but got device cpu and dtype Bool\\n\\n If I'm not mistaken you're running with torch VERSION and we're testing with\\n torch VERSION . This is a bug as we aim to support torch from VERSION +. Thank you\\n for raising the issue, you can fix it by installing torch VERSION + while we\\n work on fixing this.\\n\\n \\n You are receiving this because you commented.\\n Reply to this email directly, view it on GitHub\\n ,\\n or unsubscribe\\n \\n .\"},\n",
       " '234_14': {'created_at': '2019-11-27T15:23:44Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"The pip install FLAG . is probably working, it's just that some tests are failing due to code not tests on Torch VERSION .\\nThe install should have worked fine, and you should be fine with using every component in the library with torch VERSION except the decoder architectures on which we are working now. Updating to torch VERSION means it will work with decoder architectures too.\"},\n",
       " '234_15': {'created_at': '2019-11-27T22:00:26Z',\n",
       "  'author': 'jerrypcl',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"VERSION torch must work with cuda10.1? I have VERSION for tensorflow which is\\nstill having problem with VERSION . Thanks for the info. Really appreciate ur\\nfast response!\\n...\\nOn Wed, Nov 27, 2019 at 23:23 Lysandre Debut ***@***.***> wrote:\\n The pip install FLAG . is probably working, it's just that some tests are\\n failing due to code not tests on Torch VERSION .\\n\\n The install should have worked fine, and you should be fine with using\\n every component in the library with torch VERSION except the decoder\\n architectures on which we are working now. Updating to torch VERSION means it\\n will work with decoder architectures too.\\n\\n \\n You are receiving this because you commented.\\n Reply to this email directly, view it on GitHub\\n ,\\n or unsubscribe\\n \\n .\"},\n",
       " '234_16': {'created_at': '2019-11-28T08:56:30Z',\n",
       "  'author': 'TheEdoardo93',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"In the official PyTorch documentation, in the installation section, you can see that you can install PyTorch VERSION with CUDA VERSION or CUDA VERSION , so PyTorch VERSION + CUDA VERSION works!\\n\\n VERSION torch must work with cuda10.1? I have VERSION for tensorflow which is still having problem with VERSION . Thanks for the info. Really appreciate ur fast response!\\n...\\nOn Wed, Nov 27, 2019 at 23:23 Lysandre Debut @.***> wrote: The pip install FLAG . is probably working, it's just that some tests are failing due to code not tests on Torch VERSION . The install should have worked fine, and you should be fine with using every component in the library with torch VERSION except the decoder architectures on which we are working now. Updating to torch VERSION means it will work with decoder architectures too.  You are receiving this because you commented. Reply to this email directly, view it on GitHub , or unsubscribe URL .\"},\n",
       " '234_17': {'created_at': '2019-12-20T11:16:18Z',\n",
       "  'author': 'internetcoffeephone',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"What is the difference between the following?\\n\\npip install [ FLAG ] .\\npip install FLAG .\\npython setup.py develop\\n\\nThe first works doesn't work for me, yet is in the readme. The other two do. If this is system-dependent, shouldn't this be added to the readme?\"},\n",
       " '234_18': {'created_at': '2019-12-20T13:59:53Z',\n",
       "  'author': 'LysandreJik',\n",
       "  'author_location': nan,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'USER , using square brackets in a command line interface is a common way to refer to optional parameters. The first command means that you can either use pip install . or pip install FLAG .'},\n",
       " '234_19': {'created_at': '2019-12-20T14:16:45Z',\n",
       "  'author': 'internetcoffeephone',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': \"USER That makes sense, thanks for your answer!\\nStill, I'd argue against putting it in the readme like that. Firstly because it doesn't produce a sensible error message - secondly because anyone who wants an editable installation will know about that optional parameter already.\\nAs for the difference between the above commands, I found this page:\\n\\nTry to avoid calling setup.py directly, it will not properly tell pip that you've installed your package.\\nWith pip install FLAG :\\n\\nFor local projects, the SomeProject.egg-info directory is created relative to the project path. This is one advantage over just using setup.py develop, which creates the egg-info directly relative the current working directory.\"},\n",
       " '234_20': {'created_at': '2019-12-24T17:18:22Z',\n",
       "  'author': 'aaugustin',\n",
       "  'author_location': 'FR',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '234',\n",
       "  'text': 'I removed [ FLAG ] from the instructions because I found them confusing (before stumbling upon this issue).'},\n",
       " '235_0': {'created_at': '2019-03-06T09:43:41Z',\n",
       "  'author': 'thomwolf',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '235',\n",
       "  'text': \"Yes, feel free to open a PR if you want.\\nIt's just a regular PyTorch model so all the standard ways of training a PyTorch model work.\"},\n",
       " '235_1': {'created_at': '2019-06-15T11:44:48Z',\n",
       "  'author': 'bakszero',\n",
       "  'author_location': 'IE',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '235',\n",
       "  'text': 'Is is possible to fine-tune GPT2 on downstream tasks currently?'},\n",
       " '235_2': {'created_at': '2019-07-09T00:15:37Z',\n",
       "  'author': 'shizhediao',\n",
       "  'author_location': 'US',\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '235',\n",
       "  'text': 'same questions'},\n",
       " '236': {'created_at': '2019-03-02T22:42:27Z',\n",
       "  'author': 'mingkkkkk',\n",
       "  'author_location': None,\n",
       "  'type': 'issue',\n",
       "  'text': 'Hi,\\nI was doing prediction after fine-tuning the bert-base model and I was wondering whether the f1 and em scores will show automatically since I only saw the following two log outputs\\n FILEPATH :20:05 - INFO - main - Writing predictions to: / FILEPATH \\n FILEPATH :20:05 - INFO - main - Writing nbest to: / FILEPATH \\nWhere am I able to get those scores? Thanks for any help!'},\n",
       " '236_0': {'created_at': '2019-03-03T11:03:05Z',\n",
       "  'author': 'abeljim',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '236',\n",
       "  'text': 'Use the Squad python scripts available on their website'},\n",
       " '236_1': {'created_at': '2019-03-03T11:06:00Z',\n",
       "  'author': 'mingkkkkk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '236',\n",
       "  'text': 'Is that run_squad.py? I used that one but didnt see output scores, having\\nthe output predictions files though. Thanks!\\n\\nabeljim 2019 3 3 3:03 :\\n...\\n Use the Squad python scripts available on their website\\n\\n \\n You are receiving this because you authored the thread.\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .'},\n",
       " '236_2': {'created_at': '2019-03-03T11:08:28Z',\n",
       "  'author': 'abeljim',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '236',\n",
       "  'text': 'URL get the eval script for the correct version'},\n",
       " '236_3': {'created_at': '2019-03-03T11:10:04Z',\n",
       "  'author': 'mingkkkkk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '236',\n",
       "  'text': 'Thanks so much, really helps!\\n\\nabeljim 2019 3 3 3:08 :\\n...\\n URL get the eval script for the\\n correct version\\n\\n \\n You are receiving this because you authored the thread.\\n Reply to this email directly, view it on GitHub\\n ,\\n or mute the thread\\n \\n .'},\n",
       " '236_4': {'created_at': '2019-03-04T05:44:35Z',\n",
       "  'author': 'mingkkkkk',\n",
       "  'author_location': None,\n",
       "  'type': 'comment',\n",
       "  'parent_issue_id': '236',\n",
       "  'text': 'USER Hi Abel,\\nI got a very strange problem in running the prediction only for run_squad.py and wonder if you have any idea about why this happens.\\nI first ran the following codes to both do train and predict on the files:\\n FILEPATH / /\\nThe output predictions.json file looks normal, but when I tried to delete the \" FLAG \" part and only do the prediction on the same file, it gives very different and strange outputs, many of the answers are repetitive as below and the scores are like only VERSION :\\n(The output in predictions are like:)\\n\\n\" COMMIT \": \"mands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their\",\\n\" COMMIT \": \"mands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their\",\\n\" COMMIT \": \"mands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their\",\\n\" COMMIT \": \"Rollo, agreed to swear fealty\",\\n\" COMMIT \": \"Rollo, agreed to swear fealty\",\\n\" COMMIT \": \"Rollo, agreed to swear fealty\",\\n\\nDo you know what caused the problem?'},\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the data\n",
    "node_data = {}\n",
    "with open('/home/chef/src/DAT6003/data/processed/final_nlp_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        node_data.update(json.loads(line))\n",
    "\n",
    "node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05abbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>author</th>\n",
       "      <th>author_location</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>parent_issue_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-05T21:35:51Z</td>\n",
       "      <td>ZhaoyueCheng</td>\n",
       "      <td>None</td>\n",
       "      <td>issue</td>\n",
       "      <td>Thanks a lot for the port! I have some minor q...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_0</th>\n",
       "      <td>2018-11-05T22:01:47Z</td>\n",
       "      <td>ZhaoyueCheng</td>\n",
       "      <td>None</td>\n",
       "      <td>comment</td>\n",
       "      <td>It also seems to me that the SQuAD VERSION can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_1</th>\n",
       "      <td>2018-11-05T23:20:46Z</td>\n",
       "      <td>abeljim</td>\n",
       "      <td>None</td>\n",
       "      <td>comment</td>\n",
       "      <td>It also seems to me that the SQuAD VERSION can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_3</th>\n",
       "      <td>2018-11-06T02:05:22Z</td>\n",
       "      <td>abeljim</td>\n",
       "      <td>None</td>\n",
       "      <td>comment</td>\n",
       "      <td>Just ran on 1 GPU batch size of 10, the result...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_4</th>\n",
       "      <td>2018-11-06T03:48:21Z</td>\n",
       "      <td>ZhaoyueCheng</td>\n",
       "      <td>None</td>\n",
       "      <td>comment</td>\n",
       "      <td>Sure, Thanks, I'm checking for the reason too,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_at        author author_location     type  \\\n",
       "0    2018-11-05T21:35:51Z  ZhaoyueCheng            None    issue   \n",
       "0_0  2018-11-05T22:01:47Z  ZhaoyueCheng            None  comment   \n",
       "0_1  2018-11-05T23:20:46Z       abeljim            None  comment   \n",
       "0_3  2018-11-06T02:05:22Z       abeljim            None  comment   \n",
       "0_4  2018-11-06T03:48:21Z  ZhaoyueCheng            None  comment   \n",
       "\n",
       "                                                  text parent_issue_id  \n",
       "0    Thanks a lot for the port! I have some minor q...             NaN  \n",
       "0_0  It also seems to me that the SQuAD VERSION can...               0  \n",
       "0_1  It also seems to me that the SQuAD VERSION can...               0  \n",
       "0_3  Just ran on 1 GPU batch size of 10, the result...               0  \n",
       "0_4  Sure, Thanks, I'm checking for the reason too,...               0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform dict as a DataFrame\n",
    "df = pd.DataFrame.from_dict(node_data, orient='index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c292be81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132b765f712b4b2988355ce5032203e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbf7bc19b3e428485396785f6c4076c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5b6a1c688344499433df41ec687a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14daf388bf5144ca9025da78170f4531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642dbbbfa7fc49048cb1eaf56b9fc9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a339ba7a5143b6bb38da9ab9e87bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create transformer pipeline for sentiment analysis\n",
    "sentiment_model = transformers.pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b5ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()\n",
    "output = sentiment_model(texts, batch_size=8, truncation=True, max_length=512, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7220088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_label\"] = [o[\"label\"] for o in output]\n",
    "df[\"sentiment_score\"] = [o[\"score\"] for o in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee10aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/chef/src/DAT6003/data/processed/nlp_sentiment_analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dat6003 venv)",
   "language": "python",
   "name": "dat6003-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
